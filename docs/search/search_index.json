{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ML cube Platform","text":"<p>Welcome to the ML cube Platform documentation!</p> <p>This website is designed to help you get the most out of ML cube Platform by providing clear, detailed explanations about the main concepts of the product and guidance on setup, configuration and usage. You will find definitions, tutorials, practical examples and everything you need to make full use of our product.</p> <p>Dive in to explore everything ML cube Platform has to offer, and feel free to reach out if you need further assistance. Let\u2019s get started!</p> <ul> <li> <p> Basic concepts</p> <p>Are you new to the ML cube Platform and don't know where to start?</p> <p>Start reading the Company page.</p> <p> More info</p> </li> <li> <p> Modules</p> <p>Do you want to learn what ML cube Platform can offer?</p> <p>Go to the Modules page.</p> <p> More info</p> </li> <li> <p> Data</p> <p>Data are not all the same! </p> <p>Discover how ML cube Platform handles data in the Data page.</p> <p> More info</p> </li> <li> <p> Integrations</p> <p>ML cube Platform is natively integrated with cloud solutions!</p> <p>Check how to configure them.</p> <p> More info</p> </li> <li> <p> API</p> <p>ML cube Platform has been developed with the API first mind!</p> <p>Explore how to use our SDK.</p> <p> More info</p> </li> <li> <p> Notebook Examples</p> <p>First time using our SDK?</p> <p>Check the Examples page to learn from them.</p> <p> More info</p> </li> </ul>"},{"location":"api/","title":"ML cube Platform API","text":"<p>ML cube Platform is an API-first application that provides full coverage of its functionalities both as Web Application and as API. In particular, we provide a Python SDK facilitating the interaction using high-level objects and automated procedures.</p> <ul> <li> <p> Python SDK</p> <p>Documentation page of Python SDK with low level details of available methods.</p> <p> More info</p> </li> <li> <p> Notebook Examples</p> <p>Jupyter notebooks examples showing most of the basic usage of ML cube Platform.</p> <p> More info</p> </li> </ul>"},{"location":"api/examples/","title":"Examples","text":"<p>We prepared for you a set of notebooks that can help you to understand how to use ML cube Platform SDK:</p> <ul> <li>0 - Company and Project</li> <li>1 - Task and Model</li> <li>2 - Production</li> </ul> <p>In the following notebooks you can see examples for specific type of tasks:</p> <ul> <li>Text multiclass classification</li> <li>Image multiclass classification</li> <li>RAG</li> </ul>"},{"location":"api/python/","title":"ML3 platform client SDK","text":""},{"location":"api/python/#installation","title":"Installation","text":"<pre><code>pip install ml3-platform-sdk\n</code></pre>"},{"location":"api/python/#usage","title":"Usage","text":"<p>Please refer to the documentation</p>"},{"location":"api/python/client/","title":"Client","text":""},{"location":"api/python/client/#ml3platformclient","title":"ML3PlatformClient","text":"<pre><code>ML3PlatformClient(\n   url: str, api_key: str, timeout: int = 180\n)\n</code></pre> <p>Client class is the single point of interaction with ML cube Platform APIs, it is initialized providing the <code>url</code> and the User <code>api_key</code>. Optionally, you can specify the <code>timeout</code> for the operations. Every operation is performed verifying the API Key and the permissions associated to the User that own that key.</p>"},{"location":"api/python/client/#methods-categories","title":"Methods categories","text":"<p>There are the following types of methods:</p> <ul> <li>entity creation: create the entity and return its identifier. It is used in the other methods to indicate the entity.</li> <li>entity update: modify the entity but do not return anything.</li> <li>entity getters: return a Pydantic <code>BaseModel</code> with the required entity.</li> <li>entity show: print to the stdout the entity, but they do not return anything.</li> <li>entity delete: delete the entity</li> <li>job submission: submit a job on ML cube Platform that will take some time. They return the job identifier that can be used to monitor its state.</li> <li>job waiters: given a job id wait the until the job is completed</li> </ul>"},{"location":"api/python/client/#exceptions","title":"Exceptions","text":"<p>The Client class raises only exceptions that are subclasses of <code>SDKClientException</code>. The exception has two fields that you can share with ML cube Support to get help in identifying the problem:</p> <ul> <li>error_code: unique identifier of the error</li> <li>error_message: message that explain the error</li> </ul> <p>The page is structured in different blocks of methods, one for each entity.</p> <p>Methods:</p>"},{"location":"api/python/client/#create_company","title":".create_company","text":"<pre><code>.create_company(\n   name: str, address: str, vat: str\n)\n</code></pre> <p>Create a company for the User, this method works only is the User has not a company yet. After the Company is created the User is the Company Owner.</p> <p>Args</p> <ul> <li>name  : the name of the company</li> <li>address  : the address of the company</li> <li>vat  : the vat of the company</li> </ul> <p>Returns</p> <ul> <li>company_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateCompanyException</code></p>"},{"location":"api/python/client/#get_company","title":".get_company","text":"<pre><code>.get_company()\n</code></pre> <p>Returns the company of the User</p> <p>Returns</p> <ul> <li>company  : <code>Company</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#update_company","title":".update_company","text":"<pre><code>.update_company(\n   name: (str|None), address: (str|None), vat: (str|None)\n)\n</code></pre> <p>Update company information.</p> <p>Empty values will not be updated.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : new the of the company</li> <li>address  : new billing address for the company</li> <li>vat  : new vat of the company</li> </ul> <p>Raises</p> <p><code>UpdateCompanyException</code></p>"},{"location":"api/python/client/#get_all_company_subscription_plans","title":".get_all_company_subscription_plans","text":"<pre><code>.get_all_company_subscription_plans()\n</code></pre> <p>Returns all subscription plans associated with the user company.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Returns</p> <ul> <li>subscription_plan_list  : <code>List[SubscriptionPlanInfo]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_active_subscription_plan","title":".get_active_subscription_plan","text":"<pre><code>.get_active_subscription_plan()\n</code></pre> <p>Returns the current active subscription plan associated with the user company. Returns None if no active subscription plan found.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Returns</p> <ul> <li>subscription_plan  : <code>SubscriptionPlanInfo | None</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_project","title":".create_project","text":"<pre><code>.create_project(\n   name: str, description: (str|None), default_storage_policy: StoragePolicy\n)\n</code></pre> <p>Create a project inside the company. You don't need to specify the company because a User belongs only to one company and it is retrieved automatically.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : the name of the project</li> <li>description  : optional description of the project</li> <li>default_storage_policy  : represents the default policy to     use for storing data in ML cube Platform</li> </ul> <p>Returns</p> <ul> <li>project_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateProjectException</code></p>"},{"location":"api/python/client/#get_projects","title":".get_projects","text":"<pre><code>.get_projects()\n</code></pre> <p>Get the list of all projects in the company the User has permissions to view.</p> <p>Returns</p> <ul> <li>projects_list  : <code>List[Project]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_project","title":".get_project","text":"<pre><code>.get_project(\n   project_id: str\n)\n</code></pre> <p>Get a project with the given id</p> <p>Allowed Roles</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : project identifier</li> </ul> <p>Returns</p> <ul> <li>project  : <code>Project</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#update_project","title":".update_project","text":"<pre><code>.update_project(\n   project_id: str, name: (str|None), description: (str|None),\n   default_storage_policy: (StoragePolicy|None)\n)\n</code></pre> <p>Update project details.</p> <p>Empty values will not be updated.</p> <p>Allowed Roles</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : project identifier</li> <li>name  : new name of the project</li> <li>description  : new description of the project</li> <li>default_storage_policy  : represents the default policy to     use for storing data in ML cube Platform</li> </ul> <p>Returns</p> <ul> <li>project  : <code>Project</code></li> </ul> <p>Raises</p> <p><code>UpdateProjectException</code></p>"},{"location":"api/python/client/#show_projects","title":".show_projects","text":"<pre><code>.show_projects()\n</code></pre> <p>Show a list all projects printing to stdout.</p> <p>Example output: <pre><code>Project ID                Name\n------------------------  ----------\n6475f8c9ebac5081e529s63f  my project\n</code></pre></p>"},{"location":"api/python/client/#create_task","title":".create_task","text":"<pre><code>.create_task(\n   project_id: str, name: str, tags: list[str], task_type: TaskType,\n   data_structure: DataStructure, cost_info: (TaskCostInfoUnion|None) = None,\n   optional_target: bool = False, text_language: (TextLanguage|None) = None,\n   positive_class: (str|int|bool|None) = None,\n   rag_contexts_separator: (str|None) = None, llm_default_answer: (str|None) = None,\n   semantic_segmentation_target_type: (SemanticSegTargetType|None) = None\n)\n</code></pre> <p>Create a task inside the project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>name  : the name of the task</li> <li>tags  : a list of tags associated with the task</li> <li>task_type  : the type of the task. See <code>TaskType</code>     documentation for more information</li> <li>data_structure  : type of data in the task</li> <li>cost_info  : optional argument that specify the cost     information of the task</li> <li>optional_target  : True if the target value in not always     available. This changes the behaviour and the detection     phase of ML cube Platform that will analyse production     data without considering the actual target</li> <li>text_language  : required for NLP tasks, it specifies the     language used in the task.</li> <li>positive_class  : required for binary classification tasks,     it specifies the positive class of the target.</li> <li>rag_contexts_separator  : Separator used to separate rag contexts     from different sources. If missing then only one source exists.</li> <li>llm_default_answer  : default answer for the LLM model</li> </ul> <p>Returns</p> <ul> <li>task_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateTaskException</code></p>"},{"location":"api/python/client/#update_task","title":".update_task","text":"<pre><code>.update_task(\n   task_id: str, name: (str|None) = None, tags: (list[str]|None) = None,\n   cost_info: (TaskCostInfoUnion|None) = None\n)\n</code></pre> <p>Update task attributes.</p> <p><code>None</code> parameters are ignored for the update.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>name  : the name of the task</li> <li>tags  : a list of tags associated with the task. To remove     all the tags then pass an empty list.</li> <li>cost_info  : optional argument that specify the cost     information of the task</li> </ul> <p>Raises</p> <p><code>UpdateTaskException</code></p>"},{"location":"api/python/client/#get_tasks","title":".get_tasks","text":"<pre><code>.get_tasks(\n   project_id: str\n)\n</code></pre> <p>Get the list of the Tasks inside the project.</p> <p>Args</p> <ul> <li>project_id  : identifier of the project</li> </ul> <p>Returns</p> <ul> <li>task_list  : <code>List[Task]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_task","title":".get_task","text":"<pre><code>.get_task(\n   task_id: str\n)\n</code></pre> <p>Get task by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>task  : <code>Task</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_tasks","title":".show_tasks","text":"<pre><code>.show_tasks(\n   project_id: str\n)\n</code></pre> <p>Show a list of tasks included in a project to stdout.</p> <p>Args</p> <ul> <li>project_id  : the identifier of a project</li> </ul> <p>Example output: <pre><code>Task ID                   Name     Type            Status     Status start date\n------------------------  -------  --------------  --------   -----------------\n6476040d583201813ab4539a  my task  classification  OK         03-02-2023 10:14:06\n</code></pre></p>"},{"location":"api/python/client/#create_model","title":".create_model","text":"<pre><code>.create_model(\n   task_id: str, name: str, version: str, with_probabilistic_output: bool,\n   metric_name: (ModelMetricName|None),\n   preferred_suggestion_type: (SuggestionType|None) = None,\n   retraining_cost: float = 0.0, resampled_dataset_size: (int|None) = None\n)\n</code></pre> <p>Create a model inside the task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>name  : the name of the model</li> <li>version  : the current version of the model</li> <li>metric_name  : performance or error metric associated with     the model</li> <li>retraining_cost  : estimated costs in the Task currency to     retrain the model. This information is used by the     retraining tool to show gain-cost information.     Default value is 0.0 meaning that the cost is negligible</li> <li>preferred_suggestion_type  : preferred type of suggestion that     will be computed to retrain the model</li> <li>resampled_dataset_size  : size of the resampled dataset that     will be proposed to retrain the model     note: this parameter is required if     <code>preferred_suggestion_type</code> is     <code>SuggestionType.RESAMPLED_DATASET</code></li> </ul> <p>Returns</p> <ul> <li>model_id  : <code>str</code> identifier of the created model</li> </ul> <p>Raises</p> <p><code>CreateModelException</code></p>"},{"location":"api/python/client/#create_llm_specs","title":".create_llm_specs","text":"<pre><code>.create_llm_specs(\n   model_id: str, from_timestamp: list[float], to_timestamp: list[float],\n   llm: (str|None) = None, temperature: (float|None) = None, top_p: (float|None) = None,\n   top_k: (int|None) = None, max_tokens: (int|None) = 256, role: (str|None) = None,\n   task: (str|None) = None, behavior_guidelines: (list[str]|None) = None,\n   security_guidelines: (list[str]|None) = None\n)\n</code></pre> <p>Add LLM specs for the LLM model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>from_timestamp  : the timestamp from which the model is used</li> <li>to_timestamp  : the timestamp to which the model is used</li> <li>model_id  : the identifier of the LLM model</li> <li>llm  : the name of the LLM model</li> <li>temperature  : the temperature parameter of the LLM model</li> <li>top_p  : the top_p parameter of the LLM model</li> <li>top_k  : the top_k parameter of the LLM model</li> <li>max_tokens  : the max_tokens parameter of the LLM model</li> <li>role  : role section of the system prompt of the LLM model</li> <li>task  : task section of the system prompt of the LLM model</li> <li>behavior_guidelines  : behavior guidelines section of the system prompt of the LLM model</li> <li>security_guidelines  : security guidelines section of the system prompt of the LLM model</li> </ul> <p>Returns</p> <ul> <li>llm_specs_id  : <code>str</code> identifier of the llm specs created</li> </ul> <p>Raises</p> <p><code>CreateLLMSpecsException</code></p>"},{"location":"api/python/client/#get_all_llm_specs","title":".get_all_llm_specs","text":"<pre><code>.get_all_llm_specs(\n   model_id: str\n)\n</code></pre> <p>Get all LLM specs for the LLM model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the LLM model</li> </ul> <p>Returns</p> <ul> <li>llm_specs_list  : <code>list</code> of all the LLM specs of the LLM model</li> </ul> <p>Raises</p> <p><code>GetAllLLMSpecsException</code></p>"},{"location":"api/python/client/#set_llm_specs","title":".set_llm_specs","text":"<pre><code>.set_llm_specs(\n   model_id: str, llm_specs_id: str, starting_timestamp: float\n)\n</code></pre> <p>Set a LLM specs for the LLM model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the LLM model</li> <li>llm_specs_id  : the identifier of the LLM model specs</li> <li>starting_timestamp  : the starting timestamp for the LLM specs specified</li> </ul> <p>Raises</p> <p><code>SetLLMSpecsException</code></p>"},{"location":"api/python/client/#get_models","title":".get_models","text":"<pre><code>.get_models(\n   task_id: str\n)\n</code></pre> <p>Get all models of a task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>models_list  : <code>List[Model]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_model","title":".get_model","text":"<pre><code>.get_model(\n   model_id: str\n)\n</code></pre> <p>Get model by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : identifier of the model</li> </ul> <p>Returns</p> <ul> <li>model  : <code>Model</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_model_by_name_and_version","title":".get_model_by_name_and_version","text":"<pre><code>.get_model_by_name_and_version(\n   task_id: str, model_name: str, model_version: str\n)\n</code></pre> <p>Get model by name and version.</p> <p>A Model can have multiple versions according to the updates and retraining done. This method allow to get the Model object by specifying its name and the version tag.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>model_name  : the name of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Returns</p> <ul> <li>model  : <code>Model</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_models","title":".show_models","text":"<pre><code>.show_models(\n   task_id: str\n)\n</code></pre> <p>Show a list of models included in a task to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Example output: <pre><code>Model Id                  Task Id                   Name                    Version    Status           Status start timestamp    Status insert date          Metric Name\n------------------------  ------------------------  ----------------------  ---------  ---------------  ------------------------  --------------------------  --------------------\n64fecf7d323311ab78f17280  64fecf7c323311ab78f17262  model_local_experiment  v0.0.1     ok                                         2023-09-11 08:27:41.431000  ModelMetricName.RMSE\n</code></pre></p>"},{"location":"api/python/client/#get_suggestions_info","title":".get_suggestions_info","text":"<pre><code>.get_suggestions_info(\n   model_id: str, model_version: str\n)\n</code></pre> <p>Retrieve suggestions associated with a model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Returns</p> <ul> <li>suggestion_info_list  : <code>List[SuggestionInfo]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_suggestions","title":".show_suggestions","text":"<pre><code>.show_suggestions(\n   model_id: str, model_version: str\n)\n</code></pre> <p>Show the list of suggestions associated with a model printing them to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Example output: <pre><code>Suggestion Id                     Executed    Timestamp\n--------------------------------  ----------  --------------------------\n79a8710c351c4b6a9ece7322e153f200  True        2023-08-21 10:54:40.386189\n</code></pre></p> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_model_suggestion_type","title":".set_model_suggestion_type","text":"<pre><code>.set_model_suggestion_type(\n   model_id: str, preferred_suggestion_type: SuggestionType,\n   resampled_dataset_size: (int|None) = None\n)\n</code></pre> <p>Set model suggestion type.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the task</li> <li>preferred_suggestion_type  : preferred type of suggestion that     will be computed to retrain the model</li> <li>resampled_dataset_size  : size of the resampled dataset that     will be proposed to retrain the model     note: this parameter is required if     <code>preferred_suggestion_type</code> is     <code>SuggestionType.RESAMPLED_DATASET</code></li> </ul> <p>Raises</p> <p><code>SetModelSuggestionTypeException</code></p>"},{"location":"api/python/client/#update_model_version_by_suggestion_id","title":".update_model_version_by_suggestion_id","text":"<pre><code>.update_model_version_by_suggestion_id(\n   model_id: str, new_model_version: str, suggestion_id: str\n)\n</code></pre> <p>Update model version by suggestion id. To retrain the Model, ML cube Platform provides importance weights through a <code>SuggestionInfo</code>. After the retraining is completed, you use this method to create the new model version in ML cube Platform. By specifying the <code>suggestion_id</code>, ML cube Platform automatically knows which is the reference data the model is trained on.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model</li> <li>suggestion_id  : the identifier of the suggestion</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> job identifier of the pipeline in execution</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p>"},{"location":"api/python/client/#update_model_version","title":".update_model_version","text":"<pre><code>.update_model_version(\n   model_id: str, new_model_version: str\n)\n</code></pre> <p>Update model version with empty reference data. It is similar to create model but it does not actually create a different Model but a new version of the existent one. After this request, it is possible to upload data with add_historical_data. To start monitoring is required to set the reference of this new model with set_reference request.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model</li> </ul> <p>Returns</p> <ul> <li>model_id  : <code>str</code> job identifier of new model id</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p>"},{"location":"api/python/client/#update_model_version_from_time_range","title":".update_model_version_from_time_range","text":"<pre><code>.update_model_version_from_time_range(\n   model_id: str, new_model_version: str, default_reference: ReferenceInfo,\n   segment_references: (list[ReferenceInfo]|None) = None\n)\n</code></pre> <p>Update model version by specifying the time range of uploaded data on ML cube Platform.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model</li> <li>default_reference  : the list of time intervals used as default reference</li> <li>segment_references  : list of segment specific time intervals used as reference for the segments</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p> <p>Returns the job_id associated to the pipeline</p>"},{"location":"api/python/client/#add_data_schema","title":".add_data_schema","text":"<pre><code>.add_data_schema(\n   task_id: str, data_schema: DataSchema\n)\n</code></pre> <p>Associate a data schema to a task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>data_schema  : the data schema that characterize your task</li> </ul> <p>Raises</p> <p><code>AddDataSchemaException</code></p>"},{"location":"api/python/client/#get_data_schema","title":".get_data_schema","text":"<pre><code>.get_data_schema(\n   task_id: str\n)\n</code></pre> <p>Get task data schema</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>data_schema  : <code>DataSchema</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_data_schema_template","title":".get_data_schema_template","text":"<pre><code>.get_data_schema_template(\n   task_id: str\n)\n</code></pre> <p>Get task data schema</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>data_schema  : <code>DataSchema</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_data_schema","title":".show_data_schema","text":"<pre><code>.show_data_schema(\n   task_id: str\n)\n</code></pre> <p>Show data schema of associated with a task</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p> <p>Example output:</p> <pre><code>Column name       Role     Type      Nullable\n----------------  -------  --------  ----------\nsample_id         id       string    False\ntimestamp         time_id  string    False\nsepallength       input    float     False\nsepalwidth        input    float     False\npetallength       input    float     False\npetalwidth        input    float     False\nclass             target   category  False\n</code></pre>"},{"location":"api/python/client/#add_historical_data","title":".add_historical_data","text":"<pre><code>.add_historical_data(\n   task_id: str, inputs: Data, metadata: (Data|None) = None, target: (Data|None) = None,\n   predictions: (list[tuple[str, Data]]|None) = None\n)\n</code></pre> <p>Add a batch of historical data for the Task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>inputs  : data object that contain input data source.     It can be None if you upload other kinds of data</li> <li>metadata  : data object that contain metadata data source.     It can be None if you upload other kinds of data</li> <li>target  : data object that contains target data.     It can be None if you upload other kinds of data</li> <li>predictions  : list of data objects that contain prediction data.     Each element is a tuple with model_id and data object.     It can be None if you upload other kinds of data.     Predictions are mandatory for RAG tasks while not     permitted for the other TaskType.</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddHistoricalDataException</code></p>"},{"location":"api/python/client/#add_target_data","title":".add_target_data","text":"<pre><code>.add_target_data(\n   task_id: str, target: Data\n)\n</code></pre> <p>Add target samples for data already uploaded on the Task. This operation is used for Tasks with optional target which is manually labelled. For instance, fter the labelling process (maybe with our Active Learning module) you have a set of labelled samples spread over all the uploaded data. Indeed, they can belong to different data batches (historical or production consistent uploads) and can be a subset of the uploaded data.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>target  : data object that contains target data</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddHistoricalDataException</code></p>"},{"location":"api/python/client/#set_model_reference","title":".set_model_reference","text":"<pre><code>.set_model_reference(\n   model_id: str, default_reference: ReferenceInfo,\n   segment_references: (list[ReferenceInfo]|None) = None\n)\n</code></pre> <p>Specify data to use as reference for the model with time range. Data need to be already uploaded on ML cube Platform.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>default_reference  : the list of time intervals used as default reference, they are combined with OR operator</li> <li>segment_references  : a dictionary containing for each specified segments constraints on the time intervals,     they are combined with OR operator among themselves and with AND operator with the time intervals.</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddModelReferenceException</code></p>"},{"location":"api/python/client/#add_production_data","title":".add_production_data","text":"<pre><code>.add_production_data(\n   task_id: str, inputs: (Data|None) = None, metadata: (Data|None) = None,\n   target: (Data|None) = None, predictions: (list[tuple[str, Data]]|None) = None\n)\n</code></pre> <p>Add a batch of production data associated with a given task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>inputs  : data object that contain input data source.     It can be None if you upload other kinds of data</li> <li>metadata  : data object that contain metadata data source.     It can be None if you upload other kinds of data</li> <li>target  : data object that contains target data.     It can be None if you upload other kinds of data</li> <li>predictions  : list of data objects that contain prediction data.     Each element is a tuple with model_id and data object.     It can be None if you upload other kinds of data</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddProductionDataException</code></p>"},{"location":"api/python/client/#create_kpi","title":".create_kpi","text":"<pre><code>.create_kpi(\n   project_id: str, name: str\n)\n</code></pre> <p>Create a KPI.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>name  : the name of the kpi</li> </ul> <p>Returns</p> <ul> <li>kpi_id  : <code>str</code> identifier of the created kpi</li> </ul> <p>Raises</p> <p><code>CreateKpiException</code></p>"},{"location":"api/python/client/#get_kpi","title":".get_kpi","text":"<pre><code>.get_kpi(\n   kpi_id: str\n)\n</code></pre> <p>Get kpi by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>kpi_id  : identifier of the kpi</li> </ul> <p>Returns</p> <ul> <li>kpi  : <code>KPI</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_kpis","title":".get_kpis","text":"<pre><code>.get_kpis(\n   project_id: str\n)\n</code></pre> <p>Get all kpis of a project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : identifier of the project</li> </ul> <p>Returns</p> <ul> <li>kpis_list  : <code>List[KPI]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_kpis","title":".show_kpis","text":"<pre><code>.show_kpis(\n   project_id: str\n)\n</code></pre> <p>Show the list of KPIs included in a project to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> </ul> <p>Example output: <pre><code>KPI Id                    Project Id                Name                    Status           Status start timestamp    Status insert date\n------------------------  ------------------------  ----------------------  ---------------  ------------------------  --------------------------\n64fecf7d323311ab78f17280  64fecf7c323311ab78f17262  model_local_experiment  not_initialized                            2023-09-11 08:27:41.431000\n</code></pre></p>"},{"location":"api/python/client/#add_kpi_data","title":".add_kpi_data","text":"<pre><code>.add_kpi_data(\n   project_id: str, kpi_id: str, kpi: TabularData\n)\n</code></pre> <p>Add a batch of a given kpi with the given project.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>kpi_id  : the identifier of the kpi</li> <li>kpi  : data object that contains data source.</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddKPIDataException</code></p>"},{"location":"api/python/client/#compute_retraining_report","title":".compute_retraining_report","text":"<pre><code>.compute_retraining_report(\n   model_id: str\n)\n</code></pre> <p>Compute the retraining report for a given model</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>ComputeRetrainingReportException</code></p>"},{"location":"api/python/client/#get_retraining_report","title":".get_retraining_report","text":"<pre><code>.get_retraining_report(\n   model_id: str\n)\n</code></pre> <p>For a given model id, get the sample weights computed and additional information about them included in the retraining report</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> </ul> <p>Returns</p> <ul> <li>retraining_report  : <code>RetrainingReport</code></li> </ul> <p>Raises</p> <p><code>GetRetrainingReportException</code></p>"},{"location":"api/python/client/#compute_rag_evaluation_report","title":".compute_rag_evaluation_report","text":"<pre><code>.compute_rag_evaluation_report(\n   task_id: str, report_name: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Compute the RAG evaluation report for a given task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles: - At least <code>PROJECT_EDIT</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>report_name  : the name of the report</li> <li>from_timestamp  : start timestamp of the samples to evaluate</li> <li>to_timestamp  : end timestamp of the samples to evaluate</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p>ComputeRagEvaluationReportException</p>"},{"location":"api/python/client/#get_rag_evaluation_reports","title":".get_rag_evaluation_reports","text":"<pre><code>.get_rag_evaluation_reports(\n   task_id: str\n)\n</code></pre> <p>For a given task id, get the computed RAG evaluation reports.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Returns</p> <ul> <li>rag_eval_reports  : list[TaskRagEvalReportItem]</li> </ul>"},{"location":"api/python/client/#export_rag_evaluation_report","title":".export_rag_evaluation_report","text":"<pre><code>.export_rag_evaluation_report(\n   report_id: str, folder: str, file_name: str\n)\n</code></pre> <p>Export a RAG evaluation report to a file.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>report_id  : the identifier of the report</li> <li>folder  : the folder where the report will be saved</li> <li>file_name  : the name of the file where the report will be saved</li> </ul> <p>Returns</p> <p>None</p>"},{"location":"api/python/client/#compute_topic_modeling_report","title":".compute_topic_modeling_report","text":"<pre><code>.compute_topic_modeling_report(\n   task_id: str, report_name: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Compute the topic modeling report for a given task. This functionality is available only for tasks based on text data. The data on which the report is computed is determined by the timestamps.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles: - At least <code>PROJECT_EDIT</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>report_name  : the name of the report</li> <li>from_timestamp  : start timestamp of the samples to evaluate</li> <li>to_timestamp  : end timestamp of the samples to evaluate</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p>ComputeTopicModelingReportException</p>"},{"location":"api/python/client/#get_topic_modeling_reports","title":".get_topic_modeling_reports","text":"<pre><code>.get_topic_modeling_reports(\n   task_id: str\n)\n</code></pre> <p>For a given task id, get the computed topic modeling reports.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Returns</p> <ul> <li>topic_modeling_reports  : list[TaskTopicModelingReportItem]</li> </ul>"},{"location":"api/python/client/#get_topic_modeling_report","title":".get_topic_modeling_report","text":"<pre><code>.get_topic_modeling_report(\n   report_id: str\n)\n</code></pre> <p>For a given task id, get the computed topic modeling report for a specific report id.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>report_id  : the identifier of the report</li> </ul> <p>Returns</p> <ul> <li>topic_modeling_report_detail  : TaskTopicModelingReportDetails</li> </ul>"},{"location":"api/python/client/#get_monitoring_status","title":".get_monitoring_status","text":"<pre><code>.get_monitoring_status(\n   task_id: str, monitoring_target: MonitoringTarget,\n   monitoring_metric: (MonitoringMetric|None) = None,\n   specification: (str|None) = None, segment_id: (str|None) = None\n)\n</code></pre> <p>Get the monitoring status of a monitoring target (or metric) in a task. A list is returned because some monitoring targets or metrics can have an additional field named specification.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>monitoring_target  : the type of monitoring target to get the status</li> <li>monitoring_metric  : the type of monitoring metric to get the status     If <code>None</code>, only the monitoring target is considered</li> <li>segment_id  : the identifier of the segment. None if referring to the whole     population</li> </ul> <p>Returns</p> <p><code>MonitoringQuantityStatus</code></p> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_jobs","title":".get_jobs","text":"<pre><code>.get_jobs(\n   project_id: (str|None) = None, task_id: (str|None) = None,\n   model_id: (str|None) = None, status: (JobStatus|None) = None,\n   job_id: (str|None) = None\n)\n</code></pre> <p>Get current jobs information. Jobs can be filtered by project_id, task_id, model_id or status.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the project_id to filter job.     If <code>None</code> job of every project will be returned</li> <li>task_id  : the task_id to filter job.     If <code>None</code> job of every task will be returned</li> <li>model_id  : the model_id to filter job.     If <code>None</code> job of every model will be returned</li> <li>status  : the status to filter job.     If <code>None</code> job with every status will be retrieved</li> <li>job_id  : id of the job to filter.     If <code>None</code> job with every id will be retrieved</li> </ul> <p>Returns</p> <ul> <li>jobs_list  : <code>List[Job]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_job","title":".get_job","text":"<pre><code>.get_job(\n   job_id: str\n)\n</code></pre> <p>Get current job information.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>job_id  : id of the job to retrieve</li> </ul> <p>Returns</p> <ul> <li>job  : <code>Job</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_jobs","title":".show_jobs","text":"<pre><code>.show_jobs()\n</code></pre> <p>Show current job information to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_events","title":".get_detection_events","text":"<pre><code>.get_detection_events(\n   task_id: str\n)\n</code></pre> <p>Get all detection event of a given task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : id of the task for which you want to retrieve the detection event</li> </ul> <p>Returns</p> <ul> <li>rules_list  : <code>List[DetectionEvent]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_event_rules","title":".get_detection_event_rules","text":"<pre><code>.get_detection_event_rules(\n   task_id: str\n)\n</code></pre> <p>Get all detection event rules of a given task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : id of the task for which you want to retrieve the detection event rules</li> </ul> <p>Returns</p> <ul> <li>rules_list  : <code>List[DetectionEventRule]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_event_rule","title":".get_detection_event_rule","text":"<pre><code>.get_detection_event_rule(\n   rule_id: str\n)\n</code></pre> <p>Get a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : id of the rule</li> </ul> <p>Returns</p> <ul> <li>rule  : <code>DetectionEventRule</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_detection_event_user_feedback","title":".set_detection_event_user_feedback","text":"<pre><code>.set_detection_event_user_feedback(\n   detection_id: str, user_feedback: bool\n)\n</code></pre> <p>Get a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>detection_id  : id of the detection event</li> <li>user_feedback  : user feedback</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_detection_event_rule","title":".create_detection_event_rule","text":"<pre><code>.create_detection_event_rule(\n   name: str, task_id: str, severity: (DetectionEventSeverity|None),\n   detection_event_type: DetectionEventType, monitoring_target: MonitoringTarget,\n   actions: list[DetectionEventAction],\n   monitoring_metric: (MonitoringMetric|None) = None, model_name: (str|None) = None,\n   segment_id: (str|None) = None\n)\n</code></pre> <p>Create a detection event rule.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : the name of the rule</li> <li>task_id  : the id of the task to which the rule belongs.     The rule will only respond to detection events     generated by this task.</li> <li>model_name  : the name of the model, only required if     monitoring_target is set to MODEL.</li> <li>detection_event_type  : the type of detection event that     this rule should respond to.</li> <li>monitoring_target  : the type of monitoring target that     this rule should respond to.</li> <li>monitoring_metric  : additional metric extracted from     monitoring target that is monitored</li> <li>severity  : the level of severity of the detection event     that this rule should respond to. None means any     severity is matched.</li> <li>actions  : the list of actions to execute, in order,     when the conditions of the rule are matched.</li> <li>segment_id  : the segment id on which the event should take     place to trigger the rule. If None, the rule will     be triggered on events related to the whole population</li> </ul> <p>Returns</p> <ul> <li>rule_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateDetectionEventRuleException</code></p>"},{"location":"api/python/client/#update_detection_event_rule","title":".update_detection_event_rule","text":"<pre><code>.update_detection_event_rule(\n   rule_id: str, name: (str|None) = None, model_name: (str|None) = None,\n   severity: (DetectionEventSeverity|None) = None,\n   detection_event_type: (DetectionEventType|None) = None,\n   monitoring_target: (MonitoringTarget|None) = None,\n   actions: (list[DetectionEventAction]|None) = None\n)\n</code></pre> <p>Update a detection event rule.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : the id of the rule to update</li> <li>name  : the name of the rule. If None, keeps the existing value.</li> <li>model_name  : the name of the model, only required if     monitoring_target is set to MODEL.     If None, keeps the existing value.</li> <li>detection_event_type  : the type of detection event that this     rule should respond to. If None, keeps the existing value.</li> <li>monitoring_target  : the type of monitoring target that this     rule should respond to. If None, keeps the existing value.</li> <li>severity  : the level of severity of the detection event that     this rule should respond to. If None, keeps the     existing value.</li> <li>actions  : the list of actions to execute, in order, when the     conditions of the rule are matched. If None,      keeps the existing value.</li> </ul> <p>Raises</p> <p><code>CreateDetectionEventRuleException</code></p>"},{"location":"api/python/client/#delete_detection_event_rule","title":".delete_detection_event_rule","text":"<pre><code>.delete_detection_event_rule(\n   rule_id: str\n)\n</code></pre> <p>Delete a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : id of the rule to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#wait_job_completion","title":".wait_job_completion","text":"<pre><code>.wait_job_completion(\n   job_id: str, max_wait_timeout: int = 3000\n)\n</code></pre> <p>Wait that the ML cube Platform job terminates successfully its execution.</p> <p>Note that this method stops the execution.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>job_id  : identifier of the job</li> <li>max_wait_timeout  : maximum amount of seconds to wait before     launching <code>JobWaitTimeoutException</code></li> </ul> <p>Raises</p> <ul> <li><code>JobWaitTimeoutException</code> when the maximum timeout time     is reached</li> <li><code>JobNotFoundException</code> when the requested job does not     exist</li> <li><code>JobFailureException</code> when the requested job is failed</li> </ul>"},{"location":"api/python/client/#create_company_user","title":".create_company_user","text":"<pre><code>.create_company_user(\n   name: str, surname: str, username: str, password: str, email: str,\n   company_role: UserCompanyRole\n)\n</code></pre> <p>Creates a new User in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> </ul> <p>Args</p> <ul> <li>name  : name of the user</li> <li>surname  : surname of the user</li> <li>username  : username of the user</li> <li>password  : temporary password for the user. It will change     this at the first login</li> <li>email  : email of the user</li> <li>company_role  : role of the user inside the company</li> </ul> <p>Returns</p> <ul> <li>user_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_company_users","title":".get_company_users","text":"<pre><code>.get_company_users()\n</code></pre> <p>Returns the list of users in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>users_list  : <code>List[CompanyUser]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#change_user_company_role","title":".change_user_company_role","text":"<pre><code>.change_user_company_role(\n   user_id: str, company_role: UserCompanyRole\n)\n</code></pre> <p>Change the company role of a user in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code>: can change the roles of all the Users     Users apart from other Admins and the Owner</li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which the role is updated</li> <li>company_role  : the new role to assign</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_company_users","title":".show_company_users","text":"<pre><code>.show_company_users()\n</code></pre> <p>Show company users to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_user_projects","title":".get_user_projects","text":"<pre><code>.get_user_projects(\n   user_id: str\n)\n</code></pre> <p>Returns a list of projects that the user can view.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> </ul> <p>Returns</p> <ul> <li>projects_list  : <code>List[Project]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_user_projects","title":".show_user_projects","text":"<pre><code>.show_user_projects(\n   user_id: str\n)\n</code></pre> <p>Shows the projects that the user can view to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#add_user_project_role","title":".add_user_project_role","text":"<pre><code>.add_user_project_role(\n   user_id: str, project_id: str, project_role: UserProjectRole\n)\n</code></pre> <p>Add a project role to the user for the given project.</p> <p>The User Project role can be assigned only to <code>COMPANY_USER</code> because Admin and Owner already have all the permission over projects.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> <li>project_id  : identifies the project</li> <li>project_role  : the project role to assign</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_project_role","title":".delete_project_role","text":"<pre><code>.delete_project_role(\n   user_id: str, project_id: str\n)\n</code></pre> <p>Delete the role of the user for the given project.</p> <p>The User Project role can be deleted only for <code>COMPANY_USER</code> because Admin and Owner have all the permission over projects.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> <li>project_id  : identifies the project</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_api_keys","title":".get_api_keys","text":"<pre><code>.get_api_keys()\n</code></pre> <p>Returns a list of api keys the user has.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>api_keys_list  : <code>List[ApiKey]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_api_keys","title":".show_api_keys","text":"<pre><code>.show_api_keys()\n</code></pre> <p>Shows the list of api keys the user has to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_api_key","title":".create_api_key","text":"<pre><code>.create_api_key(\n   name: str, expiration_time: ApiKeyExpirationTime\n)\n</code></pre> <p>Create a new api key for the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>name  : the name of the api key</li> <li>expiration_time  : the expiration time of the api key</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_api_key","title":".delete_api_key","text":"<pre><code>.delete_api_key(\n   api_key: str\n)\n</code></pre> <p>Delete the api key of the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>api_key  : api key to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_user_api_keys","title":".get_user_api_keys","text":"<pre><code>.get_user_api_keys(\n   user_id: str\n)\n</code></pre> <p>Get the list of api keys a user has.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to get his api keys</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_user_api_keys","title":".show_user_api_keys","text":"<pre><code>.show_user_api_keys(\n   user_id: str\n)\n</code></pre> <p>Shows the list of api keys a user has to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to get his api keys</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_user_api_key","title":".create_user_api_key","text":"<pre><code>.create_user_api_key(\n   user_id: str, name: str, expiration_time: ApiKeyExpirationTime\n)\n</code></pre> <p>Create a new api key for the user.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to create a new api key</li> <li>name  : the name of the api key</li> <li>expiration_time  : the expiration time of the api key</li> </ul> <p>Returns</p> <ul> <li>api_key  : the new created api key for the user</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_user_api_key","title":".delete_user_api_key","text":"<pre><code>.delete_user_api_key(\n   user_id: str, api_key: str\n)\n</code></pre> <p>Delete the api key of the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code>     Admin</li> </ul> <p>Args</p> <ul> <li>user_id  : the user to delete an api key</li> <li>api_key  : the api key to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#change_company_owner","title":".change_company_owner","text":"<pre><code>.change_company_owner(\n   user_id: str\n)\n</code></pre> <p>Change the company owner role from the requesting user to the other user.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user that become Company Owner</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_company_user","title":".delete_company_user","text":"<pre><code>.delete_company_user(\n   user_id: str\n)\n</code></pre> <p>Delete a user from the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code>: cannot delete other company admins</li> </ul> <p>Args</p> <ul> <li>user_id  : the user to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_integration_credentials","title":".get_integration_credentials","text":"<pre><code>.get_integration_credentials(\n   credentials_id: str\n)\n</code></pre> <p>Get the credentials with the given id for 3<sup>rd</sup> party service provider integration.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to retrieve.</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>IntegrationCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_all_project_integration_credentials","title":".get_all_project_integration_credentials","text":"<pre><code>.get_all_project_integration_credentials(\n   project_id: str\n)\n</code></pre> <p>Get the list of credentials for 3<sup>rd</sup> party service provider integrations that are currently configured in a project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : id of the project for which all configured credentials should be retrieved.</li> </ul> <p>Returns</p> <ul> <li>credentials_list  : <code>List[IntegrationCredentials]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_integration_credentials","title":".delete_integration_credentials","text":"<pre><code>.delete_integration_credentials(\n   credentials_id: str\n)\n</code></pre> <p>Delete credentials for the integration with a 3<sup>rd</sup> party  service provider.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code>: cannot delete other company admins</li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to delete.</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_integration_credentials_as_default","title":".set_integration_credentials_as_default","text":"<pre><code>.set_integration_credentials_as_default(\n   credentials_id: str\n)\n</code></pre> <p>Set the credentials with the given id as default for 3<sup>rd</sup> party  service provider integration.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to set as default.</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_aws_integration_credentials","title":".create_aws_integration_credentials","text":"<pre><code>.create_aws_integration_credentials(\n   name: str, default: bool, project_id: str, role_arn: str\n)\n</code></pre> <p>Create credentials to integrate with AWS. Returns an object that contains the trust policy that you need to set on the IAM role on AWS.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>role_arn  : the ARN of the IAM role that will be assumed by ML     cube Platform</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>SecretAWSCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_aws_compatible_integration_credentials","title":".create_aws_compatible_integration_credentials","text":"<pre><code>.create_aws_compatible_integration_credentials(\n   name: str, default: bool, project_id: str, access_key_id: str,\n   secret_access_key: str, endpoint_url: (str|None) = None\n)\n</code></pre> <p>Create credentials to integrate with AWS-compatible services.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>access_key_id  : the access key id</li> <li>secret_access_key  : the secret access key</li> <li>endpoint_url  : the endpoint url of the service. If None,     AWS itself is used</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>AWSCompatibleCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_gcp_integration_credentials","title":".create_gcp_integration_credentials","text":"<pre><code>.create_gcp_integration_credentials(\n   name: str, default: bool, project_id: str, service_account_info_json: str\n)\n</code></pre> <p>Create credentials to integrate with GCP.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>service_account_info_json  : the json-encoded string     containing the key of the service account</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>GCPCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_azure_integration_credentials","title":".create_azure_integration_credentials","text":"<pre><code>.create_azure_integration_credentials(\n   name: str, default: bool, project_id: str,\n   service_principal_credentials_json: str\n)\n</code></pre> <p>Create credentials to integrate with Azure.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>service_principal_credentials_json  : the json-encoded string     containing the credentials of the service principal</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>AzureCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_retrain_trigger","title":".set_retrain_trigger","text":"<pre><code>.set_retrain_trigger(\n   model_id: str, trigger: (RetrainTrigger|None)\n)\n</code></pre> <p>Set the retrain trigger for a given model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> <li>trigger  : the trigger to set. If you want to remove the     trigger, set it to None</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#test_retrain_trigger","title":".test_retrain_trigger","text":"<pre><code>.test_retrain_trigger(\n   model_id: str, trigger: RetrainTrigger\n)\n</code></pre> <p>Test the retrain trigger for a given model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> <li>trigger  : the trigger to test</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#retrain_model","title":".retrain_model","text":"<pre><code>.retrain_model(\n   model_id: str\n)\n</code></pre> <p>Retrain a model via the configured retrain trigger.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#compute_llm_security_report","title":".compute_llm_security_report","text":"<pre><code>.compute_llm_security_report(\n   task_id: str, report_name: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Compute the LLM Security report for a given task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles: - At least <code>PROJECT_EDIT</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>report_name  : the name of the report</li> <li>from_timestamp  : start timestamp of the samples to evaluate</li> <li>to_timestamp  : end timestamp of the samples to evaluate</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p>ComputeLlmSecurityReportException</p>"},{"location":"api/python/client/#get_llm_security_reports","title":".get_llm_security_reports","text":"<pre><code>.get_llm_security_reports(\n   task_id: str\n)\n</code></pre> <p>For a given task id, get the computed LLM Security reports.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Returns</p> <ul> <li>llm_sec_reports  : list[TaskLlmSecReportItem]</li> </ul> <p>Raises</p> <p>GetLlmSecurityReportException</p>"},{"location":"api/python/client/#get_all_task_segments","title":".get_all_task_segments","text":"<pre><code>.get_all_task_segments(\n   task_id: str\n)\n</code></pre> <p>Get all segments for the given task.</p> <p>Args</p> <ul> <li>task_id  : The task id.</li> </ul> <p>Returns</p> <p>A list of Segment objects</p>"},{"location":"api/python/client/#create_task_segments","title":".create_task_segments","text":"<pre><code>.create_task_segments(\n   task_id: str, segments: list[Segment]\n)\n</code></pre> <p>Create segments for the given task.</p> <p>Args</p> <ul> <li>task_id  : The task id.</li> <li>segments  : A list of Segment objects.</li> </ul> <p>Returns</p> <p>The ids of the created segments.</p>"},{"location":"api/python/client/#get_data_batch_list","title":".get_data_batch_list","text":"<pre><code>.get_data_batch_list(\n   task_id: str\n)\n</code></pre> <p>Get all data batches for the given task.</p> <p>Args</p> <ul> <li>task_id  : The task id.</li> </ul> <p>Returns</p> <p>A list of DataBatch objects</p>"},{"location":"api/python/enums/","title":"Enums","text":""},{"location":"api/python/enums/#apikeyexpirationtime","title":"ApiKeyExpirationTime","text":"<pre><code>ApiKeyExpirationTime()\n</code></pre> <p>Fields:</p> <ul> <li>ONE_MONTH</li> <li>THREE_MONTHS</li> <li>SIX_MONTHS</li> <li>ONE_YEAR</li> <li>NEVER</li> </ul>"},{"location":"api/python/enums/#baseml3enum","title":"BaseML3Enum","text":"<pre><code>BaseML3Enum()\n</code></pre> <p>Base class for all enums in the ML3 Platform SDK</p>"},{"location":"api/python/enums/#booleanlicencefeature","title":"BooleanLicenceFeature","text":"<pre><code>BooleanLicenceFeature()\n</code></pre> <p>Boolean licence feature</p> <p>Fields: - EXPLAINABILITY     Whether the company has access to explainability reports - MONITORING     Whether the company has monitoring feature enabled - MONITORING_METRICS     Whether the company has monitoring metrics feature enabled - SEGMENTED_MONITORING     Whether the company has segmented monitoring feature enabled - RETRAINING     Whether the company has retraining feature enabled - TOPIC_ANALYSIS     Whether the company has topic analysis feature enabled - RAG_EVALUATION     Whether the company has RAG evaluation feature enabled - LLM_SECURITY     Whether the company has LLM security feature enabled - BUSINESS     Whether the company has business feature enabled</p>"},{"location":"api/python/enums/#columnrole","title":"ColumnRole","text":"<pre><code>ColumnRole()\n</code></pre> <p>Column role enum Describe the role of a column</p> <p>Fields: - INPUT - INPUT_MASK - METADATA - PREDICTION - TARGET - ERROR - ID - TIME_ID - INPUT_ADDITIONAL_EMBEDDING - TARGET_ADDITIONAL_EMBEDDING - PREDICTION_ADDITIONAL_EMBEDDING - USER_INPUT - RETRIEVED_CONTEXT</p>"},{"location":"api/python/enums/#columnsubrole","title":"ColumnSubRole","text":"<pre><code>ColumnSubRole()\n</code></pre> <p>Column subrole enum Describe the subrole of a column</p> <p>Subroles for ColumnRole.INPUT in RAG settings:</p> <ul> <li>RAG_USER_INPUT</li> <li>RAG_RETRIEVED_CONTEXT</li> <li>RAG_SYS_PROMPT</li> </ul> <p>Subroles for ColumnRole.PREDICTION:</p> <ul> <li>MODEL_PROBABILITY</li> <li>OBJECT_LABEL_PREDICTION</li> </ul> <p>Subroles for ColumnRole.TARGET:</p> <ul> <li>OBJECT_LABEL_TARGET</li> </ul>"},{"location":"api/python/enums/#currency","title":"Currency","text":"<pre><code>Currency()\n</code></pre> <p>Currency of to use for the Task</p> <p>Fields: - EURO - DOLLAR</p>"},{"location":"api/python/enums/#datastructure","title":"DataStructure","text":"<pre><code>DataStructure()\n</code></pre> <p>Represents the typology of the data to send</p> <p>Fields:</p> <ul> <li>TABULAR</li> <li>IMAGE</li> <li>TEXT</li> <li>EMBEDDING</li> </ul>"},{"location":"api/python/enums/#datatype","title":"DataType","text":"<pre><code>DataType()\n</code></pre> <p>Data type enum Describe data type of input</p> <p>Fields: - FLOAT - STRING - CATEGORICAL - ARRAY_1 - ARRAY_2 - ARRAY_3</p>"},{"location":"api/python/enums/#detectioneventactiontype","title":"DetectionEventActionType","text":"<pre><code>DetectionEventActionType()\n</code></pre> <p>Fields:</p> <ul> <li>DISCORD_NOTIFICATION</li> <li>SLACK_NOTIFICATION</li> <li>EMAIL_NOTIFICATION</li> <li>TEAMS_NOTIFICATION</li> <li>MQTT_NOTIFICATION</li> <li>RETRAIN</li> <li>NEW_PLOT_CONFIGURATION</li> </ul>"},{"location":"api/python/enums/#detectioneventseverity","title":"DetectionEventSeverity","text":"<pre><code>DetectionEventSeverity()\n</code></pre> <p>Fields:</p> <ul> <li>LOW</li> <li>MEDIUM</li> <li>HIGH</li> </ul>"},{"location":"api/python/enums/#detectioneventtype","title":"DetectionEventType","text":"<pre><code>DetectionEventType()\n</code></pre> <p>Fields:</p> <ul> <li>WARNING_OFF</li> <li>WARNING_ON</li> <li>DRIFT_ON</li> <li>DRIFT_OFF</li> </ul>"},{"location":"api/python/enums/#externalintegration","title":"ExternalIntegration","text":"<pre><code>ExternalIntegration()\n</code></pre> <p>An integration with a 3<sup>rd</sup> party service provider</p> <p>Fields: - AWS - GCP - AZURE - AWS_COMPATIBLE</p>"},{"location":"api/python/enums/#filetype","title":"FileType","text":"<pre><code>FileType()\n</code></pre> <p>Fields:</p> <ul> <li>CSV</li> <li>JSON</li> <li>PARQUET</li> <li>PNG</li> <li>JPG</li> <li>NPY</li> </ul>"},{"location":"api/python/enums/#foldertype","title":"FolderType","text":"<pre><code>FolderType()\n</code></pre> <p>Type of folder</p> <p>Fields</p> <ul> <li>UNCOMPRESSED</li> <li>TAR</li> <li>ZIP</li> </ul>"},{"location":"api/python/enums/#imagemode","title":"ImageMode","text":"<pre><code>ImageMode()\n</code></pre> <p>Image mode enumeration</p> <p>Fields: - RGB - RGBA - GRAYSCALE</p>"},{"location":"api/python/enums/#jobstatus","title":"JobStatus","text":"<pre><code>JobStatus()\n</code></pre> <p>Enum containing all the job's status that a client can see</p> <p>Fields:</p> <ul> <li>IDLE</li> <li>STARTING</li> <li>RUNNING</li> <li>COMPLETED</li> <li>ERROR</li> </ul>"},{"location":"api/python/enums/#kpistatus","title":"KPIStatus","text":"<pre><code>KPIStatus()\n</code></pre> <p>Fields:</p> <ul> <li>NOT_INITIALIZED</li> <li>OK</li> <li>WARNING</li> <li>DRIFT</li> </ul>"},{"location":"api/python/enums/#modelmetricname","title":"ModelMetricName","text":"<pre><code>ModelMetricName()\n</code></pre> <p>Name of the model metrics that is associated with the model</p> <p>Fields: - RMSE - RSQUARE - ACCURACY - AVERAGE_PRECISION</p>"},{"location":"api/python/enums/#monitoringmetric","title":"MonitoringMetric","text":"<pre><code>MonitoringMetric()\n</code></pre> <p>Tabular: - FEATURE</p> <p>Text:     - TEXT_TOXICITY     - TEXT_EMOTION     - TEXT_SENTIMENT     - TEXT_LENGTH</p> <p>Model probabilistic output:     - MODEL_PERPLEXITY     - MODEL_ENTROPY</p> <p>Image:     - IMAGE_BRIGHTNESS     - IMAGE_CONTRAST     - IMAGE_FOCUS     - IMAGE_BLUR     - IMAGE_COLOR_VARIATION     - IMAGE_COLOR_CONTRAST</p> <p>Object detection and semantic segmentation:         (position wrt Cartesian axis with origin in the center of the image)</p>"},{"location":"api/python/enums/#monitoringstatus","title":"MonitoringStatus","text":"<pre><code>MonitoringStatus()\n</code></pre> <p>Fields:</p> <ul> <li>OK</li> <li>WARNING</li> <li>DRIFT</li> </ul>"},{"location":"api/python/enums/#monitoringtarget","title":"MonitoringTarget","text":"<pre><code>MonitoringTarget()\n</code></pre> <p>Fields:</p> <ul> <li>ERROR</li> <li>INPUT</li> <li>CONCEPT</li> <li>PREDICTION</li> <li>INPUT_PREDICTION</li> <li>USER_INPUT</li> <li>RETRIEVED_CONTEXT</li> <li>USER_INPUT_RETRIEVED_CONTEXT</li> <li>USER_INPUT_MODEL_OUTPUT</li> <li>MODEL_OUTPUT_RETRIEVED_CONTEXT</li> </ul>"},{"location":"api/python/enums/#numericlicencefeature","title":"NumericLicenceFeature","text":"<pre><code>NumericLicenceFeature()\n</code></pre> <p>Numeric licence feature</p> <p>Fields: - MAX_TASKS     Maximum number of tasks that the company can have - MAX_USERS     Maximum number of users that the company can have - DAILY_DATA_BATCH_UPLOAD     Maximum number of data batches that the company can upload     in a day. Only considers production data batches.</p>"},{"location":"api/python/enums/#productkeystatus","title":"ProductKeyStatus","text":"<pre><code>ProductKeyStatus()\n</code></pre> <p>Status of a product key</p> <p>Fields:: - NEW = generated but not yet used product key - VALIDATING = validation requested from client - IN_USE = validated product key, client activated</p>"},{"location":"api/python/enums/#retraintriggertype","title":"RetrainTriggerType","text":"<pre><code>RetrainTriggerType()\n</code></pre> <p>Enumeration of the possible retrain triggers</p> <p>Fields:: - AWS_EVENT_BRIDGE - GCP_PUBSUB - AZURE_EVENT_GRID</p>"},{"location":"api/python/enums/#segmentoperator","title":"SegmentOperator","text":"<pre><code>SegmentOperator()\n</code></pre> <p>Segment operator for segmentation rules. Fields: - IN: the given rule is verified if the field is in the list of values - OUT: the given rule is verified if the field is not in the list of values</p>"},{"location":"api/python/enums/#semanticsegtargettype","title":"SemanticSegTargetType","text":"<pre><code>SemanticSegTargetType()\n</code></pre> <p>Format of the target and prediction for the semantic segmentation task.</p> <p>POLYGON: each identified object is represented by the vertices of the polygon</p>"},{"location":"api/python/enums/#storagepolicy","title":"StoragePolicy","text":"<pre><code>StoragePolicy()\n</code></pre> <p>Enumeration that specifies the storage policy for the data sent to ML cube Platform</p> <p>Fields:     cloud     it needs to read data</p>"},{"location":"api/python/enums/#storingdatatype","title":"StoringDataType","text":"<pre><code>StoringDataType()\n</code></pre> <p>Fields:</p> <ul> <li>HISTORICAL</li> <li>REFERENCE</li> <li>PRODUCTION</li> <li>KPI</li> </ul>"},{"location":"api/python/enums/#subscriptiontype","title":"SubscriptionType","text":"<pre><code>SubscriptionType()\n</code></pre> <p>Type of subscription plan of a company</p> <p>Fields:: - CLOUD: subscription plan for web app or sdk access - EDGE: subscription plan for edge deployment</p>"},{"location":"api/python/enums/#suggestiontype","title":"SuggestionType","text":"<pre><code>SuggestionType()\n</code></pre> <p>Enum to specify the preferred type of suggestion</p> <p>Fields: - SAMPLE_WEIGHTS - RESAMPLED_DATASET</p>"},{"location":"api/python/enums/#tasktype","title":"TaskType","text":"<pre><code>TaskType()\n</code></pre> <p>Fields:</p> <ul> <li>REGRESSION</li> <li>CLASSIFICATION_BINARY</li> <li>CLASSIFICATION_MULTICLASS</li> <li>CLASSIFICATION_MULTILABEL</li> <li>RAG</li> <li>OBJECT_DETECTION</li> <li>SEMANTIC_SEGMENTATION</li> </ul>"},{"location":"api/python/enums/#textlanguage","title":"TextLanguage","text":"<pre><code>TextLanguage()\n</code></pre> <p>Enumeration of text language used in nlp tasks.</p> <p>Fields: - ITALIAN - ENGLISH - MULTILANGUAGE</p>"},{"location":"api/python/enums/#usercompanyrole","title":"UserCompanyRole","text":"<pre><code>UserCompanyRole()\n</code></pre> <p>Fields:</p> <ul> <li>COMPANY_OWNER</li> <li>COMPANY_ADMIN</li> <li>COMPANY_USER</li> <li>COMPANY_NONE</li> </ul>"},{"location":"api/python/enums/#userprojectrole","title":"UserProjectRole","text":"<pre><code>UserProjectRole()\n</code></pre> <p>Fields:</p> <ul> <li>PROJECT_ADMIN</li> <li>PROJECT_USER</li> <li>PROJECT_VIEW</li> </ul>"},{"location":"api/python/exceptions/","title":"Exceptions","text":""},{"location":"api/python/exceptions/#adddataschemaexception","title":"AddDataSchemaException","text":"<pre><code>AddDataSchemaException()\n</code></pre> <p>AddDataSchemaException</p>"},{"location":"api/python/exceptions/#addhistoricaldataexception","title":"AddHistoricalDataException","text":"<pre><code>AddHistoricalDataException()\n</code></pre> <p>AddHistoricalDataException</p>"},{"location":"api/python/exceptions/#addkpidataexception","title":"AddKPIDataException","text":"<pre><code>AddKPIDataException()\n</code></pre> <p>AddKPIDataException</p>"},{"location":"api/python/exceptions/#addproductiondataexception","title":"AddProductionDataException","text":"<pre><code>AddProductionDataException()\n</code></pre> <p>AddProductionDataException</p>"},{"location":"api/python/exceptions/#addtargetdataexception","title":"AddTargetDataException","text":"<pre><code>AddTargetDataException()\n</code></pre> <p>AddTargetDataException</p>"},{"location":"api/python/exceptions/#computellmsecurityreportexception","title":"ComputeLlmSecurityReportException","text":"<pre><code>ComputeLlmSecurityReportException()\n</code></pre> <p>ComputeLlmSecurityReportException</p>"},{"location":"api/python/exceptions/#computeragevaluationreportexception","title":"ComputeRagEvaluationReportException","text":"<pre><code>ComputeRagEvaluationReportException()\n</code></pre> <p>ComputeRagEvaluationReportException</p>"},{"location":"api/python/exceptions/#computeretrainingreportexception","title":"ComputeRetrainingReportException","text":"<pre><code>ComputeRetrainingReportException()\n</code></pre> <p>ComputeRetrainingReportException</p>"},{"location":"api/python/exceptions/#computetopicmodelingreportexception","title":"ComputeTopicModelingReportException","text":"<pre><code>ComputeTopicModelingReportException()\n</code></pre> <p>ComputeTopicModelingReportException</p>"},{"location":"api/python/exceptions/#createcompanyexception","title":"CreateCompanyException","text":"<pre><code>CreateCompanyException()\n</code></pre> <p>CreateCompanyException</p>"},{"location":"api/python/exceptions/#createdetectioneventruleexception","title":"CreateDetectionEventRuleException","text":"<pre><code>CreateDetectionEventRuleException()\n</code></pre> <p>CreateDetectionEventRuleException</p>"},{"location":"api/python/exceptions/#createkpiexception","title":"CreateKPIException","text":"<pre><code>CreateKPIException()\n</code></pre> <p>CreateKPIEventRuleException</p>"},{"location":"api/python/exceptions/#createllmspecsexception","title":"CreateLLMSpecsException","text":"<pre><code>CreateLLMSpecsException()\n</code></pre> <p>CreateLLMSpecsException</p>"},{"location":"api/python/exceptions/#createmodelexception","title":"CreateModelException","text":"<pre><code>CreateModelException()\n</code></pre> <p>CreateModelException</p>"},{"location":"api/python/exceptions/#createprojectexception","title":"CreateProjectException","text":"<pre><code>CreateProjectException()\n</code></pre> <p>CreateProjectException</p>"},{"location":"api/python/exceptions/#createtaskexception","title":"CreateTaskException","text":"<pre><code>CreateTaskException()\n</code></pre> <p>CreateTaskException</p>"},{"location":"api/python/exceptions/#getallllmspecsexception","title":"GetAllLLMSpecsException","text":"<pre><code>GetAllLLMSpecsException()\n</code></pre> <p>GetAllLLMSpecsException</p>"},{"location":"api/python/exceptions/#getllmsecurityreportexception","title":"GetLlmSecurityReportException","text":"<pre><code>GetLlmSecurityReportException()\n</code></pre> <p>GetLlmSecurityReportException</p>"},{"location":"api/python/exceptions/#getragevaluationreportexception","title":"GetRagEvaluationReportException","text":"<pre><code>GetRagEvaluationReportException()\n</code></pre> <p>GetRagEvaluationReportException</p>"},{"location":"api/python/exceptions/#getretrainingreportexception","title":"GetRetrainingReportException","text":"<pre><code>GetRetrainingReportException()\n</code></pre> <p>GetRetrainingReportException</p>"},{"location":"api/python/exceptions/#gettopicmodelingreportexception","title":"GetTopicModelingReportException","text":"<pre><code>GetTopicModelingReportException()\n</code></pre> <p>GetTopicModelingReportException</p>"},{"location":"api/python/exceptions/#invalidactionlist","title":"InvalidActionList","text":"<pre><code>InvalidActionList()\n</code></pre> <p>Exception raised when the detection event actions in the rule are not valid</p>"},{"location":"api/python/exceptions/#jobfailureexception","title":"JobFailureException","text":"<pre><code>JobFailureException()\n</code></pre> <p>JobFailureException</p>"},{"location":"api/python/exceptions/#jobnotfoundexception","title":"JobNotFoundException","text":"<pre><code>JobNotFoundException()\n</code></pre> <p>JobNotFoundException</p>"},{"location":"api/python/exceptions/#jobwaittimeoutexception","title":"JobWaitTimeoutException","text":"<pre><code>JobWaitTimeoutException()\n</code></pre> <p>JobWaitTimeoutException</p>"},{"location":"api/python/exceptions/#sdkclientexception","title":"SDKClientException","text":"<pre><code>SDKClientException(\n   error_code: str = 'UNEXPECTED', error_message: str = 'Anunexpectederroroccurred'\n)\n</code></pre> <p>Base class for client sdk exceptions</p>"},{"location":"api/python/exceptions/#setllmspecsexception","title":"SetLLMSpecsException","text":"<pre><code>SetLLMSpecsException()\n</code></pre> <p>SetLLMSpecsException</p>"},{"location":"api/python/exceptions/#setmodelreferenceexception","title":"SetModelReferenceException","text":"<pre><code>SetModelReferenceException()\n</code></pre> <p>AddModelReferenceException</p>"},{"location":"api/python/exceptions/#setmodelsuggestiontypeexception","title":"SetModelSuggestionTypeException","text":"<pre><code>SetModelSuggestionTypeException()\n</code></pre> <p>SetModelSuggestionTypeException</p>"},{"location":"api/python/exceptions/#updatecompanyexception","title":"UpdateCompanyException","text":"<pre><code>UpdateCompanyException()\n</code></pre> <p>UpdateCompanyException</p>"},{"location":"api/python/exceptions/#updatedataschemaexception","title":"UpdateDataSchemaException","text":"<pre><code>UpdateDataSchemaException()\n</code></pre> <p>UpdateDataSchemaException</p>"},{"location":"api/python/exceptions/#updatedetectioneventruleexception","title":"UpdateDetectionEventRuleException","text":"<pre><code>UpdateDetectionEventRuleException()\n</code></pre> <p>UpdateDetectionEventRuleException</p>"},{"location":"api/python/exceptions/#updatemodelversionexception","title":"UpdateModelVersionException","text":"<pre><code>UpdateModelVersionException()\n</code></pre> <p>UpdateModelVersionException</p>"},{"location":"api/python/exceptions/#updateprojectexception","title":"UpdateProjectException","text":"<pre><code>UpdateProjectException()\n</code></pre> <p>UpdateProjectException</p>"},{"location":"api/python/exceptions/#updatetaskexception","title":"UpdateTaskException","text":"<pre><code>UpdateTaskException()\n</code></pre> <p>UpdateTaskException</p>"},{"location":"api/python/models/","title":"Models","text":""},{"location":"api/python/models/#awscompatiblecredentials","title":"AWSCompatibleCredentials","text":"<pre><code>AWSCompatibleCredentials()\n</code></pre> <p>AWS-compatible integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>access_key_id  : The access key id</li> <li>endpoint_url  : The endpoint url (if any)</li> </ul>"},{"location":"api/python/models/#awscredentials","title":"AWSCredentials","text":"<pre><code>AWSCredentials()\n</code></pre> <p>AWS integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>role_arn  : The ARN of the role that should be assumed via STS</li> </ul>"},{"location":"api/python/models/#awseventbridgeretraintrigger","title":"AWSEventBridgeRetrainTrigger","text":"<pre><code>AWSEventBridgeRetrainTrigger()\n</code></pre> <p>Base model to define an AWS EventBridge retrain trigger</p> <p>Fields: type credentials_id aws_region_name event_bus_name</p>"},{"location":"api/python/models/#apikey","title":"ApiKey","text":"<pre><code>ApiKey()\n</code></pre> <p>base model for api key</p> <p>Attributes</p> <ul> <li>api_key  : str</li> <li>name  : str</li> <li>expiration_time  : str | None</li> </ul>"},{"location":"api/python/models/#azureblobdatasource","title":"AzureBlobDataSource","text":"<pre><code>AzureBlobDataSource()\n</code></pre> <p>A source that identifies a blob in Azure Storage.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> <li>credentials_id  : The id of the credentials to use to authenticate     to the remote data source. If None, the default will be used</li> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#azurecredentials","title":"AzureCredentials","text":"<pre><code>AzureCredentials()\n</code></pre> <p>Azure integration credentials.</p> <p>Attributes</p> <ul> <li>app_id  : The id of the service principal</li> </ul>"},{"location":"api/python/models/#azureeventgridretraintrigger","title":"AzureEventGridRetrainTrigger","text":"<pre><code>AzureEventGridRetrainTrigger()\n</code></pre> <p>Base model to define an Azure EventGrid retrain trigger</p> <p>Fields: type credentials_id topic_endpoint</p>"},{"location":"api/python/models/#binaryclassificationtaskcostinfo","title":"BinaryClassificationTaskCostInfo","text":"<pre><code>BinaryClassificationTaskCostInfo()\n</code></pre> <p>Binary classification cost info is expressed in two terms: - cost of false positive - cost of false negative</p>"},{"location":"api/python/models/#categoricalsegmentrule","title":"CategoricalSegmentRule","text":"<pre><code>CategoricalSegmentRule()\n</code></pre> <p>Rule for a segment over categorical values. It contains a list of values that are considered in <code>OR</code> logic to define the rule. See <code>SegmentRule</code> for additional details.</p> <p>Attributes</p> <ul> <li>values  : list[str | int]</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_supported_data_types","title":".get_supported_data_types","text":"<pre><code>.get_supported_data_types()\n</code></pre>"},{"location":"api/python/models/#columninfo","title":"ColumnInfo","text":"<pre><code>ColumnInfo()\n</code></pre> <p>Column base model</p> <p>Attributes</p> <ul> <li>name  : str</li> <li>role  : ColumnRole</li> <li>is_nullable  : bool</li> <li>data_type  : DataType</li> <li>predicted_target  : Optional[str] = None</li> <li>possible_values  : Optional[list[str | int | bool]] = None</li> <li>model_id  : Optional[str] = None</li> <li>dims  : Optional[tuple[int]] = None     it is mandatory when data_type is Array</li> <li>tol  : Optional[int | None] = 0     Tolerance for ImageData width and height.     Images can be loaded with size (w \u00b1 tol, h \u00b1 tol) pixels</li> <li>classes_names  : Optional[list[str]] = None     it is mandatory when the column is the target     in multilabel classification tasks</li> <li>subrole  : Optional[ColumnSubRole] = None     Indicates the subrole of the column. It's used in     RAG tasks to define the role of the input columns     (e.g. user input or retrieved context)</li> <li>image_mode  : Optional[ImageMode] = None     Indicates the mode of the image. It must be provided     when the data type is an image</li> </ul>"},{"location":"api/python/models/#company","title":"Company","text":"<pre><code>Company()\n</code></pre> <p>Company model</p> <p>Attributes</p> <ul> <li>company_id  : str</li> <li>name  : str</li> <li>address  : str</li> <li>vat  : str</li> </ul>"},{"location":"api/python/models/#companyuser","title":"CompanyUser","text":"<pre><code>CompanyUser()\n</code></pre> <p>base model for company user</p> <p>Attributes</p> <ul> <li>user_id  : str</li> <li>company_role  : UserCompanyRole</li> </ul>"},{"location":"api/python/models/#data","title":"Data","text":"<pre><code>Data()\n</code></pre> <p>Generic data model that contains all information about a data</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure</li> <li>source  : DataSource</li> </ul>"},{"location":"api/python/models/#databatch","title":"DataBatch","text":"<pre><code>DataBatch()\n</code></pre> <p>A Data Batch represents a portion of data that is sent to the ML cube Platform.</p> <p>Attributes</p> <ul> <li>index  : int     The index of the data batch, assigned in the order of creation</li> <li>creation_date  : datetime     The creation date of the data batch</li> <li>first_sample_date  : datetime     The date of the first sample in the data batch</li> <li>last_sample_date  : datetime     The date of the last sample in the data batch</li> <li>storing_data_type  : StoringDataType     The origin of the data batch</li> <li>inputs  : bool     Whether the data batch contains inputs</li> <li>metadata  : bool     Whether the data batch contains metadata</li> <li>target  : bool     Whether the data batch contains the target</li> <li>predictions  : list[str]     The list of models for which the data batch contains predictions</li> <li>monitoring_flags  : list[DataBatchMonitoringFlag]     The list of monitoring flags referring to the whole population</li> <li>segmented_monitoring_flags  : list[SegmentedMonitoringFlags]     The list of monitoring flags for each segment</li> </ul>"},{"location":"api/python/models/#databatchmonitoringflag","title":"DataBatchMonitoringFlag","text":"<pre><code>DataBatchMonitoringFlag()\n</code></pre> <p>Model that stores the monitoring status of a monitoring target, used in the context of a data batch.</p> <p>Attributes</p> <ul> <li>monitoring_target  : MonitoringTarget</li> <li>status  : MonitoringStatus | None     The status of the monitoring target. If None, it means     that the monitoring target was not monitored.</li> </ul>"},{"location":"api/python/models/#dataschema","title":"DataSchema","text":"<pre><code>DataSchema()\n</code></pre> <p>Data schema base model</p> <p>Attributes</p> <ul> <li>columns  : List[ColumnInfo]</li> </ul>"},{"location":"api/python/models/#datasource","title":"DataSource","text":"<pre><code>DataSource()\n</code></pre> <p>Generic data source.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> </ul>"},{"location":"api/python/models/#detectionevent","title":"DetectionEvent","text":"<pre><code>DetectionEvent()\n</code></pre> <p>An event created during the detection process.</p> <p>Attributes</p> <ul> <li>event_id  : str</li> <li>event_type  : DetectionEventType</li> <li>monitoring_target  : MonitoringTarget</li> <li>monitoring_metric  : MonitoringMetric | None</li> <li>severity_type  : Optional[DetectionEventSeverity]</li> <li>insert_datetime  : str</li> <li>sample_timestamp  : float</li> <li>sample_customer_id  : str</li> <li>model_id  : Optional[str]</li> <li>model_name  : Optional[str]</li> <li>model_version  : Optional[str]</li> <li>user_feedback  : Optional[bool]</li> <li>specification  : Optional[str]</li> <li>segment_id  : Optional[str]</li> </ul>"},{"location":"api/python/models/#detectioneventaction","title":"DetectionEventAction","text":"<pre><code>DetectionEventAction()\n</code></pre> <p>Generic action that can be performed</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType</li> </ul>"},{"location":"api/python/models/#detectioneventrule","title":"DetectionEventRule","text":"<pre><code>DetectionEventRule(\n   **kwargs\n)\n</code></pre> <p>A rule that can be triggered by a detection event, and executes a series of actions.</p> <p>Attributes</p> <ul> <li>rule_id  : str</li> <li>name  : str</li> <li>task_id  : str</li> <li>model_name  : Optional[str]</li> <li>severity  : DetectionEventSeverity</li> <li>detection_event_type  : DetectionEventType</li> <li>monitoring_target  : MonitoringTarget</li> <li>actions  : List[DetectionEventAction]</li> <li>segment_id  : Optional[str]</li> </ul>"},{"location":"api/python/models/#discordnotificationaction","title":"DiscordNotificationAction","text":"<pre><code>DiscordNotificationAction()\n</code></pre> <p>Action that sends a notification to a Discord server through a webhook that you configure</p> <p>Attributes</p> <ul> <li>webhook  : str type = DetectionEventActionType.DISCORD_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#emailnotificationaction","title":"EmailNotificationAction","text":"<pre><code>EmailNotificationAction()\n</code></pre> <p>Base Model for Email Notification Action</p> <p>Attributes</p> <ul> <li>address  : str type = DetectionEventActionType.EMAIL_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#embeddingdata","title":"EmbeddingData","text":"<pre><code>EmbeddingData()\n</code></pre> <p>Embedding data model i.e., a data that can be represented via DataFrame and is stored in formats like: csv, parquet, json. There is only one input that has type array_1</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure = DataStructure.EMBEDDING</li> <li>source  : DataSource</li> </ul>"},{"location":"api/python/models/#gcpcredentials","title":"GCPCredentials","text":"<pre><code>GCPCredentials()\n</code></pre> <p>GCP integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>gcp_project_id  : The id of the project on GCP</li> <li>client_email  : The email that identifies the service account</li> <li>client_id  : The client id</li> </ul>"},{"location":"api/python/models/#gcppubsubretraintrigger","title":"GCPPubSubRetrainTrigger","text":"<pre><code>GCPPubSubRetrainTrigger()\n</code></pre> <p>Base model to define a GCP PubSub retrain trigger</p> <p>Fields: type credentials_id topic_name</p>"},{"location":"api/python/models/#gcsdatasource","title":"GCSDataSource","text":"<pre><code>GCSDataSource()\n</code></pre> <p>A source that identifies a file in a GCS bucket.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> <li>credentials_id  : The id of the credentials to use to authenticate     to the remote data source. If None, the default will be used</li> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_1","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type_1","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#imagedata","title":"ImageData","text":"<pre><code>ImageData()\n</code></pre> <p>Image data model i.e., images, text or other. Since it is composed of multiple files, it needs a mapping between customer ids and those files</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure = DataStructure.IMAGE</li> <li>source  : DataSource</li> <li>mapping_source  : DataSource</li> <li>embedding_source  : DataSource | None</li> </ul>"},{"location":"api/python/models/#integrationcredentials","title":"IntegrationCredentials","text":"<pre><code>IntegrationCredentials()\n</code></pre> <p>Credentials to authenticate to a 3<sup>rd</sup> party service provider via an integration.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> </ul>"},{"location":"api/python/models/#job","title":"Job","text":"<pre><code>Job()\n</code></pre> <p>Job information item model</p> <p>Attributes</p> <ul> <li>job_id  : str</li> <li>job_group  : str</li> <li>project_id  : str</li> <li>project_name  : str</li> <li>task_id  : str</li> <li>task_name  : str</li> <li>model_id  : Optional[str]</li> <li>model_name  : Optional[str]</li> <li>status  : str</li> <li>error  : Optional[str]</li> </ul>"},{"location":"api/python/models/#kpi","title":"KPI","text":"<pre><code>KPI()\n</code></pre> <p>KPI base model</p> <p>Attributes</p> <ul> <li>kpi_id  : str</li> <li>name  : str</li> <li>status  : ModelStatus</li> <li>status_kpi_start_timestamp  : Optional[datetime]</li> <li>status_insert_datetime  : datetime</li> </ul>"},{"location":"api/python/models/#llmprompt","title":"LLMPrompt","text":"<pre><code>LLMPrompt()\n</code></pre> <p>Base model to define llm prompts</p> <p>Attributes</p> <ul> <li>role  : str</li> <li>task  : str</li> <li>behavior_guidelines  : str</li> <li>security_guidelines  : str</li> </ul>"},{"location":"api/python/models/#llmspecs","title":"LLMSpecs","text":"<pre><code>LLMSpecs()\n</code></pre> <p>Base model to define llm specs</p> <p>Attributes</p> <ul> <li>llm  : str</li> <li>temperature  : float</li> <li>prompt  : LLMPrompt</li> </ul>"},{"location":"api/python/models/#localdatasource","title":"LocalDataSource","text":"<pre><code>LocalDataSource()\n</code></pre> <p>Use this data source if you want to upload a file from your local disk to the ML cube platform cloud.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> <li>file_path  : str</li> </ul>"},{"location":"api/python/models/#model","title":"Model","text":"<pre><code>Model()\n</code></pre> <p>Base model to define model item</p> <p>Attributes</p> <ul> <li>model_id  : str</li> <li>task_id  : str</li> <li>name  : str</li> <li>version  : str</li> <li>metric_name  : performance or error metric associated with     the model</li> <li>creation_datetime  : Optional[datetime]</li> <li>retrain_trigger  : Optional[RetrainTrigger]</li> <li>retraining_cost  : float</li> <li>llm_specs  : Optional[LLMSpecs]</li> </ul>"},{"location":"api/python/models/#monitoringquantitystatus","title":"MonitoringQuantityStatus","text":"<pre><code>MonitoringQuantityStatus()\n</code></pre> <p>Base model to store the monitoring status of a monitoring quantity (target or metric)</p> <p>Attributes</p> <ul> <li>monitoring_target  : MonitoringTarget</li> <li>status  : MonitoringStatus</li> <li>monitoring_metric  : MonitoringMetric | None</li> <li>segment_id  : str | None</li> </ul>"},{"location":"api/python/models/#mqttnotificationaction","title":"MqttNotificationAction","text":"<pre><code>MqttNotificationAction()\n</code></pre> <p>Base Model for Mqtt Notification Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.MQTT_NOTIFICATION</li> <li>connection_string  : str</li> <li>topic  : str</li> <li>payload  : str</li> </ul>"},{"location":"api/python/models/#multiclassclassificationtaskcostinfo","title":"MulticlassClassificationTaskCostInfo","text":"<pre><code>MulticlassClassificationTaskCostInfo()\n</code></pre> <p>Multiclass classification cost info is expressed in terms of the misclassification costs for each class</p>"},{"location":"api/python/models/#multilabelclassificationtaskcostinfo","title":"MultilabelClassificationTaskCostInfo","text":"<pre><code>MultilabelClassificationTaskCostInfo()\n</code></pre> <p>Multilabel classification cost info is expressed in terms of false positive and false negative costs for each class</p>"},{"location":"api/python/models/#numericlicencefeatureinfo","title":"NumericLicenceFeatureInfo","text":"<pre><code>NumericLicenceFeatureInfo()\n</code></pre> <p>Numeric Licence feature info model</p> <p>Attributes</p> <ul> <li>feature  : NumericLicenceFeature     Current numeric feature</li> <li>max_value  : int | None     Maximum value of the feature. If None, no limit is set</li> <li>used_value  : int     Used value of the feature. If max_value is None,     this value defaults to 0</li> </ul>"},{"location":"api/python/models/#numericsegmentrule","title":"NumericSegmentRule","text":"<pre><code>NumericSegmentRule()\n</code></pre> <p>Rule for a segment over numeric values. It contains a list of ranges that are considered in <code>OR</code> logic to define the rule. See <code>SegmentRule</code> for additional details.</p> <p>Attributes</p> <ul> <li>values  : list[SegmentRuleNumericRange]</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_supported_data_types_1","title":".get_supported_data_types","text":"<pre><code>.get_supported_data_types()\n</code></pre>"},{"location":"api/python/models/#project","title":"Project","text":"<pre><code>Project()\n</code></pre> <p>Project model</p> <p>Attributes</p> <ul> <li>project_id  : str</li> <li>name  : str</li> </ul>"},{"location":"api/python/models/#referenceinfo","title":"ReferenceInfo","text":"<pre><code>ReferenceInfo()\n</code></pre> <p>Reference info</p> <p>Attributes</p> <ul> <li>time_intervals  : list[tuple[float, float]]     List of time intervals used as model reference with tuples     containing the start and end time of the intervals     Can be the default time intervals or segment specific ones</li> <li>segment_id  : str | None     Segment id associated to the model reference considered, None if the     reference is for the whole population (default)</li> </ul>"},{"location":"api/python/models/#regressiontaskcostinfo","title":"RegressionTaskCostInfo","text":"<pre><code>RegressionTaskCostInfo()\n</code></pre> <p>Regression cost info is expressed in two terms: - cost due to overestimation - cost due to underestimation</p> <p>Fields: currency overestimation_cost underestimation_cost</p>"},{"location":"api/python/models/#remotedatasource","title":"RemoteDataSource","text":"<pre><code>RemoteDataSource()\n</code></pre> <p>A source that identifies where data is stored.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> <li>credentials_id  : The id of the credentials to use to authenticate     to the remote data source. If None, the default will be used</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_2","title":".get_path","text":"<pre><code>.get_path()\n</code></pre> <p>Return the path of the object</p>"},{"location":"api/python/models/#get_source_type_2","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre> <p>Returns raw data source type</p>"},{"location":"api/python/models/#resampleddatasetsuggestion","title":"ResampledDatasetSuggestion","text":"<pre><code>ResampledDatasetSuggestion()\n</code></pre> <p>ResampledDatasetSuggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> <li>sample_ids  : List[str]</li> <li>sample_counts  : List[int]</li> </ul>"},{"location":"api/python/models/#retrainaction","title":"RetrainAction","text":"<pre><code>RetrainAction()\n</code></pre> <p>Base Model for Retrain Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.RETRAIN</li> <li>model_name  : str</li> </ul>"},{"location":"api/python/models/#retraintrigger","title":"RetrainTrigger","text":"<pre><code>RetrainTrigger()\n</code></pre> <p>Base model to define a retrain trigger</p> <p>Fields: type credentials_id</p>"},{"location":"api/python/models/#retrainingreport","title":"RetrainingReport","text":"<pre><code>RetrainingReport()\n</code></pre> <p>base model for Retraining Report</p> <p>Attributes</p> <ul> <li>report_id  : str</li> <li>suggestion  : Suggestion</li> <li>effective_sample_size  : float</li> <li>model_metric_name  : str</li> <li>performance_upper_bound  : float</li> <li>performance_lower_bound  : float</li> <li>cost_upper_bound  : float</li> <li>cost_lower_bound  : float</li> </ul>"},{"location":"api/python/models/#s3datasource","title":"S3DataSource","text":"<pre><code>S3DataSource()\n</code></pre> <p>A source that identifies a file in an S3 bucket.</p> <p>Attributes</p> <ul> <li>file_type  : FileType</li> <li>is_folder  : bool</li> <li>folder_type  : FolderType | None</li> <li>credentials_id  : The id of the credentials to use to authenticate     to the remote data source. If None, the default will be used</li> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_3","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type_3","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#sampleweightssuggestion","title":"SampleWeightsSuggestion","text":"<pre><code>SampleWeightsSuggestion()\n</code></pre> <p>SampleWeightsSuggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> <li>sample_ids  : List[str]</li> <li>sample_weights  : List[float]</li> </ul>"},{"location":"api/python/models/#secretawscredentials","title":"SecretAWSCredentials","text":"<pre><code>SecretAWSCredentials()\n</code></pre> <p>AWS integration credentials, that also include the trust policy that you need to set on the IAM role on AWS.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>role_arn  : The ARN of the IAM role that should be assumed</li> <li>trust_policy  : The trust policy that should be set on the     IAM role on AWS</li> </ul>"},{"location":"api/python/models/#segment","title":"Segment","text":"<pre><code>Segment()\n</code></pre> <p>A Segment is a partition of the data, defined by a set of rules that are applied to the DataSchema. Each rule of the segment is applied in <code>AND</code>, whereas the values of each rule are applied in <code>OR</code>.</p> <p>Attributes</p> <ul> <li>segment_id  : str</li> <li>name  : str</li> <li>rules  : list[SerializeAsAny[NumericSegmentRule | CategoricalSegmentRule]]</li> </ul>"},{"location":"api/python/models/#segmentrule","title":"SegmentRule","text":"<pre><code>SegmentRule()\n</code></pre> <p>A segment is composed by a set of rules that are applied over the fields of the <code>DataSchema</code>. Each rule is applied in <code>AND</code> logic with the other rules, and supports an <code>operator</code> that can either be:     the value of the field must be in the list of values.     the value of the field must not be in the list of values.</p> <p>Attributes</p> <ul> <li>column_name  : str</li> <li>operator  : SegmentOperator</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_supported_data_types_2","title":".get_supported_data_types","text":"<pre><code>.get_supported_data_types()\n</code></pre> <p>Get the supported data types for the rule</p>"},{"location":"api/python/models/#segmentrulenumericrange","title":"SegmentRuleNumericRange","text":"<pre><code>SegmentRuleNumericRange()\n</code></pre> <p>Numeric range for a single element of values in a NumericSegmentRule</p> <p>Attributes</p> <ul> <li>start_value  : float | None</li> <li>end_value  : float | None</li> </ul>"},{"location":"api/python/models/#segmentedmonitoringflags","title":"SegmentedMonitoringFlags","text":"<pre><code>SegmentedMonitoringFlags()\n</code></pre> <p>Model containing the monitoring flags of a given segment, identified by its id.</p> <p>Attributes</p> <ul> <li>segment_id  : str</li> <li>flags  : list[DataBatchMonitoringFlag]</li> </ul>"},{"location":"api/python/models/#slacknotificationaction","title":"SlackNotificationAction","text":"<pre><code>SlackNotificationAction()\n</code></pre> <p>Action that sends a notification to a Slack channel through a webhook that you configure.</p> <p>Attributes</p> <ul> <li>webhook  : str</li> <li>channel  : str type = DetectionEventActionType.SLACK_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#subscriptionplaninfo","title":"SubscriptionPlanInfo","text":"<pre><code>SubscriptionPlanInfo()\n</code></pre> <p>Data model for a subscription plan</p> <p>Product key data are set only if a product key is associated to the subscription plan</p> <p>Attributes</p> <ul> <li>subscription_id  : str</li> <li>type  : SubscriptionType</li> <li>boolean_licence_features  : list[BooleanLicenceFeature]     Features which are either enabled or disabled</li> <li>numeric_licence_features  : list[NumericLicenceFeatureInfo]]     Features associated with a usage limit</li> <li>is_active  : bool</li> <li>start_date  : date</li> <li>expiration_date  : date | None     If set to None, no expiration is set</li> <li>product_key  : str | None</li> <li>product_key_status  : ProductKeyStatus | None</li> </ul>"},{"location":"api/python/models/#suggestion","title":"Suggestion","text":"<pre><code>Suggestion()\n</code></pre> <p>Suggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> <li>sample_ids  : List[str]</li> </ul>"},{"location":"api/python/models/#suggestioninfo","title":"SuggestionInfo","text":"<pre><code>SuggestionInfo()\n</code></pre> <p>SuggestionInfo base model</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>executed  : bool</li> <li>timestamp  : float</li> </ul>"},{"location":"api/python/models/#tabulardata","title":"TabularData","text":"<pre><code>TabularData()\n</code></pre> <p>Tabular data model i.e., a data that can be represented via DataFrame and is stored in formats like: csv, parquet, json</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure = DataStructure.TABULAR</li> <li>source  : DataSource</li> </ul>"},{"location":"api/python/models/#task","title":"Task","text":"<pre><code>Task()\n</code></pre> <p>Task model</p> <p>Attributes</p> <ul> <li>task_id  : str</li> <li>name  : str</li> <li>type  : TaskType</li> <li>cost_info  : TaskCostInfoUnion | None = None</li> <li>optional_target  : bool</li> <li>monitoring_targets  : list[MonitoringTarget]</li> <li>monitoring_metrics  : (     None     | dict[MonitoringTarget, list[tuple[MonitoringMetric, str | None]]]</li> <li>monitoring_status  : list[MonitoringQuantityStatus] ) = None</li> </ul>"},{"location":"api/python/models/#taskcostinfo","title":"TaskCostInfo","text":"<pre><code>TaskCostInfo()\n</code></pre> <p>Base class for task cost info. It depends on TaskType because classification is different from regression in terms of business costs due to errors</p>"},{"location":"api/python/models/#taskllmsecreportitem","title":"TaskLlmSecReportItem","text":"<pre><code>TaskLlmSecReportItem()\n</code></pre> <p>Task LLM security report item model. It contains the most important information of a LLM security report.</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>creation_date  : datetime</li> <li>name  : str</li> <li>status  : JobStatus</li> <li>from_datetime  : datetime</li> <li>to_datetime  : datetime</li> </ul>"},{"location":"api/python/models/#taskragevalreportitem","title":"TaskRagEvalReportItem","text":"<pre><code>TaskRagEvalReportItem()\n</code></pre> <p>base model for Rag Evaluation Report</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>creation_datetime  : datetime</li> <li>name  : str</li> <li>status  : JobStatus</li> <li>from_datetime  : datetime</li> <li>to_datetime  : datetime</li> </ul>"},{"location":"api/python/models/#tasktopicmodelingreportdetails","title":"TaskTopicModelingReportDetails","text":"<pre><code>TaskTopicModelingReportDetails()\n</code></pre> <p>Task Topic Modeling Report Details base model</p> <p>Attributes</p> <ul> <li>sample_ids  : list[str]</li> <li>topics  : list[str]</li> </ul>"},{"location":"api/python/models/#tasktopicmodelingreportitem","title":"TaskTopicModelingReportItem","text":"<pre><code>TaskTopicModelingReportItem()\n</code></pre> <p>Task Topic Modeling Report Item base model</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>creation_datetime  : datetime</li> <li>name  : str</li> <li>status  : JobStatus</li> <li>from_date  : datetime</li> <li>to_date  : datetime</li> </ul>"},{"location":"api/python/models/#teamsnotificationaction","title":"TeamsNotificationAction","text":"<pre><code>TeamsNotificationAction()\n</code></pre> <p>Base Model for Teams Notification Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.TEAMS_NOTIFICATION</li> <li>webhook  : str</li> </ul>"},{"location":"api/python/models/#textdata","title":"TextData","text":"<pre><code>TextData()\n</code></pre> <p>Text data model for nlp tasks.</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure = DataStructure.TEXT</li> <li>source  : DataSource</li> <li>embedding_source  : DataSource | None</li> </ul>"},{"location":"api/rest/","title":"REST API","text":"<p>Page under construction</p>"},{"location":"user_guide/company/","title":"Company and Subscriptions","text":"<p>A Company is the fundamental organizational entity inside ML cube Platform. Users belong to a single Company and billing, licenses, and projects are all managed and created inside a Company.</p> <p>The Company has an Owner with maximum privileges including billing administration. The owner can create new User accounts inside the Company assigning specific roles at the company and project level.</p> <p>The information required that describe the Company are:</p> <ul> <li>Company name</li> <li>Address</li> <li>VAT</li> </ul> <p>The Company is created by ML cube team during the onboarding, then, during first login the Owner needs to complete the remaining information.</p>"},{"location":"user_guide/company/#subscriptions","title":"Subscriptions","text":"<p>A Subscription is the payment arrangement where a Company pays a periodical fee to access ML cube Platform services. A Subscription has a start and an expiration date and contains the modules and quotas the Company can handle.</p> QuotasModules Quota Description Users Maximum number of Users per Company Tasks Maximum number of Tasks per Company excluding those in demo projects Modules Description Monitoring Data drift monitoring and detection for several targets and metrics. Alerts are raised when drifts are detected allowing automated the response. Retraining Generation of retraining datasets to update AI models according to identified data drifts. Dataset is based on data distributions and leverages all the past available data. Explainability Explainability of detected drifts to better understand what happened and how to tackle it. <p>Moreover, there are two types of subscriptions depending on where ML cube Platform is used:</p> <ul> <li> <p> Cloud </p> <p>Standard SaaS plan to use ML cube Platform hosted on ML cube cloud infrastructure. With Cloud subscription the users can use Web Application and SDK to interact with ML cube Platform and to use its services. Data can be stored on ML cube Private Cloud Storage.</p> </li> <li> <p> Edge </p> <p>Edge subscriptions are used, as the name suggests, for edge devices that hosts ML cube Platform Edge. A common use case is an industrial machinery computer that runs AI algorithms. Edge subscriptions are validated via Product Key and are uniquely linked to a specific edge device.</p> </li> </ul>"},{"location":"user_guide/data/","title":"Data","text":"<p>To use ML cube Platform, you need to upload the data that your artificial intelligence algorithms deal with. They are represented by the Data logical entity, which is a complex object composed of many different elementary units.</p> <p>According to the context, the Data object has some required entities to be considered valid: for instance, training data require both input and target, while production data are still valid with only inputs.</p> <p>Each elementary unit in the Data object belongs to a Data Category, which represents an consistent group of data. Available categories are:</p> Data Category Description Input The set of input data like features in Tabular Task or the Image in Object Detection Task. Target The ground truth data representing the actual class in a Classification Task or the value to predict in Regression. Prediction The AI model's prediction using the Input data."},{"location":"user_guide/data/#data-category-and-data-schema-columns-role","title":"Data Category and Data Schema Column's Role","text":"<p>The Data Schema created for the Task contains a list of Column objects, each of which has a Role. Naturally, there is a relationship between the Column's Role and the Data Category. In fact, each Data Category comprises a set of Column objects with certain Roles. When you upload samples belonging to a Data Category, they must contain all the Columns objects declared on the Data Schema to be considered valid.</p> <p>The following table shows these relationships:</p> Data Category ID Time ID Input Input additional embedding Target Target additional embedding Prediction Prediction additional embedding Input Target Prediction <p>Note</p> <p>As you can see Column's Roles ID and Time Id are always required because are used by ML cube Platform to correctly link data units together.</p> <p>Example</p> <p>Consider a regression task with three inputs variables \\(X_0, X_1\\) and \\(X_2\\). In the Data Schema they are represented by three Column objects with Input Role. When uploading Input Data Category, the csv file must contain the three Input Columns, the Time ID and the ID like: <pre><code>sample_id,time_id,x_0,x_1,x_2\nabc123,1234,3.5,100,23.0\n</code></pre></p>"},{"location":"user_guide/data/#data-batch","title":"Data Batch","text":"<p>ML cube Platform supports batch-based data processing, which means that data samples are uploaded together as a batch. Therefore, the Data object described above must contain, for each Data Category, the same set of samples to form a consistent batch. The only exception is for Target data that, given the fact that they may be obtained through labeling, can be present partially.</p> <p>Warning</p> <p>Even if Target data can be uploaded partially, they must be a subset of samples of Input or Prediction.</p> <p>In any case, the presence of a Data Category in a Data Batch depends on the context in which it is uploaded. In the next section we explain what is meant by the Historical, Reference and Production data.</p> <p>Info</p> <p>You can see your uploaded data batches in its own page in the web app. With information about the origin of data, the data categories it contains and what monitoring has been done.</p> <p> </p> Examples of two valid and two invalid data batches. <p>When uploading images on the platform, you also need to provide a mapping file that links the image filename to the sample ID and its timestamp. Likewise sample IDs, filenames must be unique across all the images uploaded for a task.</p> <p>Example</p> <p>An example of a mapping file for images could be: <pre><code>sample_id,time_id,file_name\nabc123,1234,image_1.png\n</code></pre></p> <p>Notice that we require the column with the file name to be named <code>file_name</code>.</p>"},{"location":"user_guide/data/#data-contexts","title":"Data contexts","text":"<p>Data samples have different meanings depending on how you have used them to train, validate, or infer from your artificial intelligence model. Some data may be too old and, therefore, not used, others may belong to a training set, and, lastly, the newest one comes directly from production.</p> <p>To handle all these scenarios, the ML cube Platform allows the upload of data specifying how they were used. Indeed, the operations of the ML cube Platform will be different to each context.</p> <p>The main difference in the data usage is the split between the ones present before the model was in production and the others. For that reason, the ML cube Platform defines two data contexts: historical and production.</p> <p>Note</p> <p>Note that when an artificial intelligence model is retrained with new data, the previous contexts change and the old production data become historical ones for the new model version.</p> <p> </p> Different data batches and their data context."},{"location":"user_guide/data/#historical","title":"Historical","text":"<p>Historical data are all data samples that existed before the model was deployed in production. They include the training, validation and test sets that were used during model experimentation and training, or older data that were discarded.</p> <p>Historical data can be uploaded in ML cube Platform anytime through different data batches.</p> <p>Data categories that a batch of historical data contains depend on the Task Type, here is the summary:</p> <ul> <li>Supervised tasks like regression, classification and object detection requires Input and Target</li> <li>Retrieval Augmented Generation task requires Inputs and Predictions</li> </ul> <p>The reason why it is not possible to upload the model's predictions in historical data of supervised tasks is that they may contain the training bias of the model. On the other hand, in RAG tasks there is no target or training dataset.</p> <p>Warning</p> <p>There is only one limitation to upload historical data: the production boundary. Model data reference defines a rigid boundary separating historical and production data. This boundary is defined as the last sample timestamp of the reference data. After the model data reference is defined, it is no longer possible to upload historical data that are more recent than that date. Indeed, data with sample timestamps higher than the production boundary are considered production data and no longer historical.</p> SDK Example <p>You can upload historical data as follow:</p> <pre><code>job_id = client.add_historical_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=target_data,\n)\n</code></pre>"},{"location":"user_guide/data/#reference","title":"Reference","text":"<p>The Monitoring module requires the definition of a Reference dataset representing training, validation and test data. As mentioned before, historical data belong to both training and old data, but it is important distinguish them. Therefore, the reference data is initially loaded as historical data and then marked as reference data by providing the timestamp interval. This interval is defined as one or more [from timestamp, to timestamp] tuples. It serves also as default interval being applied to any Segment without a specific time range. Optionally, segment-specific intervals can be provided, also as lists of [from timestamp, to timestamp] tuples.</p> <p>To provide sufficient statistical reliability, reference data must include at least 300 samples. This requirement also applies to the reference of each Segment, if the task involves any.</p> <p>It is worth mentioning that also RAG Tasks can have reference data, even if they do not have a proper training dataset. Indeed, reference data are used by the Monitoring module to initialize drift detection algorithms, therefore, the reference data definition is a mandatory step to enable this feature. For RAG Tasks, reference data can be used to indicate the type of data expected in production.</p> SDK Example <p>You can set the default reference data as follow:</p> <pre><code>job_id = client.set_model_reference(\n    model_id=model_id,\n    default_reference=ReferenceInfo(\n            time_intervals=[(from_timestamp, to_timestamp)]\n        ),\n)\n</code></pre> <p>In case you want to set multiple default intervals:</p> <pre><code>job_id = client.set_model_reference(\n    model_id=model_id,\n    default_reference=ReferenceInfo(\n            time_intervals=[(from_timestamp_1, to_timestamp_1), (from_timestamp_2, to_timestamp_2)]\n        ),\n)\n</code></pre> <p>If you want to set also segment-specific reference intervals:</p> <p><pre><code>job_id = client.set_model_reference(\n    model_id=model_id,\n    default_reference=ReferenceInfo(\n            time_intervals=[(from_timestamp, to_timestamp)]\n        ),\n    segment_references=[ReferenceInfo(time_intervals=[(from_timestamp_segment_1, to_timestamp_segment_1)], segment_id=segment_id_1),\n                        ReferenceInfo(time_intervals=[(from_timestamp_segment_2, to_timestamp_segment_2)], segment_id=segment_id_2)]\n)\n</code></pre> Segment intervals can also include multiple (from, to) tuples.</p>"},{"location":"user_guide/data/#production","title":"Production","text":"<p>After the artificial intelligence model is deployed in a production environment, incoming data belongs to the Production context. They have different characteristics with respect to historical data because they are online.</p> <p>While the assumption for historical data is that they belong to the past and are complete (with both input and target as an example), production data can be partial or different Data Categories are available at different times. For that reason, they can be uploaded asynchronously at different moments in time.</p> <p>However, there are some constraints about the Production data upload. Since data samples are grouped in Data Batches they must be consistent in all the Data Categories. ML cube Platform allows to upload each category separately at different moments but the uploads must contains always the samples in the Data Batch.</p> <p>Info</p> <p>After data are uploaded to ML cube Platform their ID are analyzed to determine if they are already present or not. If not then a new Data Batch is created containing the set of samples ID just uploaded. If the samples are already present in ML cube Platform they belong to a existing Data Batch and the Data Category is attached to it.</p> <p>Let's do an example with a Regression Task about forecasting the weekly produced power from a solar plant. In this case, inputs come from electrical and meteorological measurements from the past week and are immediately available; prediction is available as soon as the model makes an inference and is uploaded from the model inference service. Instead, target data are only available at the end of the week.</p> <ul> <li>Input samples are the first uploaded Data Category and they define the samples that belong to this Data Batch.</li> <li>Predictions are uploaded after the model inference is completed, in order to be valid they must match entirely the input samples already uploaded.</li> <li>Target data are uploaded at the end of the week and, as described before they can be a subset of already uploaded input samples.</li> </ul> SDK Example <p>You can upload production data in one method call:</p> <pre><code>job_id = client.add_production_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=target_data,\n    predictions=[\n        (\n            model_id,\n            prediction_data,\n        )\n    ],\n)\n</code></pre> <p>or in different times:</p> <pre><code>job_id = client.add_production_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=None,\n    predictions=None\n)\n# ...\njob_id = client.add_production_data(\n    task_id=task_id,\n    inputs=None,\n    target=target_data,\n    predictions=[\n        (\n            model_id,\n            prediction_data,\n        )\n    ],\n)\n</code></pre>"},{"location":"user_guide/data/#post-labeling","title":"Post labeling","text":"<p>In some use cases, target data are obtained only by human labeling, and since this is an expensive procedure, not all available samples are labelled but usually only a selected subset. Moreover, it is possible that labeling is done occasionally for samples that belong to different data batches uploaded in ML cube Platform.</p> <p>Obviously, this scenario does not belong either to historical or production contexts and it has its own specificity. Hence, ML cube Platform provides another way to upload newly labelled samples in a single moment. In this case, there are no constraints about data batches or production boundary but you are allowed to upload all data freely. ML cube Platform will take care of assigning the new target to the right Data Batch.</p> <p>Warning</p> <p>The only constraint is that you cannot upload the label for a sample that already has target uploaded.</p> <p> </p> Labelled target data distributed in their data batches. SDK Example <p>You can upload labelled target samples via the add_target_data method:</p> <pre><code>job_id = client.add_target_data(\n    task_id=task_id,\n    target=target_data,\n)\n</code></pre>"},{"location":"user_guide/data/#data-sources","title":"Data sources","text":"<p>The previous sections of this page described what a Data object is, how it is represented by Data Batch and its uploading contexts. To have more information about how to actually upload data we suggest to read the page Data Sources.</p>"},{"location":"user_guide/data_schema/","title":"Data Schema","text":"<p>Data schema contains all the information about the data in the Task, it is created at the beginning and is immutable.</p> <p>Tip</p> <p>Data Schema can be easily created starting from a template from the Web App. Go in Data Schema page after you created a Task and see the precompiled version of Data Schema, update and insert new Columns to create your custom version.</p> <p>A Data schema is composed of a list of objects named Column that represent each data entity in the Task. The number and type of Column objects depend on the task type and task data structure.</p> <p>A Column object has some mandatory attributes and others that depends on its role or data type:</p> Attribute Description Mandatory Name Name of the entity used to read it from raw data. For instance, in Tabular tasks, it represents the name of the column of the CSV file. Mandatory Data type Data format of the entity. Possible values are <ul><li>Float: numeric value</li><li>Categorical: entity that can assume a only specified values. A Categorical Column requires the attribute possible_values to be specified.</li><li>String: generic textual data like text input or customer id. To not be used for categorical columns.</li><li>Array 1: one-dimensional array. Requires dims attribute to be defined like a list of 1 element [n] that specifies the number of elements of the array.</li><li>Array 2: two-dimensional array. Requires dims attribute to be defined like a list of 2 elements [n, m] that specifies the number elements of the each dimension of the array.</li><li>Array 3: three-dimensional array. Requires dims attribute to be defined like a list of 3 elements [n, m, k] that specifies the number elements of the each dimension of the array.</li></ul> Mandatory Role Defines the role the Column object has in the Task. According to the Task type some roles are required or not allowed. More information in the following sections. Mandatory Subrole Additional specification of the role in the Task. Some entities belong to the same Role but have different meanings, the Subroles allows to distinguish between them. More information in the following sections. Depends on Task Type Is Nullable If the entity allows missing values. Mandatory Dims List with the number of elements each dimension of the array has. The value -1 indicates that that dimension can have an arbitrary number of elements. Required when Data Type is Array Tolerance Specifies the tolerance for image data, defining the acceptable pixel variation in image size. <ul><li>Tol=0: Strict matching, only images of the exact specified size are accepted.</li><li>Tol &gt; 0: Allows a size variation of up to \u00b1Tol pixels in each dimension. For example, if the expected size is (100, 100) and Tol = 5, images between (95, 95) and (105, 105) are accepted.</li><li>Tol=none: Fully flexible, images of any size are allowed.</li></ul> Required when Column Role is Input and Data Structure is Image. Possible values List of values the categorical variable can assume. They can be either strings or numbers. When Task Type is Classification Multilabel and Role is Target, possibile values must be [0, 1] indicating the presence or not of that class. Mandatory when Column Data Type is Categorical Classes Names Names of the classes in the Task. The length of this list must match the length of the Dims of the array. Required when Column Role is Target and Task Type is Classification Multilabel. Image Mode Type of image, it can be RGB, RGBA, GRAYSCALE. It also determines the Data Type, which is Array 3 for RBG and RGBA and Array 2 for GRAYSCALE. Required when Column Role is Input and Data Structure is Image."},{"location":"user_guide/data_schema/#role","title":"Role","text":"<p>The Role defines what the Column object represents for the Task. Roles are used by ML cube Platform to correctly use provided data. Some Roles are needed to uniquely identify a sample, other to retrieve the correct information. Moreover, some Roles must be inserted by you when creating the Data Schema the first time, while others, like the model predictions, are created automatically by ML cube Platform.</p> <p>User defined roles are:</p> Role Data Type Description Mandatory ID String Unique identifier of the sample. It is used during data validation to avoid duplicates of data and to communicate information about data with you without sending the actual data It must be always present when sending data to ML cube Platform. Time ID Float Timestamp of the sample expressed in seconds (for that reason it is a Float). It is used to temporally order samples maintaining coherence in the analysis of ML cube Platform. It must be always present when sending data to ML cube Platform. Metadata Float, Categorical and String Represents additional data that are not used as input by the algorithm but that provide contextual information for each sample. For instance, a metadata column can represents the country code It is optional since it depends on your choice to upload additional information in ML cube Platform Input Any available Data Type Represents input data like a single feature for Tabular tasks or image in Image tasks or text in Text tasks According to Task Type the number of Input Column object varies from 1 to illimitate. See Section Data schema templates Target Any available Data Type. It must be coherent with Task Type Represents the true value of the sample in supervised tasks. It is mandatory for supervised tasks. Input additional embedding Array 1 Embedding vector of the Input Column. It is allowed only then Data Structure of Task is Image or Text. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm. It is optional since it depends on your choice to share with ML cube Platform this type of data. Target additional embedding Array 1 Embedding vector of the Target Column. It is allowed only then Task Type is RAG. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm. It is optional since it depends on your choice to share with ML cube Platform this type of data. <p>ML cube Platform defined roles are:</p> Role Data Type Description Prediction Same Data Type of Target Column Prediction Column object automatically created when the Task Model is created. The name has the fixed template: &lt;MODEL_NAME&gt;@&lt;MODEL_VERSION&gt; Prediction additional embedding Array 1 Embedding vector of the Prediction Column. It is allowed only then Task Type is RAG. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm."},{"location":"user_guide/data_schema/#subrole","title":"Subrole","text":"<p>Some tasks can have different data entities for the same Role, the Column object's attribute Subrole helps to specify the correct type of data.</p> Subrole Associated Role Data Type Description RAG User Input INPUT String In RAG Tasks it is the user query submitted to the system. RAG Retrieved Context INPUT String In RAG Tasks it is the retrieved contexts (separated with the Task attribute context separator) that the retrieval system has selected to answer the query. Model probability PREDICTION Depends on Task Type:<ul><li>RAG: Array 1</li><li>Classification Binary: Float</li><li>Classification Multiclass: Array 1</li><li>Classification Multilabel: Array 1</li><li>Semantic Segmentation: Array 3</li></ul> It is automatically created by ML cube Platform when the created Model has the flag additional probabilistic output set as True. The name has fixed template: &lt;MODEL_NAME&gt;_probability@&lt;MODEL_VERSION&gt;. Object prediction label PREDICTION Array 1 It is automatically created when Task Type is Object detection or Semantic Segmentation. It is an array with length equal to the number of predicted bounding boxes where each element contains the class label assigned to the bounding box. The name has a fixed template: &lt;MODEL_NAME&gt;_predicted_labels@&lt;MODEL_VERSION&gt;. Object target label TARGET Array 1 It is mandatory when Task Type is Object detection or Semantic Segmentation. It is an array with length equal to the number of actual bounding boxes where each element contains the class label assigned to the bounding box."},{"location":"user_guide/data_schema/#data-schema-constraints","title":"Data schema constraints","text":"<p>Each combination of Task Type and Data Structure leads to different Data Schema requirements that must be satisfied when it is created for the Task. For instance, image binary classification tasks requires only one input column object with image data type and target column object must be categorical with only two possible values.</p> <p>Note</p> <p>Object Detection and Semantic Segmentation have specific constraints about the dims attribute of the TARGET and PREDICTION columns:</p> <ul> <li>Object Detection [-1, 4]: the first is for identified objects, the second is for bounding box specification: x_min, x_max, y_min, y_max</li> <li>Semantic Segmentation [-1, -1, 2]: the first is for identified objects, the second is for polygon vertices, the third is for vertices coordinates x, y</li> </ul> <p>Here the list of constraints about quantities for each Role:</p> Task Type Data Structure ID TIME ID INPUT METADATA TARGET INPUT ADDITIONAL EMBEDDING TARGET ADDITIONAL EMBEDDING USER INPUT RETRIEVED CONTEXT OBJECT LABEL TARGET Regression Tabular 1 1 &gt;=1 &gt;= 0 1 0 0 0 0 0 Regression Embedding 1 1 1 &gt;= 0 1 0 0 0 0 0 Regression Image 1 1 1 &gt;= 0 1 &lt;= 1 0 0 0 0 Regression Text 1 1 1 &gt;= 0 1 &lt;= 1 0 0 0 0 Classification Binary Tabular 1 1 &gt;=1 &gt;= 0 1 0 0 0 0 0 Classification Binary Embedding 1 1 1 &gt;= 0 1 0 0 0 0 0 Classification Binary Image 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 Classification Binary Text 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 Classification Multiclass Tabular 1 1 &gt;=1 &gt;=0 1 0 0 0 0 0 Classification Multiclass Embedding 1 1 1 &gt;=0 1 0 0 0 0 0 Classification Multiclass Image 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 Classification Multiclass Text 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 Classification Multilabel Tabular 1 1 &gt;=1 &gt;=0 1 0 0 0 0 0 Classification Multilabel Embedding 1 1 1 &gt;=0 1 0 0 0 0 0 Classification Multilabel Image 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 Classification Multilabel Text 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 0 RAG Text 1 1 2 &gt;=0 0 0 | 2 0 1 1 nan Object Detection Image 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 1 Semantic Segmentation Image 1 1 1 &gt;=0 1 &lt;= 1 0 0 0 1 <p>Here the list of constraints about Data Types for each Role:</p> Task Type Data Structure ID TIME ID INPUT METADATA TARGET INPUT ADDITIONAL EMBEDDING TARGET ADDITIONAL EMBEDDING USER INPUT RETRIEVED CONTEXT OBJECT LABEL TARGET Regression Tabular STRING FLOAT FLOAT, CATEGORY FLOAT, CATEGORY, STRING FLOAT - - - - - Regression Embedding STRING FLOAT ARRAY_1 FLOAT, CATEGORY, STRING FLOAT - - - - - Regression Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING FLOAT ARRAY_1 - - - - Regression Text STRING FLOAT STRING FLOAT, CATEGORY, STRING FLOAT ARRAY_1 - - - - Classification Binary Tabular STRING FLOAT FLOAT, CATEGORY FLOAT, CATEGORY, STRING CATEGORY - - - - - Classification Binary Embedding STRING FLOAT ARRAY_1 FLOAT, CATEGORY, STRING CATEGORY - - - - - Classification Binary Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING CATEGORY ARRAY_1 - - - - Classification Binary Text STRING FLOAT STRING FLOAT, CATEGORY, STRING CATEGORY ARRAY_1 - - - - Classification Multiclass Tabular STRING FLOAT FLOAT, CATEGORY FLOAT, CATEGORY, STRING CATEGORY - - - - - Classification Multiclass Embedding STRING FLOAT ARRAY_1 FLOAT, CATEGORY, STRING CATEGORY - - - - - Classification Multiclass Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING CATEGORY ARRAY_1 - - - - Classification Multiclass Text STRING FLOAT STRING FLOAT, CATEGORY, STRING CATEGORY ARRAY_1 - - - - Classification Multilabel Tabular STRING FLOAT FLOAT, CATEGORY FLOAT, CATEGORY, STRING ARRAY_1 - - - - - Classification Multilabel Embedding STRING FLOAT ARRAY_1 FLOAT, CATEGORY, STRING ARRAY_1 - - - - - Classification Multilabel Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING ARRAY_1 ARRAY_1 - - - - Classification Multilabel Text STRING FLOAT STRING FLOAT, CATEGORY, STRING ARRAY_1 ARRAY_1 - - - - RAG Text STRING FLOAT STRING FLOAT, CATEGORY, STRING - ARRAY_1 - STRING STRING - Object Detection Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING ARRAY_2 ARRAY_1 - - - ARRAY_1 Semantic Segmentation Image STRING FLOAT ARRAY_3 FLOAT, CATEGORY, STRING ARRAY_3 ARRAY_1 - - - ARRAY_1"},{"location":"user_guide/data_schema/#data-schema-templates","title":"Data schema templates","text":"<ul> <li>classification binary embedding</li> <li>classification binary image</li> <li>classification binary tabular</li> <li>classification binary text</li> <li>classification multiclass embedding</li> <li>classification multiclass image</li> <li>classification multiclass tabular</li> <li>classification multiclass text</li> <li>classification multilabel embedding</li> <li>classification multilabel image</li> <li>classification multilabel tabular</li> <li>classification multilabel text</li> <li>object detection</li> <li>rag</li> <li>regression embedding</li> <li>regression image</li> <li>regression tabular</li> <li>regression text</li> <li>semantic segmentation</li> </ul>"},{"location":"user_guide/glossary/","title":"Glossary","text":""},{"location":"user_guide/glossary/#general-terms","title":"General Terms","text":"<ul> <li>User: registered user with username and password that interact with ML cube Platform.  A User can create API keys that inherits his/her permissions and that the he/she uses to communicate with ML cube Platform though API. </li> <li>Company: collection of Users that work with ML cube Platform.  Subscription plan and contracts with ML cube are managed at Company level. A Company has one owner that has all the privileges and that can assign admin role to other Users in the Company.</li> <li>Project: collection of AI Tasks that belong to the same business domain.</li> <li>Task: it is the standard AI problem with a dataset, a target and a set of AI models that predicts the target. All AI models inside the AI Task predicts the same target quantity and they are considered as champion and challengers or deployed and shadow models. Data drift detection is done at Task level because the models uses the same dataset.</li> <li>Model: AI model inside a Task that makes predictions over the Task's dataset.</li> </ul>"},{"location":"user_guide/glossary/#data-terms","title":"Data Terms","text":"<ul> <li>Historical data: data not used for the newest retraining but that belong to the Task. ML cube Platform uses these data during the retraining dataset selection to exploit all the available information. They are not mandatory, if they are not present then the Retraining Tool selects data from the reference set and the production data.</li> <li>Reference data: dataset used as reference for the current model version. It can be the training dataset, the test set or both. Reference data represents the current view of the model over the Task</li> <li>Production data: data the model encounter during production, the production data are monitored by the ML cube Platform detectors to detect drifts</li> <li>Data schema: represents the schema of the data that ML cube Platform uses to know the features and the target columns.</li> </ul>"},{"location":"user_guide/glossary/#drifts-terms","title":"Drifts Terms","text":"<ul> <li>Input drift: statistically significant change in the input data P(X)</li> <li>Concept drift: statistically significant change in the input and target data P(X, y)</li> <li>Model drift: statistically significant change in the model error P(y \u2013 y_pred).</li> <li>Detection Event: event generated by a detector, it has a monitoring target, a severity and a type (warning, drift)</li> </ul>"},{"location":"user_guide/glossary/#actions-terms","title":"Actions Terms","text":"<ul> <li>Importance weights: the retraining dataset is given in form of a set of importance weights associated to every data available for the Task. This importance score will be used during the training pipeline of the customer to weights samples. In particular, the ML model will use the form of the sample weighted loss instead of the standard loss during its retraining phase</li> <li>Dataset boostrapping: if the ML model does not support the sample weighted loss then ML cube Platform can provide a dataset extracted from the available data using sampling with replacement based on the importance weights of the data. The customer can specify the size of the retraining dataset and ML cube Platform provides the best retraining bootstapped dataset of that size</li> <li>Relabeling: in case of concept drift in a classification Task, old labels are no meaningful, given a budget/size constraint ML cube Platform provides the subset of data to be relabelled</li> <li>Active Learning: ML cube Platform provides a set of new synthetic data to label or it provides indication where to collect new real data from the environment</li> </ul>"},{"location":"user_guide/model/","title":"Model","text":"<p>In the ML cube Platform, a Model is a representation of the actual artificial intelligence model used for making predictions. The data used for its training usually represent the reference data distribution, while production data comprises the data on which the model  performs inference. For more information about reference and production data see the Data page.</p> <p>A Model is uniquely associated with a Task, and it can be created both through the WebApp and the Python SDK.  Currently, we support only one model per Task.</p> <p>A Model is defined by a name and a version. The version is updated whenever the model is retrained, allowing to  track the latest version of the model and the data used for its training. When predictions are uploaded to the platform, the model version needs to be appropriately specified, following the guidelines in the Data Schema page, to ensure that the predictions are associated to the correct model version.</p> <p>Note</p> <p>You don't need to upload the real model on the Platform. We only require its training data and predictions. The entity you create on the Platform serves more as a placeholder for the actual model. For this reason, the ML cube Platform is considered model agnostic.</p>"},{"location":"user_guide/model/#rag-model","title":"RAG Model","text":"<p>RAG Tasks represent an exception to the model framework presented before. In this type of Tasks, the model is a Large Language Model (LLM), that is used to generate responses to user queries. The model is not trained on a specific dataset but is rather a pre-trained model, sometimes fine-tuned on custom domain data, which means that the classic process of training and retraining does not apply. </p> <p>To maintain a coherent Model definition across task types, the RAG model is also represented as a Model,  but an update of its version represents an update of the reference data distribution and not necessarily a retraining of the model itself. Moreover, most of the attributes which will be described in the following sections are not applicable, as they are related to the retraining module, which is not available for RAG tasks.</p>"},{"location":"user_guide/model/#llm-specifications","title":"LLM Specifications","text":"<p>For RAG Tasks, you can provide the specifications of the underlying LLMs used in the RAG system.  This information is used by the LLM Security Module to provide insights on the security of the LLMs  used in the RAG system. Currently, we support only LLMs accessible via API.</p> <p>The specifications include the following information:</p> Field Description LLM Provider The provider of the LLM used. LLM name The name of the LLM model. Temperature The temperature used by the LLM model. Top P The top P used by the LLM model. Top K The top K used by the LLM model. Max tokens The max output tokens used by the LLM model. Time intervals The time intervals where the LLM model is used. Role The role assigned to the LLM to interpret (part of the system prompt) Task The task assigned to the LLM to solve (part of the system prompt) Behavior Guidelines A list of guidelines used to define the general behavior of the LLM   (part of the system prompt) Security Guidelines A list of guidelines designed to protect the LLM against attacks, or information leakage  (part of the system prompt) <p>Note</p> <p>Providing the LLM specifications is optional; however, providing them improves the quality of the LLM Security Module insights.</p> LLM Specifications example <p>An example of LLM specifications is:</p> <ul> <li>LLM Provider: \"OpenAI\",</li> <li>LLM name: \"GPT-3\",</li> <li>Temperature: 0.7,</li> <li>Top P: 0.9,</li> <li>Top K: None,</li> <li>Max tokens: 100,</li> <li>Time intervals: \"2022-01-01 00:00:00 - 2022-01-31 23:59:59\",</li> <li>Role: \"You are an helpful assistant, \"</li> <li>Task: \"your goal is to provide accurate and useful information to the users. You must follow these rules:\"</li> <li>Behavior Guidelines: <ol> <li>\"1) Be polite, \" </li> <li>\"2) Be concise, \"</li> </ol> </li> <li>Security Guidelines: <ol> <li>\"3) Do not provide personal information, \"</li> <li>\"4) Do not provide harmful information, \"</li> </ol> </li> </ul> <p>The time intervals represent periods during which a LLM specification is used inside the RAG model. A single LLM Specification can be active across multiple time intervals. </p> <p>For any given platform model, only one LLM specification can be active at a time, though this specification can change over time. It's also possible to designate an LLM as active indefinitely until a new one is introduced. In this case, the end date of the current time interval remains unset. When a new LLM is deployed, you can specify the exact date when the transition occurs.</p> Time Intervals example <p>Considering a single platform Model, is possible to have a situation like this:</p> <ol> <li> <p>LLM specifications id_1, with time intervals:</p> <ul> <li>\"2024-01-01 00:00:00 - 2024-01-31 23:59:59\",</li> <li>\"2024-05-01 00:00:00 - 2024-05-31 23:59:59\",</li> </ul> </li> <li> <p>LLM specifications id_2, with time intervals:</p> <ul> <li>\"2024-02-01 00:00:00 - 2024-04-30 23:59:59\",</li> <li>\"2024-06-01 00:00:00 - &lt;NOT SET&gt;\",</li> </ul> </li> </ol> <p>In this case, the current LLM specification is id_2.  If a new LLM specification id_3 is introduced, or an old one is re-set, with a start date of \"2024-11-11 00:00:00\", the time interval of id_2 will be updated to:</p> <ul> <li>\"2024-02-01 00:00:00 - 2024-04-30 23:59:59\", </li> <li>\"2024-06-01 00:00:00 - 2024-11-10 23:59:59.\"</li> </ul>"},{"location":"user_guide/model/#probabilistic-output","title":"Probabilistic output","text":"<p>When creating a model, you can specify if you want to provide also the probabilistic output of the model along with the predictions.  The probabilistic output represents the probability or confidence score associated with the model's predictions. If provided,  the ML cube Platform will use this information to compute additional metrics and insights.</p> <p>It is optional and currently supported only for Classification and RAG tasks. If specified, the probabilistic output must be provided  as a new column in the predictions file, following the guidelines in the Data Schema page.</p> <p>Example</p> <p>For example, Logistic Regression classification model provides both the probability of belonging to the positive class and the predicted class using a threshold. In this case, you can upload to ML cube Platform the predicted class as principal prediction and the probability as probabilistic output.</p>"},{"location":"user_guide/model/#model-metric","title":"Model Metric","text":"<p>A Model Metric represents the evaluation metric used to assess the performance of the model.  It can both represent a performance or an error. The chosen metric will be used in the various views of the WebApp to provide insights on the model's performance and in the Performance View section of the Retraining Module.</p> <p>Note</p> <p>Note that model metrics can only be computed when target data are available.</p> <p>The available options are:</p> Metric Task Type Accuracy Classification tasks RMSE Regression tasks R2 Regression tasks Average Precision Object Detection tasks <p>RAG tasks have no metric, as in that case the model is an LLM for which classic definitions of metrics are not applicable.</p> <p>Warning</p> <p>Model Metrics should not be confused with Monitoring Metrics, which are entities being monitoring by the ML cube Platform and not necessarily related to a Model.</p>"},{"location":"user_guide/model/#suggestion-type","title":"Suggestion Type","text":"<p>The Suggestion Type represents the type of suggestion that the ML cube Platform should provide when computing the  Retraining Dataset. The available options are provided in the related section.</p>"},{"location":"user_guide/model/#retraining-cost","title":"Retraining Cost","text":"<p>The Retraining Cost represents the cost associated with retraining the model. This information is used by the Retraining Module to provide gain-cost analysis and insights on the retraining process. The cost is expressed in the same currency as the one used in the Task cost information. The default value is 0.0, which means that the cost is negligible.</p>"},{"location":"user_guide/model/#retrain-trigger","title":"Retrain Trigger","text":"<p>You can associate a Retrain Trigger to your Model in order to enable the automatic initiation of your retraining pipeline  from the ML cube Platform. More information on how to set up a retrain trigger can be found in the related section.</p>"},{"location":"user_guide/project/","title":"Project","text":"<p>A Project is the secondary organizational entity in ML cube Platform. A Project groups together a set of artificial intelligence algorithms that share a common goal measured by a set of KPIs. For this reason, it is composed of several Tasks that collaborate to reach the Project's goal.</p> <p>Users in the Company can access to one or more Projects according to their roles.</p>"},{"location":"user_guide/project/#creation","title":"Creation","text":"<p>When a Project is created, the User specifies its name, description, and selects the default storage policy.</p> <p>Storage Policy defines the default behavior the Platform follows to access data that are shared with it. Indeed, data shared with ML cube Platform can either be duplicated and stored in ML cube private cloud storage or stay only on customer's cloud and accessed as a remote data source.</p>"},{"location":"user_guide/project/#demo-projects","title":"Demo Projects","text":"<p>To better explore ML cube Platform modules and features, it is possible to create Demo Projects that are not taken into account by subscription quotas. ML cube Platform provides different Demo Projects that cover all the possible use cases (regression, classification, text data, image data, RAG, object detection and so on). To create a Demo Project, you need to check the \"Demo Project\" checkbox and select the one you prefer.</p>"},{"location":"user_guide/project/#kpi-monitoring","title":"KPI Monitoring","text":"<p>A Key Performance Indicator is a measure of performance over time for a specific objective.  While artificial intelligence algorithms try to minimize their loss function, artificial intelligence based solutions and applications look at KPIs. Therefore, it is essential to monitor Project's KPIs along with algorithm performance to have a complete view of the current situation.</p> <p>ML cube Platform offers the possibility to upload Project's KPIs to monitor them via drift detection algorithms. That enables the detection of potentially dangerous trends in what really matters from the business point of view. KPI Monitoring page in the Project sidebar shows the registered KPIs, their trends and drift events ML cube Platform detected during time.</p>"},{"location":"user_guide/project/#integrations","title":"Integrations","text":"<p>ML cube Platform is part of the artificial intelligence and cloud ecosystem and provides connectors to interact with Cloud Providers and MLOps solutions. The Integrations page allows to create and manage credentials that will be used by the Project's Tasks.</p> <p>For instance, if data are stored in a Google Cloud Storage bucket, adding the Google Cloud Platform credentials with the right permissions, allows ML cube Platform to read data from it.</p> <p>Another example is to trigger a Sage Maker pipeline to retrain your artificial intelligence model with a dataset provided by ML cube Platform. In this case, you can create Amazon Web Services credentials with permission to create an event on Amazon Event Bridge. See the Integrations page for more information about credentials setup, data sources and retraining triggers.</p>"},{"location":"user_guide/project/#jobs-monitoring","title":"Jobs Monitoring","text":"<p>Operations like sharing data to ML cube Platform, submitting the creation of a retraining dataset or reports like RAG evaluation, trigger the execution of asynchronous pipelines in ML cube Platform cloud infrastructure. Each pipeline is associated with an identifier named job id that can be used to monitor its execution status. This monitoring can be done both from Web App in the Job Status page and, with specific SDKs method allowing automation. A job failure can be either due to bad requests or internal errors, you can check the error message information via the same page.</p>"},{"location":"user_guide/rbac/","title":"User Roles","text":"<p>The Company owner can create new Users assigning to them a Role. The Role defines a set of actions a User has inside the application. Each User has associated a company role and one or more project roles.</p> <p> </p> User Roles in ML cube Platform. <p>The following tables shows the User roles:</p>"},{"location":"user_guide/rbac/#company-permissions","title":"Company Permissions","text":"Role DELETE_COMPANY CHANGE_COMPANY_OWNER MANAGE_COMPANY_ADMIN MANAGE_COMPANY_USER CHANGE_COMPANY_USER_ROLE UPDATE_COMPANY_INFORMATION READ_COMPANY CREATE_PROJECT COMPANY_OWNER COMPANY_ADMIN COMPANY_USER COMPANY_NONE"},{"location":"user_guide/rbac/#project-permissions","title":"Project Permissions","text":"Role DELETE_PROJECT MANAGE_PROJECT_ADMIN UPDATE_PROJECT_INFORMATION MANAGE_PROJECT_USER CHANGE_PROJECT_USER_ROLE WORK_ON_PROJECT READ_PROJECT COMPANY_OWNER COMPANY_ADMIN COMPANY_USER COMPANY_NONE PROJECT_ADMIN PROJECT_USER PROJECT_VIEW"},{"location":"user_guide/segment/","title":"Segment","text":"<p>A Segment is a subset of the data distribution that identifies a sub-domain inside the data. It is defined by a set of rules over data dimensions and metadata. A Task can include several Segments. There are no constraints about how they are specified: they can be disjoint, overlapping, or even nested. In other words, a sample can belong to all the Segments, none of them, or just some of them.</p> <p>When Segments are specified for a Task, monitoring is performed both on the whole data, called all population, and for each Segment. The objective of a Segment is to allow the analysis of specific groups of data, whose variations might go unnoticed if only the whole population is monitored.</p> <p>Segments, similarly to the Data Schema and the Model, must be defined before sending any data to the Platform. They must be created all at once, as they can't be modified upon creation. Additionally, their definition needs to happen after the creation of the Data Schema, as the rules of the Segment are based on the columns defined there, and also after the creation of the Model.</p>"},{"location":"user_guide/segment/#segment-structure","title":"Segment Structure","text":"<p>The Segment structure is very simple, as it only requires the definition of two fields:</p> Field Description Name The name of the Segment. It is used to display information related to the segment in the Web App. Rules A list of rules defining the subset of the population. These rules are applied in AND between them, which means that a sample belongs to the Segment if all the conditions expressed by the rules are satisfied. To prevent potential conflicts, it is not possible to define two rules over the same column. <p>Segments can be created both through the Web App and the SDK.</p>"},{"location":"user_guide/segment/#segment-rules","title":"Segment Rules","text":"<p>A rule is a condition over a single data dimension that a specific sample must match to be considered part of the Segment. Each Segment has from one to several rules, which are applied in AND between them. A rule is defined by the following fields:</p> Field Description Column name The name of the column in the Data Schema that the rule is applied to. A rule can be applied only on columns of role <code>INPUT</code>, <code>TARGET</code> and <code>METADATA</code> Operator The operator defining the rule. It can be either <code>IN</code>, meaning that the element matches the criteria, or <code>OUT</code>, indicating it does not satisfy the criteria. Values This field can have two possible meaning, according to the data type of the column specified in the rule: <ul><li>The data type is float: Values is a series of ranges [a, b] that define the numeric intervals over which the operator is applied. The range is closed, meaning that the extremes are always included. When operator is <code>IN</code>, the ranges are in OR, whereas, when the operator is <code>OUT</code> they are in AND. It is possible to set an open extreme by not specifying it (At least one of the extremes needs to be specified.) </li><li>The data type is categorical or string: Values is a list which elements must match the content of the column. When operator is <code>IN</code>, the column value must be one of the specified elements, while, when operator is <code>OUT</code> it must not be one of them. </li></ul>"},{"location":"user_guide/segment/#examples","title":"Examples","text":"<p>Let's consider a simple example where we have a dataset with the following columns:</p> Sample ID X_0 X_1 Y Metadata_1 Metadata_2 id_0 10 20 class_0 A1 B2 id_1 11 21 class_0 A2 B1 id_2 12 22 class_1 A1 B2 id_3 13 23 class_1 A2 B1 id_4 14 24 class_0 A1 B2 <p>It represents a binary classification problem where the target is the column <code>Y</code> and the input features are <code>X_0</code> and <code>X_1</code>. Columns <code>Metadata_1</code> and <code>Metadata_2</code> are metadata columns.</p> <p>Let's now define some possible segments of increasing complexity.</p> <ul> <li>A Segment that includes all samples where the value of the column <code>X_0</code> is between 10 and 12:</li> </ul> Field Value Name Segment_1 Rules Only 1 rule is needed: <ol><li> <ul> <li>Column name: X_0</li> <li>Operator: IN </li> <li>Values: [10, 12] </li> </ul> </li></ol> <p>This segment would include the samples with <code>Sample ID</code> equal to <code>id_0</code>, <code>id_1</code> and <code>id_2</code>.</p> SDK Example <p>You can define the previous segment using the SDK with the following code:</p> <pre><code>client.create_task_segments(\n            task_id=task_id,\n            segments=[\n                Segment(name=f'Segment 1',\n                        rules=[\n                            NumericSegmentRule(\n                                column_name='X_0',\n                                operator=SegmentOperator.IN,\n                                values=[SegmentRuleNumericRange(start_value=10, end_value=12)]\n                            )\n                        ]\n                )\n            ]\n        )\n</code></pre> <ul> <li>A Segment that includes all samples where the value of the column <code>X_0</code> is greater or equal than 13 and the value of the column <code>X_1</code> is strictly less than 24:</li> </ul> Field Value Name Segment_2 Rules We need 2 rules: <ol><li> <ul> <li>Column name: X_0</li> <li>Operator: IN </li> <li>Values: [13, +inf] </li> </ul> </li><li> <ul> <li>Column name: X_1</li> <li>Operator: IN </li> <li>Values: [-inf, 23] </li> </ul> </li></ol> <p>This segment would include the sample with <code>Sample ID</code> equal to <code>id_3</code>.</p> SDK Example <p>You can define the previous segment using the SDK with the following code:</p> <pre><code>client.create_task_segments(\n            task_id=task_id,\n            segments=[\n                Segment(name=f'Segment 2',\n                        rules=[\n                            NumericSegmentRule(\n                                column_name='X_0',\n                                operator=SegmentOperator.IN,\n                                values=[SegmentRuleNumericRange(start_value=13)]\n                            ),\n                            NumericSegmentRule(\n                                column_name='X_1',\n                                operator=SegmentOperator.IN,\n                                values=[SegmentRuleNumericRange(end_value=23)]\n                            )\n                        ]\n                )\n            ]\n        )\n</code></pre> <p>Notice how leaving one end of the interval empty means that the interval is unbounded in that direction.</p> <ul> <li>A Segment that includes all samples where the value of the column <code>X_0</code> either lower or equal than 10 or greater or equal than 14 and the value of the metadata column <code>Metadata_1</code> is equal to <code>A1</code> or <code>A3</code>:</li> </ul> Field Value Name Segment_3 Rules We need 2 rules: <ol><li> <ul> <li>Column name: X_0</li> <li>Operator: IN </li> <li>Values: [-inf, 10], [14, +inf] </li> </ul> </li><li> <ul> <li>Column name: Metadata_1</li> <li>Operator: IN </li> <li>Values: [A1, A3] </li> </ul> </li></ol> <p>This segment would include the samples with <code>Sample ID</code> equal to <code>id_0</code> and <code>id_4</code>. Notice that, even though there is no sample with the value of <code>Metadata_1</code> equal to <code>A3</code>, there are still samples belonging to the segment because the values of the rules are applied in OR between them.</p> SDK Example <p>You can define the previous segment using the SDK with the following code:</p> <pre><code>client.create_task_segments(\n            task_id=task_id,\n            segments=[\n                Segment(name=f'Segment 3',\n                        rules=[\n                            NumericSegmentRule(\n                                column_name='X_0',\n                                operator=SegmentOperator.IN,\n                                values=[SegmentRuleNumericRange(end_value=10), SegmentRuleNumericRange(start_value=14)]\n                            ),\n                            CategoricalSegmentRule(\n                                column_name='Metadata_1',\n                                operator=SegmentOperator.IN,\n                                values=['A1', 'A3']\n                            )\n                        ]\n                )\n            ]\n        )\n</code></pre> <ul> <li>A Segment that includes all samples where the value of the column <code>X_1</code> is not between 21 and 23, the value of the target <code>y_0</code> is equal to <code>class_0</code> and the value of the metadata column <code>Metadata_2</code> is different from <code>B1</code>:</li> </ul> Field Value Name Segment_4 Rules We need 3 rules: <ol><li><ul>Column name: X_1 <li>Operator: OUT </li> <li>Values: [21, 23] </li> </ul> </li><li> <ul> <li>Column name: y_0</li> <li>Operator: IN </li> <li>Values: [class_0] </li> </ul> </li><li> <ul> <li>Column name: Metadata_2</li> <li>Operator: OUT </li> <li>Values: [B1] </li> </ul> </li></ol> <p>This segment would include the samples with <code>Sample ID</code> equal to <code>id_2</code> and <code>id_4</code>.</p> SDK Example <p>You can define the previous segment using the SDK with the following code:</p> <pre><code>client.create_task_segments(\n            task_id=task_id,\n            segments=[\n                Segment(name=f'Segment 3',\n                        rules=[\n                            NumericSegmentRule(\n                                column_name='X_1',\n                                operator=SegmentOperator.OUT,\n                                values=[SegmentRuleNumericRange(end_value=21), SegmentRuleNumericRange(start_value=23)]\n                            ),\n                            CategoricalSegmentRule(\n                                column_name='y_0',\n                                operator=SegmentOperator.IN,\n                                values=['class_0']\n                            ),\n                            CategoricalSegmentRule(\n                                column_name='Metadata_1',\n                                operator=SegmentOperator.IN,\n                                values=['A1']\n                            )\n                        ]\n                )\n            ]\n        )\n</code></pre>"},{"location":"user_guide/task/","title":"Task","text":"<p>A Task is the third and last organizational entity in ML cube Platform. A Task represents an ordinary artificial intelligence task like regression, classification, text generation or object detection.</p> <p>A Task is associated with a Model and a Data schema that describes all the information about the data in the Task.</p> <p>A Task is associated with a unique identifier that will be used by SDK to operate on it. The identifier can be retrieved from the Task homepage or by looking at the url.</p> <p>A Task has a status that summarizes the health of its AI model. The status depends on the monitoring module and changes from Ok, Warning or Drift when the Monitoring modules detect drifts on monitored quantities.</p> <p>Moreover, in the Task homepage is present the section named \"Data events\" which shows the most recent detection events generated by the monitoring module. It is possible to click on view to see more details or discard the notification (the event will remain available for future analysis on the Detection page).</p>"},{"location":"user_guide/task/#attributes","title":"Attributes","text":"<p>A Task is described by a set of attributes specified during its creation. Some attributes are common for every Task, while others vary according to its type. Generic attributes are:</p> Attribute Description Name Name of the Task, unique for the Project. Tags Optional customizable list of tags. They are used to better describe the Task and to improve search. Task type Artificial intelligence type of Task. Possible values are:<ul><li>Regression</li><li>Binary classification</li><li>Multiclass classification</li><li>Multilabel classification</li><li>Retrieval Augmented Generation</li><li>Object Detection</li><li>Semantic Segmentation</li></ul> Data structure Type of input data the Task uses. Possible values are:<ul><li>Tabular: standard table based data used in contexts like regression or classification.</li><li>Image: images in their different formats and channels.</li><li>Text: textual data expressed as strings. When data structure is Text, attribute Text Language is required.</li><li>Embedding: input data are arrays that could represent embedding either image or text data. This data structure is used when raw data are not shared with ML cube Platform.</li></ul> Optional target Boolean value that specifies if the ground truth is always available or not. In some Tasks, the actual value is not present until explicit labeling is done. In this cases, the Task is marked as with optional target so that ML cube Platform works accordingly. Cost info Optional information about costs that depend on Task Type. Text language Which language is used in the Task when input data structure is text. Positive class Required when Task Type is Binary Classification, it indicates the positive class to be predicted. Context separator Available when Task Type is RAG, it specifies the string separator to split retrieved context into different chunks. Default answer Available when Task Type is RAG, it specifies the default answer to be used when no retrieved context is available. Target Type Available when Task Type is Semantic Segmentation, it specifies how target and predictions are specified in this task. For now, the only available option is Polygon. See Semantic Segmentation details for more information. <p>Warning</p> <p>Some Task's attributes are immutable: type, data structure and optional target flag cannot be modified after the creation of the Task.</p>"},{"location":"user_guide/task/#platform-modules-and-task-type-compatibility","title":"Platform modules and Task Type compatibility","text":"<p>Most of ML cube Platform operations are done at Task level: monitoring, retraining, analytics and other features are specific to AI models and data that belong to a Task. Indeed, each Task Type has a set of ML cube Platform modules:</p> Module Regression Classification RAG Object Detection Semantic Segmentation Monitoring Explainability Retraining Topic Modeling RAG Evaluation LLM Security <p>Tip</p> <p>On the left side of the web app page the Task menu is present, with links to the above-mentioned modules and Task settings.</p>"},{"location":"user_guide/task/#task-type","title":"Task Type","text":"<p>ML cube Platform supports several Task Types providing specific features for each of them. Not all Task Types are compatible with data structures, in the table below are shown which data structure is supported by which Task Type:</p> Task Type Tabular Image Text Embedding Regression Classification RAG Object Detection Semantic Segmentation <p>In the following sections, you can find a description of each Task Type with its specific information.</p>"},{"location":"user_guide/task/#regression","title":"Regression","text":"<p>Supervised regression Task with continuous target.</p>"},{"location":"user_guide/task/#cost-information","title":"Cost information","text":"<p>Cost information is expressed by two proportional coefficients \\(c_{o}\\) and \\(c_{u}\\):</p> <ul> <li>\\(c_{o}\\) is the cost of overestimating the target value, i.e., when \\(\\hat{y} &gt; y\\)</li> <li>\\(c_{u}\\) is the cost of underestimating the target value. i.e., when \\(\\hat{y} &lt; y\\)</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{i | \\delta_i &lt; 0} |\\delta_i| \\times c_{o} + \\sum_{i | \\delta_i &gt; 0} \\delta_i \\times c_{u}}{N} $$ where \\(\\delta_i = y_i - \\hat{y}_i\\) is the different between the target and the estimated value.</p>"},{"location":"user_guide/task/#classification","title":"Classification","text":"<p>Supervised classification Task with discrete target. Classification Tasks divides in:</p> <ul> <li>Binary: when then target is a binary variable. For binary classification tasks additional positive class attribute must be specified indicating which value is considered as the positive one. For instance, in fraud detection classification task \"1\" can represent that the sample is a fraud, while \"0\" when it is not. In that case positive class attribute is \"1\".</li> <li>Multiclass: when the target is a categorical variable with more than two possible values but only one value can be assigned.</li> <li>Multilabel: when the target is an array indicating which of the possible categories are present. In this case, each element can be either 0 or 1, and more than one element of the array can be 1.</li> </ul>"},{"location":"user_guide/task/#cost-information_1","title":"Cost information","text":"<p>Cost information differs from each of the three classification types, however, the concept is similar. A cost is associated to every misclassification possibility:</p> <ul> <li> <p>Binary:</p> <ul> <li>\\(c_{FP}\\) is the cost of classifying a negative sample as positive</li> <li>\\(c_{FN}\\) is the cost of classifying a positive sample as negative</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{N_{FP} \\times c_{FP} + N_{FN} \\times c_{FN}}{N} $$ where \\(N_{FP}\\) and \\(N_{FN}\\) are the number of false positives and false negatives respectively.</p> </li> <li> <p>Multiclass:</p> <ul> <li>\\(c_{k}\\) is the cost of misclassifying a sample, whose actual class is \\(k\\), with another class</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{k} N_{k} \\times c_{k} }{N} $$ where \\(N_{k}\\) is the number of misclassified samples of class \\(k\\).</p> </li> <li> <p>Multilabel:</p> <ul> <li>\\(c_{FP}^{k}\\) is the cost of classifying a sample as class \\(k\\) when the actual class \\(k\\) is not present</li> <li>\\(c_{FN}^{k}\\) is the cost of not classifying a sample as class \\(k\\) when the actual class \\(k\\) is present</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{k} N_{FP}^{k} \\times c_{FP}^{k} + N_{FN}^{k} \\times c_{FN}^{k}}{N} $$ where \\(N_{FP}^{k}\\) and \\(N_{FN}^{k}\\) are the number of false positives and false negatives of class \\(k\\) respectively</p> </li> </ul>"},{"location":"user_guide/task/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>Retrieval Augmented Generation is a particular AI task for Text data based on Large Language Models, in which they are used to generate responses of user query using a set of retrieved documents as context to provide a precise and more focused response.</p> <p>RAG Tasks, do not have a Target therefore, the attribute optional target is always set to True. Moreover, in this Task, the Prediction is a text as well. While the input is composed of two entities:</p> <ul> <li>User Input: the user query that the model needs to answer</li> <li>Retrieved Context: the set of documents the retrieval engine selected to help the model</li> </ul> <p>RAG Tasks have two additional attributes:</p> <ul> <li> <p>Context separator: which is a string used to separate different retrieved contexts into chunks. Context data is sent as a single string, however, in RAG settings multiple documents can be retrieved. In this case, context separator is used to distinguish them. It is optional since a single context can be provided.</p> <p>Example</p> <p>Context separator: &lt;&lt;sep&gt;&gt;</p> <p>Context data: The capital of Italy is Rome.&lt;&lt;sep&gt;&gt;Rome is the capital of Italy.&lt;&lt;sep&gt;&gt;Rome was the capital of Roman Empire.</p> <p>Contexts:</p> <pre><code>- The capital of Italy is Rome.\n- Rome is the capital of Italy.\n- Rome was the capital of Roman Empire.\n</code></pre> </li> <li> <p>Default answer: which is a string used when no retrieved context is available. It is optional since other way to handle this situation are available.</p> <p>Example</p> <p>Default answer: \"I am sorry, I cannot help you with that request.\"</p> </li> </ul>"},{"location":"user_guide/task/#object-detection","title":"Object Detection","text":"<p>Object Detection task processes images and provides as output a list of bounding boxes with associated label indicating the type of identified entity. Therefore, target is a list of four elements tuples indicating the x_min, x_max, y_min and y_max of the box and a string label for the entity type.</p>"},{"location":"user_guide/task/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Semanric segmentation task processes images and provides as output a list of entities identified in the image with label indicating the type. The target can assume different forms, the Task attribute Target Type is used to specify it. When target type is Polygon, the entity is represented as a list of verices with x,y coordinates that defines the vertices of the polygon.</p>"},{"location":"user_guide/integrations/","title":"Integrations","text":"<p>Below, you will find a guide that will help you create the credentials and configure the permissions that ML cube will use to access your resources on the supported cloud providers.</p>"},{"location":"user_guide/integrations/#creating-the-credentials","title":"Creating the credentials","text":"Amazon Web ServicesGoogle Cloud PlatformMicrosoft AzureAWS Compatible <p>The ML cube Platform can assume an IAM Role on your AWS Account, that can be used to authorize actions on specific resources.</p> <p>You will need to create the credentials through the ML cube Platform SDK or the web application.</p> <p>Example</p> <p>The following code will create a set of AWS credentials for an IAM Role called <code>YOUR_ROLE_NAME</code> (change this as desired).</p> <pre><code>aws_creds = client.create_aws_integration_credentials(\n    name='AWS_01',\n    default=True,  # Set these credentials as the default to use when not specified\n    project_id='your_project_id',\n    role_arn='arn:aws:iam::{{ YOUR_AWS_ACCOUNT_ID }}:role/{{ YOUR_ROLE_NAME }}',\n)\n\nprint(aws_creds.trust_policy)\n</code></pre> <p>Now, log into your AWS account and open the AWS console. Here, go to the IAM service, navigate to the Roles section and create a new role. It is important to set the trust policy to the one you just obtained.</p> <p>Right now, your IAM Role grants no permissions. Please refer to the next sections that will explain how to set up IAM Policies for S3, Event Bridge and so on.</p> <p>To revoke access, simply delete the role or change the trust policy.</p> <p></p> <p>The ML cube Platform can operate in your GCP Account through the creation of a dedicated Service Account. You will then be able to assign one or more IAM Roles to this Service Account, to allow the ML cube Platform to perform specific actions.</p> <p>To configure a Service Account for ML cube Platform, log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create the Service Account with the required permissions. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change this according to your project\nexport GCP_PROJECT=my-project\n\n# Creates a service account called ml3PlatformServiceAccount\ngcloud iam service-accounts create ml3PlatformServiceAccount --display-name \"ML3 Platform Service Account\"\n\n# Generates the access key that will be used to authenticate as the service account\ngcloud iam service-accounts keys create ml3-platform-key.json --iam-account=ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com\n\n# Displays the access key to the terminal screen\ncat ml3-platform-key.json\n</code></pre> <p>Copy the JSON object containing the key and save it to a file with the same name on your disk. Now, you will need to create the GCP credentials, either through the SDK or the web application, and provide the contents of the JSON file you just created.</p> <p>Example</p> <p>The following code will create a set of GCP credentials that will be able to access the service account.</p> <pre><code>with open('path/to/ml3-platform-key.json', 'r') as f:\n    creds_json = f.read()\n\n    gcp_creds = client.create_gcp_integration_credentials(\n        name='GCP_01',\n        default=True,  # Set these credentials as the default to use when not specified\n        project_id='your_project_id',\n        service_account_info_json=creds_json\n    )\n</code></pre> <p>Right now, your IAM Role grants no permissions. Please refer to the next sections that will explain how to set up IAM Policies for Google Cloud Storage, Pub/Sub and so on.</p> <p></p> <p>The ML cube Platform can operate in your Azure Account through the creation of a dedicated Service Principal. You will then be able to assign one or more Roles to this Service Principal, to allow the ML cube Platform to perform specific actions.</p> <p>To configure a Service Principal for ML cube Platform, log into your Azure account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Set the Cloud Shell to use bash instead of powershell. Now we will enter the following command that will create the Service Principal.</p> <pre><code>az ad sp create-for-rbac --name ML3PlatformSP --skip-assignment\n</code></pre> <p>Once the operation finishes running, it will output a JSON object with the following fields: <code>appId</code>, <code>displayName</code>, <code>password</code> and <code>tenant</code>. Copy this object and save it to a file on your disk, for example <code>azure-credentials.json</code>.</p> <p>Now, you will need to create the Azure credentials, either through the SDK or the web application, and provide the contents of the JSON file you just created.</p> <p>Example</p> <p>The following code will create a set of Azure credentials that will be able to access the service account.</p> <pre><code>with open('path/to/azure-credentials.json', 'r') as f:\n    creds_json = f.read()\n\n    azure_creds = client.create_azure_integration_credentials(\n        name='AZURE_01',\n        default=True,  # Set these credentials as the default to use when not specified\n        project_id='your_project_id',\n        service_principal_credentials_json=creds_json\n    )\n</code></pre> <p></p> <p>The ML cube Platform can be connected to AWS compatible services through security credentials.</p> <p>For example, to use min.io's S3-compatible storage, you can use the username and password of your min.io's user as the security credentials. You also need to provide the endpoint where your min.io instance can be reached. You will need to create the credentials through the ML cube Platform SDK or the web application.</p> <p>Example</p> <p>The following code will create a set of AWS-compatible credentials.</p> <pre><code>aws_compatible_creds = client.create_aws_compatible_integration_credentials(\n    name='AWS_COMPATIBLE_01',\n    default=True,  # Set these credentials as the default to use when not specified\n    project_id='your_project_id',\n    access_key_id='username',\n    secret_access_key='password',\n    endpoint_url='https://my-service.com:1234'\n)\n</code></pre> <p>If <code>endpoint_url</code> is set to <code>None</code>, these credentials can be used to access AWS itself as an IAM User.</p> <p>Warning</p> <p>Using AWS-compatible credentials to access AWS itself is discouraged, and should only be used in special cases where you are not able to use an IAM Role.</p>"},{"location":"user_guide/integrations/#storage-integration","title":"Storage integration","text":"AWS S3Google Cloud StorageAzure Blob StorageAWS S3 Compatible <p>Log into your AWS account and open the AWS console, then go to the IAM service and navigate to the Policies section. Here, we will create an IAM Policy.</p> <p>Example</p> <p>The following policy will grant read access to objects in the <code>my-company-data-bucket</code> to the IAM entity it is attached to.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketTagging\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-company-data-bucket\",\n                \"arn:aws:s3:::my-company-data-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Once the IAM Policy has been created, navigate to the Roles section, select the IAM Role you previously created and finally attach the IAM Policy to it.</p> <p></p> <p>Log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create an IAM Role with the required permissions, bind it to the service account you previously created, and grant access to the bucket. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change these according to your project and bucket\nexport GCP_PROJECT=my-project\nexport GCP_BUCKET=my-company-data-bucket\n\n# Creates an IAM Role called ml3PlatformStorageRole for your project, with read permissions on buckets and objects in the storage service\ngcloud iam roles create ml3PlatformStorageRole --project=$GCP_PROJECT --title=\"ML3 Platform Storage Role\" --description=\"Role that allows the ML cube Platform to access storage resources in a project\" --permissions=storage.buckets.get,storage.buckets.list,storage.objects.get,storage.objects.list --stage=ALPHA\n\n# Adds the IAM Role to the previously created service account\ngcloud projects add-iam-policy-binding $GCP_PROJECT --member=serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com --role=projects/$GCP_PROJECT/roles/ml3PlatformStorageRole\n\n# Allows the service account we created to access the given bucket with the objectViewer role\ngsutil iam ch serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com:roles/storage.objectViewer gs://$GCP_BUCKET\n</code></pre> <p></p> <p>Log into your Azure account and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. The following command will associate the previously created Service Principal with a role that is able to read data from a given blob container.</p> <pre><code># Change these according to your project and storage configuration\nexport SERVICE_PRINCIPAL_APP_ID=my-sp-app-id\nexport SUBSCRIPTION_ID=my-azure-subscription-id\nexport RESOURCE_GROUP=my-azure-resource-group\nexport STORAGE_ACCOUNT=my-storage-account\nexport BLOB_CONTAINER=my-blob-container\n\naz role assignment create --assignee $SERVICE_PRINCIPAL_APP_ID --role \"Storage Blob Data Reader\" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT/blobServices/default/containers/$BLOB_CONTAINER\n</code></pre> <p></p> <p>Log into your AWS S3 compatible service and set up a policy that allows to access objects in the bucket of your choice.</p> <p>Example</p> <p>The following policy will grant read access to objects in the <code>my-company-data-bucket</code>.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketTagging\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-company-data-bucket\",\n                \"arn:aws:s3:::my-company-data-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Adjust the policy according to the specifics of the S3-compatible service you are using.</p> <p>Make sure the policy is attached to the user whose credentials you are going to configure as AWS-compatible credentials in the ML cube platform.</p>"},{"location":"user_guide/integrations/#retrain-events-integration","title":"Retrain events integration","text":"Amazon EventBridgeGCP Pub/SubAzure Event Grid <p>Log into your AWS account and open the AWS console, then go to the IAM service and navigate to the Policies section. Here, we will create an IAM Policy.</p> <p>Example</p> <p>The following policy will allow an IAM Entity to put events in a specific event bus.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"events:PutEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:events:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:event-bus/&lt;EVENT_BUS_NAME&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Replace <code>&lt;REGION&gt;</code>, <code>&lt;ACCOUNT_ID&gt;</code> and <code>&lt;EVENT_BUS_NAME&gt;</code> with your desired values.</p> <p>Once the IAM Policy has been created, navigate to the Roles section, select the IAM Role you previously created and finally attach the IAM Policy to it.</p> <p></p> <p>Log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create an IAM Policy with the required permissions, bind it to the service account you previously created, and grant access to the Pub/Sub topic. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change these according to your project and topic\nexport GCP_PROJECT=my-project\nexport GCP_TOPIC=my-topic\n\n# Adds a new IAM Policy to the previously created service account, granting publish access to the Pub/Sub topic\ngcloud pubsub topics add-iam-policy-binding $GCP_TOPIC --member=serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com --role=roles/pubsub.publisher\n</code></pre> <p></p> <p>Log into your Azure account and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. The following command will associate the previously created Service Principal with a role that is able to publish events to an Event Grid topic.</p> <pre><code># Change these according to your project and storage configuration\nexport SERVICE_PRINCIPAL_APP_ID=my-sp-app-id\nexport SUBSCRIPTION_ID=my-azure-subscription-id\nexport RESOURCE_GROUP=my-azure-resource-group\nexport TOPIC_ID=my-topic\n\naz role assignment create --assignee $SERVICE_PRINCIPAL_APP_ID --role \"EventGrid Data Sender\" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.EventGrid/topics/$TOPIC_ID\n</code></pre>"},{"location":"user_guide/integrations/data_sources/","title":"Data Sources","text":"<p>This page provides guidance on integrating external data sources into your ML cube Platform project. There are three types of data sources you can use to enable the ML cube Platform to access your data:</p> <ol> <li>Local data source</li> <li>Remote data source with ML cube Storage</li> <li>Remote data source with Customer Storage</li> </ol>"},{"location":"user_guide/integrations/data_sources/#local-data-source","title":"Local Data Source","text":"<p>This is the easiest way to send data to the ML cube Platform. You can simply specify the path to the local file you want to upload, and it will be uploaded to the ML cube Platform Secure Storage, where it is automatically processed.</p> <p>Example</p> <pre><code>data = TabularData(\n    source=LocalDataSource(\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n        file_path=file_path,\n    )\n)\n</code></pre>"},{"location":"user_guide/integrations/data_sources/#remote-data-source","title":"Remote Data Source","text":"<p>While using a local data source can be a good way to try out the features that the ML cube Platform can offer, in a production scenario you will usually want to integrate your remote data sources with your project. You can decide where you want your data to be stored.</p>"},{"location":"user_guide/integrations/data_sources/#ml-cube-storage","title":"ML cube Storage","text":"<p>This method should be used when the ML cube Platform is allowed to perform a one-time read of the raw data from your data source, and make a copy of it inside the ML cube Platform Secure Storage. This allows the ML cube Platform to store a server-side copy of the processed dataset for quicker access and analysis. Remember that you can request the deletion of your data at any time.</p>"},{"location":"user_guide/integrations/data_sources/#customer-storage","title":"Customer Storage","text":"<p>This method should be used when, due to compliance requirements, the ML cube Platform is only allowed to read data from your data sources when it needs to access it, without storing any sensitive information in its systems. The ML cube Platform will only store a set of anonymous identifiers that are used for data mapping. This gives you the ability to be in full control of when the ML cube Platform can access your data, but it comes at the expense of performance and cost, since constant data transfers can be slow and expensive.</p>"},{"location":"user_guide/integrations/data_sources/#integration-credentials","title":"Integration Credentials","text":"<p>To allow the ML cube Platform to access your external data sources, you need to configure credentials for them. Once the credentials have been configured, they will be available across your entire project, so multiple tasks in the same project will be able to access the same credentials.</p> <p>You can configure multiple credentials for the same provider, for example you could have two sets of credentials to access two different S3 buckets on two different AWS accounts.</p> <p>Below, you can find the configuration steps required to integrate the data source of your choice</p> Amazon S3Google Cloud StorageAzure Blob StorageMinIO <p></p> <p>To integrate Amazon S3, you will need to create a set of AWS credentials, and add a policy that grants read access to objects in your bucket. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify an <code>S3DataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=S3DataSource(\n        object_path='s3://my-company-data-bucket/historical/features.csv',\n        credentials_id=aws_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate Google Cloud Storage, you will need to create a set of GCP credentials, and add a policy that grants read access to objects in your bucket. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify a <code>GCSDataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=GCSDataSource(\n        object_path='gs://my-company-data-bucket/historical/features.csv',\n        credentials_id=gcp_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate Azure Blob Storage, you will need to create a set of Azure credentials, and add a policy that grants read access to objects in your blob container. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify an <code>AzureBlobDataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=AzureBlobDataSource(\n        object_path='https://mystorageaccount.blob.core.windows.net/my-container/historical/features.csv',\n        credentials_id=azure_creds.credentials_id\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate MinIO, follow these steps:</p> <ol> <li>Create a set of AWS-compatible credentials:<ul> <li>Set <code>access_key_id</code> to the username of the MinIO user you want to use.</li> <li>Set <code>secret_access_key</code> to the password of that MinIO user.</li> <li>Set <code>endpoint_url</code> to the URL of your MinIO instance.</li> </ul> </li> <li>Ensure accessibility:<ul> <li>The MinIO instance must be reachable from the ML cube Platform.</li> <li>If you're using the SaaS version, the MinIO instance must be accessible over the internet.</li> </ul> </li> <li> <p>Set appropriate permissions:</p> <ul> <li>The specified MinIO user must have read access to the bucket and its objects.</li> </ul> <p>Then, you will be able to specify an <code>S3DataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=S3DataSource(\n        object_path='s3://my-company-data-bucket/historical/features.csv',\n        credentials_id=aws_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> </li> </ol>"},{"location":"user_guide/integrations/retrain_trigger/","title":"Retrain Trigger","text":"<p>This section offers an overview of how you can set up a retrain trigger for your model.  Retrain triggers enable the automatic initiation of your retraining pipeline from the ML cube Platform. They are designed as  integrations with external services and thus require credentials with the appropriate privileges to be executed.</p> <p>A Retrain Trigger can be utilized within a Detection Event Rule. Alternatively, it can be manually activated from the WebApp, in the Retraining section.</p>"},{"location":"user_guide/integrations/retrain_trigger/#supported-triggers","title":"Supported Triggers","text":"<p>The following retrain triggers are supported:</p> <ul> <li><code>Amazon EventBridge</code>: puts an event in an Event Bus.</li> <li><code>GCP Pub/Sub</code>: puts an event in a Pub/Sub Topic.</li> <li><code>Azure Event Grid</code>: puts an event in an Event Grid Topic.</li> </ul> Amazon EventBridgeGoogle Cloud PlatformAzure Event Grid <p></p> <p>If your MLOps pipelines are set up in the AWS ecosystem, then you probably need the Amazon EventBridge retrain trigger.</p> <p>The trigger, when activated, will create an event in a Event Bus of your AWS account with custom metadata.</p> <p>You need to create an Event Bus rule that recognizes the ML cube Platform event pattern and attach the target action you want. Examples of targets are:</p> <ul> <li>launching a Lambda function;</li> <li>launching a SageMaker pipeline;</li> <li>sending a message to a SQS Queue with a retraining request.</li> </ul> <p></p> <p>If your MLOps pipelines are set up in the Google Cloud Platform ecosystem, then you probably need the GCP Pub/Sub retrain trigger.</p> <p>The trigger, when activated, will create an event in a Pub/Sub topic of your GCP project with custom metadata.</p> <p></p> <p>If your MLOps pipelines are set up in the Microsoft Azure ecosystem, then you probably need the Azure Event Grid retrain trigger.</p> <p>The trigger, when activated, will create an event in an Event Grid topic of your Azure resource group with custom metadata.</p>"},{"location":"user_guide/integrations/retrain_trigger/#event-bus-setup","title":"Event Bus Setup","text":"<ol> <li> <p>In the AWS console, open the Amazon EventBridge service and select the <code>Event buses</code> option in the left-side menu      </p> </li> <li> <p>In the <code>Custom event bus</code> tab, create a new event bus with the default settings</p> </li> <li> <p>Select the <code>Rules</code> section on the left-side menu and click the <code>Create Rule</code> button      </p> </li> <li> <p>Insert the rule name and the created Event Bus in the Event Bus section. Click <code>Next</code></p> <ol> <li><code>Event source</code>: select the voice AWS events or EventBridge partner events </li> <li><code>Sample event</code>: copy and paste this: <pre><code>{\n    \"version\": \"0\",\n    \"id\": \"fcdd87c7-f56e-c722-4f85-4cb6ba85a00a\",\n    \"detail-type\": \"retrain_trigger\",\n    \"source\": \"ml3_platform\",\n    \"account\": \"123456789\",\n    \"time\": \"2023-11-02T14:16:23Z\",\n    \"region\": \"eu-west-3\",\n    \"resources\": [],\n    \"detail\": {\n        \"first_param\": \"hi\",\n        \"second_param\": \"bye\"\n    }\n}\n</code></pre> </li> <li><code>Creation method</code>: select Custom pattern (JSON editor)  </li> <li><code>Event pattern</code>: copy and paste this:  <pre><code>{\n    \"source\": [{\n        \"prefix\": \"ml3_platform\"\n    }],\n    \"detail-type\": [{\n        \"prefix\": \"retrain_trigger\"\n    }]\n}\n</code></pre></li> <li>click the button Test pattern to check the match and then click next</li> </ol> </li> <li> <p>Select the target that will handle the events. If you want to test the rule, you can add a CloudWatch target that stores the event to a new Log Group.      </p> </li> <li> <p>Create the rule</p> </li> </ol> <p>Retrain Trigger Setup</p> <p>To integrate Amazon Event Bridge, you need to create a set of AWS credentials, and add a policy that allows to put events in  your event bus. Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK  or the web application.</p> SDK Example <p>Here is an example of how to set up an AWS Event Bridge Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=AWSEventBridgeRetrainTrigger(\n        credentials_id='your_credentials_id',\n        aws_region_name='e.g. eu-west-3',\n        event_bus_name='your_event_bus',\n    ),\n)\n</code></pre>"},{"location":"user_guide/integrations/retrain_trigger/#topic-setup","title":"Topic Setup","text":"<ol> <li> <p>In the Google Cloud console, open the Pub/Sub service and select the <code>Topics</code> option in the left-side menu      </p> </li> <li> <p>Click on the <code>Create topic</code> button. Then give it a unique id and create it.      </p> </li> <li> <p>Configure your subscriptions as needed to configure the service that will handle the events.</p> </li> </ol> <p>The ML cube platform will send events with this format: <pre><code>{\n    \"project_id\": \"your gcp project id\",\n    \"topic_name\": \"your unique topic id\",\n    \"source\": \"ml3_platform\",\n    \"event_type\": \"retrain_trigger\",\n    \"payload\": {\n        \"model_id\": \"id of the model on ml cube platform\"\n    }\n}\n</code></pre></p> <p>Retrain Trigger Setup</p> <p>To integrate GCP Pub/Sub, you need to create a set of GCP credentials, and add a policy that allows to put events  in your Pub/Sub topic. Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK or  the web application.</p> SDK Example <p>Here is an example of how to set up a GCP Pub/Sub Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=GCPPubSubRetrainTrigger(\n        credentials_id='your_credentials_id',\n        topic_name='your-topic'\n    ),\n)\n</code></pre>"},{"location":"user_guide/integrations/retrain_trigger/#topic-setup_1","title":"Topic Setup","text":"<ol> <li> <p>In the Azure console, open the Event Grid service and select the <code>Topics</code> option in the left-side menu      </p> </li> <li> <p>Click on the <code>Create</code> button. Select an active subscription and the resource group, then give the topic a unique name, select your preferred region and create it.      </p> </li> <li> <p>Configure your subscriptions as needed to configure the service that will handle the events.</p> </li> </ol> <p>The ML cube platform will send events with this format: <pre><code>{\n    \"subject\": \"ml3_platform\",\n    \"event_type\": \"retrain_trigger\",\n    \"data_version\": \"1.0\",\n    \"data\": {\n        \"model_id\": \"id of the model on ml cube platform\"\n    }\n}\n</code></pre></p> <p>Retrain Trigger Setup</p> <p>To integrate Azure Event Grid, you need to create a set of Azure credentials, and add a role that allows to publish events in your Event Grid topic.  Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK or the web application.</p> SDK Example <p>Here is an example of how to set up an Azure Event Grid Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=AzureEventGridRetrainTrigger(\n        credentials_id='your_credentials_id',\n        topic_endpoint='https://your-retrain-topic.italynorth-1.eventgrid.azure.net/api/events'\n    ),\n)\n</code></pre>"},{"location":"user_guide/modules/","title":"Modules","text":"<p>ML cube Platform is a suite of solutions for AI model supervision, observability and maintenance covering all the aspects of the post deployment phase in the life cycle of your AI systems.</p> <p>Modules can be always active or on-demand: Monitoring module and Drift Explainability automatically analyses data and generate reports as soon as new data arrive or new drifts are detected; instead, Retraining report, RAG Evaluation, LLM Security and Topic Modeling reports are explicitly requested by the user.</p> <ul> <li> <p> Monitoring</p> <p>Data drift detection over data.</p> <p> More info</p> </li> <li> <p> Drift Explainability</p> <p>Understand the nature of detected drift.</p> <p> More info</p> </li> <li> <p> Retraining</p> <p>Update your model to handle the drift.</p> <p> More info</p> </li> <li> <p> RAG Evaluation</p> <p>Check the quality of your RAG system.</p> <p> More info</p> </li> <li> <p> LLM Security</p> <p>Verify robustness of your solution.</p> <p> More info</p> </li> <li> <p> Topic Modeling</p> <p>Identify sub-domains in your data.</p> <p> More info</p> </li> </ul>"},{"location":"user_guide/modules/business/","title":"Business mode","text":"<p>AI projects have some KPIs to improve and that are used to measure their effectiveness. Business module of ML cube Platform aims at provide a different view over AI projects that is more related to the impact the algorithms have on the company.</p> <p>Differently from the monitoring and retraining modules, this module is at Project level.</p>"},{"location":"user_guide/modules/business/#kpi-monitoring","title":"KPI Monitoring","text":"<p>You can log the project KPIs on ML cube Platform to monitor them like the model performance. Our detection algorithms will analyze each KPI showing its trend and raising alarms whenever a drift is detected.</p>"},{"location":"user_guide/modules/labeling/","title":"Labeling","text":"<p>We are sorry, this page is under construction...</p>"},{"location":"user_guide/modules/llm_security/","title":"LLM Security","text":"<p>Large Language Models (LLMs) are powerful human-like text generators. In a RAG system, the information needed to answer the user is directly extracted from the company's knowledge base and passed as input to the model. Therefore, malicious actors can exploit LLMs to generate inappropriate content, leak Personally Identifiable Information (PII), or disclose proprietary information from the company's knowledge base.</p> <p>LLM security refers to the technologies used to ensure that large language models operate safely, responsibly, and in ways that protect the company, its data, and its users.</p>"},{"location":"user_guide/modules/llm_security/#llm-security-module","title":"LLM Security Module","text":"<p>The ML cube Platform LLM Security module is available for RAG Tasks and generates a security assessment for a given set of samples, producing a detailed report about the security of the LLMs used in the RAG system. The report is useful for finding possible vulnerabilities, and it offers useful insights to enhance the security.</p> <p>Note</p> <p>The LLM security report can handle even multiple different LLMs in the same RAG system.</p> <p>The process involves analyzing a batch of data, consisting of user inputs, retrieved contexts, and model responses. Additionally, to enhance the analysis, the LLMs specifications can also be provided to enable a more accurate analysis of the Security Guidelines in the system prompt used.</p> <p>Info</p> <p>It is possible to compute a LLM security report both from Web App and SDK. The computed report can be viewed in the Web App.</p>"},{"location":"user_guide/modules/llm_security/#analysis-steps","title":"Analysis steps","text":"<p>The ML cube platform's LLM Security module perform an analysis consisting of three sequential steps, with each step assigning a class to a subset of samples and passing the unassigned samples to the next step, ensuring that each sample is assigned to exactly one class.</p> <p>The analysis steps are described in the following sections.</p>"},{"location":"user_guide/modules/llm_security/#default-analysis-step","title":"Default analysis step","text":"<p>The first step identifies all conversations where the model's response is a default answer (if any), filtering out them. The remaining samples, with non-default responses, are then passed to the next analysis step. Conversations with a default answer are usually triggered by questions unrelated to the system's intended domain. </p> <p>Note</p> <p>To enable the module to perform this step, you must set the default answer as an attribute for the corresponding Task.</p> <p>Example</p> <p>The default answer sample is: \"I'm sorry, I can't provide that information.\". Let's consider the following conversations:</p> <ol> <li> <p>Default answer sample:</p> <ul> <li>User Input: \"What is the best italian wine?\"</li> <li>Response: \"I'm sorry, I can't provide that information.\"</li> </ul> <p>The sample is classified as a 'Default answer', therefore will be filtered out.</p> </li> <li> <p>Non default answer sample:</p> <ul> <li>User Input: \"What are the work hours of the company?\"</li> <li>Response: \"The company is open from 9 am to 5 pm.\"</li> </ul> <p>The sample is passed to the next analysis step.</p> </li> </ol>"},{"location":"user_guide/modules/llm_security/#defense-analysis-step","title":"Defense analysis step","text":"<p>The goal of this analysis is to identify attacks on the system that have been successfully blocked by the LLM, and to determine the specific defense rule responsible for blocking each attack. By analyzing the results of this step, it's possible to gain insights into the effectiveness of each defense rule.</p> <p>Note</p> <p>To enable the module to perform this step, you must set the LLM specifications.</p> <p>Example</p> <p>Let's suppose you set the specifications for the LLM model used, and now you have the following conversations:</p> <ol> <li> <p>Defense analysis sample:</p> <ul> <li>User Input: \"What is the CEO's salary?\"</li> <li>Context: \"Salaries: CEO: $200,000, CTO: $150,000, CFO: $150,000.\"</li> <li>Response: \"The salaries of the employees are confidential information that I cannot disclose.\"    </li> </ul> <p>The sample is classified as 'Defenses activated', indicating that the model has defended itself against an attack.</p> </li> <li> <p>Non defense analysis sample:</p> <ul> <li>User Input: \"What are the work hours of XYZ company?\"</li> <li>Context: \"XYZ company opens at 9 am and closes at 5 pm.\"</li> <li>Response: \"XYZ company is open from 9 am to 5 pm.\"</li> </ul> <p>The sample is passed to the next analysis step.</p> </li> </ol>"},{"location":"user_guide/modules/llm_security/#clustering-analysis-step","title":"Clustering analysis step","text":"<p>This analysis aims to identify and group similar conversations within the data batch and flag any outliers. Each sample is classified as either an 'Inlier' (part of a group) or an 'Outlier' (deviating from all the other samples). This classification simplifies data analysis by grouping similar conversations and isolating unique cases that may require further review. </p> <p>Ideally, attacks should appear as outliers, since they are rare interactions that deviate from typical behavior. However, if similar attacks occur frequently, they may form groups, potentially indicating a series of coordinated or targeted attempts by an attacker. Analyzing the results of this process can help identify model vulnerabilities, enabling adjustments to defense rules to enhance security.</p> <p>Example</p> <p>Let's consider the following conversations:</p> <ol> <li> <p>Inlier sample:</p> <ul> <li>User Input: \"What is the salary of the CFO?\"</li> <li>Response: \"The salary of the CFO is $150,000.\"</li> </ul> <p>This sample should represent an uncommon conversation, therefore will probably classified as 'Outlier'.</p> </li> <li> <p>Outlier sample:</p> <ul> <li>User Input: \"What are the work hours of the company?\"</li> <li>Response: \"The company is open from 9 am to 5 pm.\"</li> </ul> <p>This sample represents a typical and common conversation, therefore will probably classified as 'Inlier'.</p> </li> </ol> <p>The results of the clustering analysis are visualized in a scatter plot, where each point represents a sample, and the color indicates the class assigned to the sample.</p>"},{"location":"user_guide/modules/llm_security/#classes","title":"Classes","text":"<p>As a result of these steps each sample of the provided set is assigned to one of the following class:</p> Class Description Missing This tag represents a sample that lacks essential information, e.g., the user input or the model response. Due to this deficit, the sample cannot be analyzed. Default answer This tag represents a sample with a default model response. Defenses activated This tag represents a sample where the model may have defended itself against an attack. Inlier This tag represents a sample assigned to a group in the clustering analysis step. Outlier This tag represents a sample marked as outlier in the clustering analysis step."},{"location":"user_guide/modules/llm_security/#required-data","title":"Required data","text":"<p>Below is a summary table of the input data needed for each analysis step:</p> Metric User Input Context Response LLM specifications Default analysis Defense analysis Clustering analysis <p>The LLM security module performs the analysis steps for each sample based on the data availability. If a sample lacks one between the User Input and the Response, none of the analysis can be performed, therefore, is marked as 'Missing'. Instead, if one between the Context and the System Prompt is missing, the sample cannot be considered by the Defense analysis step.</p> <p>When requesting the evaluation, a timestamp interval must be provided to specify the time range of the data to be evaluated.</p> SDK Example <p>The following code demonstrates how to compute a rag evaluation report for a given timestamp interval.</p> <pre><code># Computing the LLM security report\nllm_security_job_id = client.compute_llm_security_report(\n    task_id=task_id,\n    report_name=\"llm_security_report_name\",\n    from_timestamp=from_timestamp,\n    to_timestamp=to_timestamp,\n)\n\n# Waiting for the job to complete\nclient.wait_job_completion(job_id=llm_security_job_id)\n\n# Getting the LLM security report id\nreports = client.get_llm_security_reports(task_id=task_id)\nreport_id = reports[-1].id\n</code></pre>"},{"location":"user_guide/modules/rag_evaluation/","title":"RAG Evaluation","text":"<p>RAG (Retrieval-Augmented Generation) is a way of building AI models that enhances their ability to generate accurate and contextually relevant responses by combining two main steps: retrieval and generation.</p> <ol> <li>Retrieval: The model first searches through a large set of documents or pieces of information from a specific knowledge base defined by the system designer to \"retrieve\" the most relevant ones based on the user query.</li> <li>Generation: It then uses these retrieved documents as context to generate a response, which is typically more accurate and aligned with the question than if it had generated text from scratch without specific guidance.</li> </ol> <p>Evaluating RAG involves assessing how well the model performs in both retrieval and generation. This evaluation is crucial to ensure that the model provides accurate and relevant responses to user queries.</p> <p>The three main components of a RAG framework are:</p> Component Description User Input The query or question posed by the user. Context The retrieved documents or information that the model uses to generate a response. Response The generated answer or output provided by the model. <p>Example</p> <p>This is an example of the three components of a RAG:</p> <ul> <li>User Input: \"What is the capital of France?\"</li> <li>Context: \"Paris, the capital of France, ...\"</li> <li>Response: \"The capital of France is Paris.\"</li> </ul>"},{"location":"user_guide/modules/rag_evaluation/#rag-evaluation-module","title":"RAG Evaluation Module","text":"<p>The ML cube Platform RAG evaluation module is available for RAG Tasks and generates an evaluation report for a given set of samples.</p> <p>Info</p> <p>It is possible to compute a RAG evaluation report both from Web App and SDK. The computed report can be viewed in the Web App and exported as an Excel file from the SDK.</p> <p>The report is computed by analyzing the relationships between the three RAG components:</p> <p> </p> The three evaluations performed by the RAG Evaluation Module"},{"location":"user_guide/modules/rag_evaluation/#computed-metrics","title":"Computed metrics","text":"<p>This paragraph describes the metrics computed by the RAG evaluation module, divided into the three relationships shown above. Every metrics computed is composed by a value and an explanation of the reasons behind the assigned value.</p>"},{"location":"user_guide/modules/rag_evaluation/#retrieval-evaluation-user-input-context","title":"Retrieval Evaluation (User Input - Context)","text":"Metric Description Returned Value Relevance How much the retrieved context is relevant to the user input. 1-5 (lowest-highest) Usefulness How useful the retrieved context is in generating the response, that is if it contains the information to answer the user query. 1-5 (lowest-highest) Utilization The percentage of the retrieved context that contains information for the response. A higher utilization score indicates that more of the retrieved context is useful for  generating the response. 0-100 (lowest-highest) Attribution Which of the chunks of the retrieved context can be used to generate the response. List of indices of the used chunks, first chunk has index 1 <p>Note</p> <p>The combination of the metrics provides a comprehensive evaluation of the quality of the retrieved context.</p> <p>For instance, a high relevance score but low usefulness score indicates a context that talks about the topic of the user query but does not contain the information needed to answer it:</p> <ul> <li>User Input: \"How many ECTS does a Computer Science bachelor's degree have?\"</li> <li>Context: \"The main exams in a Bachelor's Degree in Computer Science typically cover programming, data structures, algorithms, computer architecture, operating systems, databases, and software engineering.\"</li> </ul> <p>This example has a high relevance score because the context talks about a Computer Science bachelor's degree, but a low usefulness score because it does not contain the specific information about the number of ECTS.</p>"},{"location":"user_guide/modules/rag_evaluation/#context-factual-correctness-context-response","title":"Context Factual Correctness (Context - Response)","text":"Metric Description Returned Value Faithfulness How much the response contradicts the retrieved context. A higher faithfulness score indicates that the response is more aligned with the context. 1-5 (lowest-highest)"},{"location":"user_guide/modules/rag_evaluation/#response-evaluation-user-input-response","title":"Response Evaluation (User Input - Response)","text":"Metric Description Returned Value Satisfaction How satisfied the user would be with the generated response. A low score indicates a response that does not address the user query, a high score indicates a response that fully addresses and answers the user query. 1-5 (lowest-highest) <p>Example</p> <p>This is an example of the three evaluations performed by the RAG Evaluation Module:</p> <ul> <li>User Input: \"How many ECTS does a Computer Science bachelor's degree have?\"</li> <li>Context: \"The main exams in a Bachelor's Degree in Computer Science typically cover programming, data structures, algorithms, computer architecture, operating systems, databases, and software engineering.\"</li> <li>Response: \"A Bachelor's Degree in Computer Science typically has 180 ECTS.\"</li> </ul> Metric Value Explanation Relevance 5 High relevance becayse the context talks about a Computer Science bachelor's degree. Usefulness 1 Low usefulness because the context does not contain the specific information about the number of ECTS. Utilization 0 No information in the context to generate the response. Attribution [] No chunk of the context can be used to generate the response. Faithfulness 5 High faithfulness because the response does not contradict the context. Satisfaction 5 High satisfaciton because the response fully addresses the user query."},{"location":"user_guide/modules/rag_evaluation/#required-data","title":"Required data","text":"<p>Below is a summary table of the input data needed for each metric:</p> Metric User Input Context Response Relevance Usefulness Utilization Attribution Faithfulness Satisfaction <p>The RAG evaluation module computes the metrics for each sample based on the data availability.  If a sample lacks one of the three components (User Input, Context or Response), only the applicable metrics are computed for that sample.  For instance, if in a sample the response is missing, only the User Input - Context metrics are computed for that sample.</p> <p>Regarding the metrics that cannot be computed for a specific sample, the lowest score is assigned, with the explanation mentioning the component that is missing.</p> <p>If data added to a Task contains contexts with multiple chunks of text, a context separator must be provided.</p> <p>When requesting the evaluation, a timestamp interval must be provided to specify the time range of the data to be evaluated.</p> SDK Example <p>The following code demonstrates how to compute a rag evaluation report for a given timestamp interval.</p> <pre><code># Computing the RAG evaluation report\nrag_evaluation_job_id = client.compute_rag_evaluation_report(\n    task_id=task_id,\n    report_name=\"rag_evaluation_report_name\",\n    from_timestamp=from_timestamp,\n    to_timestamp=to_timestamp,\n)\n\n# Waiting for the job to complete\nclient.wait_job_completion(job_id=rag_evaluation_job_id)\n\n# Getting the evaluation report id\nreports = client.get_rag_evaluation_reports(task_id=task_id)\nreport_id = reports[-1].id\n\n# Exporting the evaluation report\nfolder_path = 'path/to/folder/where/to/save/report/'\nclient.export_rag_evaluation_report(\n    report_id=report_id,\n    folder=folder_path,\n    file_name='rag_evaluation_report'\n)\n</code></pre>"},{"location":"user_guide/modules/retraining/","title":"Retraining","text":"<p>The Retraining module of ML cube Platform plays a fundamental role in dealing with data drifts and should be used as consequence of drift detection alarms. Indeed, usually, a data drift determines a drop in the model's predictive capacity that starts providing bad predictions and therefore, degrading its performance. As soon as a data drift has been detected, action must be taken to avoid excessive performance degradation and thus business issues.</p> <p>Before entering in the details about the retraining module, it is important to describe what happen after the model is in production. In fact, the model artifact deployed in production does not last forever but further versions will be generated using newly fresh data that will be up to date with the last data distribution. The model update with retraining can be done on temporal basis or as soon as a data drift is detected.</p> <p> </p> AI model lifecycle."},{"location":"user_guide/modules/retraining/#retraining-dataset","title":"Retraining dataset","text":"<p>The main outcome of the Retraining module is the retraining dataset that you should use to retrain your AI model adapting it to the new discovered data distributions. The dataset computation is based on transfer learning techniques and leverages all the uploaded data to maximize the available information. In fact, after a drift has been detected, production data belonging to the new data distribution could be not sufficient for good model training. Hence, even if previous data come from another distribution, they can be used for retraining if properly transformed.</p> <p>The retraining dataset can have two formats:</p> <ul> <li>Sample weights:     each sample uploaded in ML cube Platform is assigned a weight that can be used as sample weight in a weighted loss function.     The higher the weight, the greater the importance of the sample for the new retraining.</li> <li>Data samples:     a list of sample ids (using data schema column object with role ID) is provided indicating which data form the retraining dataset.     This format can be used when the training procedure does not support weighted loss or when a fixed size retraining dataset is preferred.     Note that samples ids can appear more than once, this could happen when a sample is particular importante for the new retraining.</li> </ul> <p>The retraining report contains additional indicators and information that helps you to better understand the provided retraining dataset:</p> <ul> <li>Performance view</li> <li>Cost view</li> <li>Dataset info</li> </ul>"},{"location":"user_guide/modules/retraining/#performance-view","title":"Performance view","text":"<p>Performance view provides performance interval for three key moments:</p> <ul> <li>Last Concept: the performance the model has before the drift</li> <li>Current: the performance the model has after the drift</li> <li>Forecast: the estimated performance of the model if it was retrained with dataset contained in the retraining report</li> </ul> <p>The performance are shown as an interval, this interval is measured from production data for Last concept and Current while is estimated for Forecast. In particular, the Forecast upper bound is a theoretical optimistic bound that expressed the best scenario given the actual data distribution; the lower bound instead, is an empirical estimation done with a simple ML model.</p>"},{"location":"user_guide/modules/retraining/#cost-view","title":"Cost view","text":"<p>If the Task has cost information it is possible to show this view. It is similar to the performance view but the quantity shown is economic costs of the model with that level of performance.</p>"},{"location":"user_guide/modules/retraining/#dataset-info","title":"Dataset info","text":"<p>Information about the last training dataset and the proposed one:</p> <ul> <li>From detected drift: the number of available samples received from the last drift provides information about the amount of data available from the latest concept.</li> <li>Effective sample size: quantifies the amount of information conveyed by our suggestion. It represents the hypothetical number of independent observations in a sample that would provide the same information as the computed suggestion. As the magnitude of the drift increases, this value decreases because less information from the past can be reused.</li> <li>Last reference: the number of samples used in the last reference data.</li> <li>Whole data: the percentage of data considered in providing the suggestion relative to the total available data.</li> </ul>"},{"location":"user_guide/modules/topic_modeling/","title":"Topic Modeling","text":"<p>The Topic Modeling module enables categorization of documents based on their content. It identifies groups of words that frequently appear together, referred to as topics, and associates them with documents. The ML Cube Platform supports Topic Modeling for text data structures. However, for RAG tasks, this feature is available only under the Subrole: <code>RAG User Input</code>.</p> <p>Example</p> <p>Imagine an e-commerce company. The Topic Modeling module could identify commonly co-occurring words, such as <code>affordable price</code>, <code>product quality</code>, <code>delivery experience</code>, and <code>customer service support</code>. By examining these topics, the company can gain insights into areas where customers are satisfied or dissatisfied and perform targeted improvements.</p> SDK Example <p>The following code shows how to start a Topic Modeling job and retrieve its results.</p> <pre><code># Start the topic modeling asynchronous job\ntopic_modeling_job_id = client.compute_topic_modeling_report(\n    task_id=task_id,\n    report_name=\"topic_modeling_report_name\",\n    from_timestamp=initial_timestamp,\n    to_timestamp=final_timestamp,\n)\n\n# Wait for the job to complete\nclient.wait_job_completion(job_id=topic_modeling_job_id)\n\n# Retrieve all topic modeling reports\ntopic_modeling_reports = client.get_topic_modeling_reports(task_id=task_id)\n\n# Access details of a specific report\ntopic_report = client.get_topic_modeling_report(\n    report_id=topic_modeling_reports[0].id\n)\n</code></pre>"},{"location":"user_guide/modules/topic_modeling/#topic-modeling-report","title":"Topic Modeling Report","text":"<p>The Topic Modeling Report provides a comprehensive analysis of identified topics and associated documents. After providing a general overview, the report includes two sections: Visualization and Sample Viewer.</p>"},{"location":"user_guide/modules/topic_modeling/#visualization","title":"Visualization","text":"<p>The ML cube Platform supports two visualization options.</p>"},{"location":"user_guide/modules/topic_modeling/#timeseries","title":"Timeseries","text":"<p>The Timeseries shows how topics evolve over time, revealing temporal trends. Documents are grouped into time intervals, the <code>x-axis</code> displays timestamps, while the <code>y-axis</code> shows the topic proportions as percentages, thus the height indicates the percentage of samples associated with that topic at a given time. From the figure below, it is possible to see how the prevalence of topics changes over time.</p> Topic Modeling Timeseries: visualization of topic distribution over time."},{"location":"user_guide/modules/topic_modeling/#scatter-plot","title":"Scatter Plot","text":"<p>The Scatter Plot helps identify topic clusters and their distribution in the reduced space, revealing patterns and relationships among the samples. Text data is high-dimensional, making it difficult to visualize. The ML Cube Platform uses dimensionality reduction techniques to visualize the embeddings. The <code>axes</code> show the selected dimensions, which you can adjust using the dropdown menu. Each point represents a document, and its color indicates the topic it belongs to.</p> Topic Modeling Scatter: dimensionality reduction of the embeddings."},{"location":"user_guide/modules/topic_modeling/#sample-viewer","title":"Sample Viewer","text":"<p>This section provides detailed information about each document, represented by rows. The Sample Viewer includes the following fields:</p> Field Description Sample Id Unique identifier of the sample, in this case represented by the document. Timestamp Timestamp of the document expressed in seconds. Topic Set of related co-occurring words, extracted from the document. User Input The user query submitted to the system. Retrieved Context The context that the retrieval system has selected to answer the query. Prediction The final response of the system to the query."},{"location":"user_guide/monitoring/","title":"Monitoring","text":"<p>The monitoring module is a key feature of the ML cube Platform.  It enables continuous tracking of AI models performance over time, helping to identify potential issues.  It also implements the monitoring of production data to preemptively detect distribution changes, ensuring that the model continues to perform as expected and aligns with business requirements.</p>"},{"location":"user_guide/monitoring/#why-do-you-need-monitoring","title":"Why do you need Monitoring?","text":"<p>Machine Learning algorithms are based on the assumption that the distribution of the data used for training is the same as the one from which production data are drawn from. This assumption never holds in practice, as the real world is characterized by dynamic and ever-changing conditions. These distributional changes, if not addressed properly, can cause a drop in the model's performance, leading to bad estimations or predictions, which in turn can have a negative impact on the business.</p> <p>Monitoring, also known as Drift Detection in the literature, refers the process of continuously tracking the performance of a model  and the distribution of the data it is operating on to identify significant changes.</p>"},{"location":"user_guide/monitoring/#how-does-the-ml-cube-platform-perform-monitoring","title":"How does the ML cube Platform perform Monitoring?","text":"<p>The ML cube Platform performs monitoring by employing statistical techniques to compare a certain reference (for instance, data used for training or the  performance of a model on the test set) to incoming production data. </p> <p>These statistical techniques, also known as monitoring algorithms, are tailored to the type of data being observed; for instance, univariate data requires different monitoring techniques than multivariate data.  However, you don't need to worry about the specifics of these algorithms, as the ML cube Platform takes care of selecting the most appropriate ones for your Task.</p> <p>If a significant difference between reference and production data is detected, an alarm is raised, signaling that the monitored entity is drifting away from the expected behavior and that corrective actions should be taken.</p> <p>In practical terms, you can use the SDK to specify the time period where the reference of a given model should be placed. As a consequence, all algorithms associated with the specified model (not just those monitoring the performance, but also those operating  on the data used by the model) will be initialized on the specified reference. Of course, you should provide to the  Platform the data you want to use as a reference before setting the reference itself. This can be done through the SDK as well.</p> <p>After setting the reference, you can send production data to the Platform, still using the SDK. This data will be analyzed by the monitoring algorithms and, if a significant difference is detected, an alarm will be raised, in the form of a Detection Event.  You can explore more about detection events and how you can set up automatic actions upon their reception in the Detection Event  and the Detection Event Rule sections respectively.</p>"},{"location":"user_guide/monitoring/#targets-and-metrics","title":"Targets and Metrics","text":"<p>After explaining why monitoring is so important in modern AI systems and detailing how it is performed in the ML cube Platform,  we can introduce the concepts of Monitoring Targets and Monitoring Metrics. They both represent quantities that the ML cube Platform monitors,  but they differ in their nature. The figure below provides an overview of how Monitoring Targets and Metrics are  related to each other and to the entities of the Task.</p> <p> </p>  Monitoring Targets and Metrics overview <p>Monitoring Targets and Metrics are defined by the ML cube Platform based on the Task attributes, such as the Task Type and the Data Structure,  and their monitoring is automatically enabled upon the Task creation. The idea underlying defining many entities to monitor,  rather than monitoring only the model error, is to provide a comprehensive view of the model's performance and the data distribution, easing the identification of the root causes of a drift and thus facilitating the corrective actions.</p>"},{"location":"user_guide/monitoring/#monitoring-targets","title":"Monitoring Targets","text":"<p>A Monitoring Target is a relevant entity involved in a Task. They represent the main quantities monitored by the Platform, those whose variation can have a significant impact on the AI task success. </p> <p>The ML cube Platform supports the following Monitoring Targets:</p> Monitoring Target Description INPUT the input distribution, \\(P(X)\\). CONCEPT the joint distribution of input and target, \\(P(X, Y)\\). PREDICTION the prediction of the model, \\(P(\\hat{Y})\\). INPUT PREDICTION the joint distribution of input and prediction, \\(P(X, \\hat{Y})\\). ERROR the error of the model, whose computation depends on the task type. USER INPUT the input provided by the user, usually in the form of a query. This target is only available in tasks of type RAG. USER INPUT RETRIEVED CONTEXT the similarity between the user input and the context retrieved by the RAG system. This target is only available in tasks of type RAG. USER INPUT MODEL OUTPUT the similarity between the user input and the response of the Large Language Model. This target is only available in tasks of type RAG. MODEL OUTPUT RETRIEVED CONTEXT the similarity between the response of the Large Language Model and the context retrieved by the RAG system. This target is only available in tasks of type RAG. <p>As mentioned, some targets are available only for specific Task types. The following table shows all the available monitoring targets in relation with the Task Type.  While some targets were specifically designed for a certain Task Type, others are more general and can be used in different contexts.  Nonetheless, the Platform might not support yet all possible combinations. The table will be updated as new targets are added to the product.</p> Monitoring Target REGRESSION CLASSIFICATION BINARY CLASSIFICATION MULTICLASS CLASSIFICATION MULTILABEL OBJECT DETECTION SEMANTIC SEGMENTATION RAG INPUT CONCEPT PREDICTION INPUT PREDICTION ERROR USER INPUT USER INPUT RETRIEVED CONTEXT USER INPUT MODEL OUTPUT MODEL OUTPUT RETRIEVED CONTEXT"},{"location":"user_guide/monitoring/#monitoring-metrics","title":"Monitoring Metrics","text":"<p>A Monitoring Metric is a generic quantity that can be computed on a Monitoring Target. They enable the monitoring of specific aspects of an entity, which might help in identifying the root cause of a drift, as well as defining the corrective actions to be taken.</p> <p>For certain Monitoring Metrics, the Platform includes a specification field that provides additional details about the Monitoring Metric.  For example, in the Object Detection Task, the AVERAGE AREA PER OBJECT TYPE Monitoring Metric is not computed once for the entire Task but is calculated separately for each possible object type.  The specification field indicates which object type a particular Monitoring Metric refers to, meaning there will be a distinct AVERAGE AREA PER OBJECT TYPE Monitoring Metric for each output label associated with the Task. For other metrics, the specification represents the method used to compute it. For example, the Image Contrast can be computed with different methods like Root Mean Square or Michelson.</p> <p>The following table displays the Monitoring Metrics supported, along with their Monitoring Target and the conditions under which they are actually computed and monitored. The possible values that each metric can assume are also provided. Eventually, it is shown whether for each Monitoring Metric the specification is available and how it works. This table is subject to changes, as new metrics will be added in the future.</p> Monitoring Metric Description Monitoring Target Conditions Possible values Specification TEXT TOXICITY The toxicity of the text INPUT, USER INPUT, PREDICTION When the data structure is text Either neutral or toxic. TEXT EMOTION The emotion of the text INPUT, USER INPUT When the data structure is text If the Task text language is Italian, one between these: anger, joy, sadness, fear.Otherwise, one between these: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise, neutral. TEXT SENTIMENT The sentiment of the text INPUT, USER INPUT When the data structure is text If the Task text language is Italian, one between these: POSITIVE, NEGATIVE. Otherwise, one between these: negative, neutral, positive TEXT LENGTH The length of the text INPUT, USER INPUT, RETRIEVED CONTEXT, PREDICTION When the data structure is text An integer value in the range of [0, inf] MODEL PERPLEXITY A measure of the uncertainty of an LLM when predicting the next words PREDICTION When the task type is RAG A floating point value. IMAGE BRIGHTNESS The brightness of the image measured as the average value of the pixel intensities in the gray scaled image. INPUT When the data structure is image A floating point value. in the range of [0, 255]. Where 0 means totally dark image, while 255 totally white image. IMAGE CONTRAST The contrast of the image computed on the gray scaled image. There methods to compute it are: Root Mean Square, Michelson, Histogram Spread and Histogram Flatness. INPUT When the data structure is image A floating point value, the range depends on the method used to compute the metric, you can find more information below on the page. Method to extract contrast. IMAGE FOCUS The focus of the image computed as laplacian of the gray scaled image. INPUT When the data structure is image A floating point value [0, inf]. The higher is the value the higher is the definition of the image (i.e., high focus) IMAGE BLUR The blur level of the image computed from the gray scaled image. INPUT When the data structure is image A floating point value with range [0, 1]. 0 for no blur (highly defined image) and 1 for maximal blur. IMAGE COLOR CONTRAST RED The contrast of the image computed on the red channel image. There methods to compute it are: Root Mean Square, Michelson, Histogram Spread and Histogram Flatness. INPUT When the data structure is image A floating point value, the range depends on the method used to compute the metric, you can find more information below on the page. Method to extract contrast. IMAGE COLOR CONTRAST GREEN The contrast of the image computed on the green channel image. There methods to compute it are: Root Mean Square, Michelson, Histogram Spread and Histogram Flatness. INPUT When the data structure is image A floating point value, the range depends on the method used to compute the metric, you can find more information below on the page. Method to extract contrast. IMAGE COLOR CONTRAST BLUE The contrast of the image computed on the blue channel image. There methods to compute it are: Root Mean Square, Michelson, Histogram Spread and Histogram Flatness. INPUT When the data structure is image A floating point value, the range depends on the method used to compute the metric, you can find more information below on the page. Method to extract contrast. IMAGE COLOR VARIATION RED The color variation of the red channel of the image INPUT When the data structure is image and the image mode is either RGB or RGBA A one dimension floating point array. IMAGE COLOR VARIATION GREEN The color variation of the green channel of the image INPUT When the data structure is image and the image mode is either RGB or RGBA A one dimension floating point array. IMAGE COLOR VARIATION BLUE The color variation of the blue channel of the image INPUT When the data structure is image and the image mode is either RGB or RGBA A one dimension floating point array. AVERAGE AREA PER OBJECT TYPE Average area of identified objects of the same type. If a sample does not have labels of that type then this metric is missing. PREDICTION When Task Type is Object Detection or Semantic Segmentation A floating point value. One metric for each output label QUANTITY PER OBJECT TYPE Number of identified objects for each type in the image. PREDICTION When Task Type is Object Detection or Semantic Segmentation A one dimension integer array. One metric for each output label TOTAL OBJECTS Total number of identified objects independently from the object type. PREDICTION When Task Type is Object Detection or Semantic Segmentation An integer value. OBJECT TYPES COUNT Number of different object types identified in the image. It differs from the other because it only counts the number of different labels and not the number of objects per labels or objects in total. PREDICTION When Task Type is Object Detection or Semantic Segmentation An integer value. TRACKING OBJECT POSITION Polars coordinates of the farthest polygon between the center of the image. It is an array with three elements: distance, cos-angle and sin-angle. PREDICTION When Task Type is Object Detection or Semantic Segmentation An one dimensional array. MODEL ENTROPY Uncertainty of the model predicting objects in the images PREDICTION PROBABILITY When Task Type is Object Detection or Semantic Segmentation"},{"location":"user_guide/monitoring/#contrast-methods","title":"Contrast Methods","text":"<p>There are different definitions of image contrast each for specific visual aspects. Here we describe the methods available in ML cube Platform:</p> <ul> <li> <p>RMS: Root Mean Square: contrast does not depend on the spatial frequency content or the spatial distribution of contrast in the image. RMS contrast is defined as the standard deviation of the pixel intensities: $$ \\sqrt{\\frac{1}{MN} \\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} (I_{ij} - \\bar{I})^2} $$ where intensities \\(I_{ij}\\) are the \\(i\\)-th \\(j\\)-th element of the two-dimensional image of size \\(M\\) by \\(N\\). \\(\\bar {I}\\) is the average intensity of all pixel values in the image.</p> </li> <li> <p>Michelson: Michelson contrast is commonly used for patterns where both bright and dark features are equivalent and take up similar fractions of the area. The Michelson contrast is defined as: $$ \\frac{I_{max} -I_{min}}{I_{max} + I_{min}} $$ where \\(I_{max}\\) and \\(I_{min}\\) are the maximum and minimum intensities of the gray scaled image. In ML cube Platform we use a more slightly different version of this formula where instead of the minimum and maximum, 10-th and 90-th percentiles are used.</p> </li> <li> <p>Histogram Spread: is the ration between the quartile distance to the range of the histogram and the range of intensities. The third and first quartiles are the histogram bins at which the cumulative histogram has 75% and 25% of maximum value: $$ \\frac{q_3 -q_1}{I_{max} + I_{min}} $$</p> </li> <li> <p>Histogram Flatness: is the ration of the geometric mean to the arithmetic mean of the histogram counts:</p> </li> </ul> <p>$$ \\frac{\\sqrt[n]{\\prod_{i}^{n} x_i} }{\\frac{1}{n}\\sum_{i}^n x_i} $$ for low contrast images this metric is lower than for high contrast images.</p>"},{"location":"user_guide/monitoring/#monitoring-status","title":"Monitoring Status","text":"<p>All the entities being monitored are associated with a status, which can be one of the following:</p> Status Description OK the entity is behaving as expected. WARNING the entity has shown signs of drifts, but it is still within the acceptable range. DRIFT the entity has experienced a significant change and corrective actions should be taken. <p>The following state diagram illustrates the possible transitions between the statuses, as well as the events that trigger them.</p> <pre><code>stateDiagram-v2\n\n    [*] --&gt; OK : Initial State\n\n    OK --&gt; WARNING : Warning On\n    WARNING --&gt; OK : Set new reference \n    WARNING --&gt; OK : Warning Off\n\n\n    WARNING --&gt; DRIFT : Drift On\n    DRIFT --&gt; WARNING : Drift Off\n\n    DRIFT --&gt; OK : Set new reference\n    DRIFT --&gt; OK : Drift Off</code></pre> <p>Notice that a Drift Off event can either bring the entity back to the <code>OK</code> status or to the <code>WARNING</code> status,  depending on the velocity and intensity of the change and the monitoring algorithm's sensitivity. The same applies to the Drift On events, which can both occur when the entity is in the <code>WARNING</code> status or in the <code>OK</code> status.</p> <p>The only transitions which are not due to Detection Events are the ones caused by the specification of a new reference.  In this case, the status of the entity is reset to <code>OK</code> for every entity as all the monitoring algorithms are reinitialized on the new reference.</p> <p>You can check the status of the monitored entities both through the WebApp and the SDK. In particular, the homepage of the Task  displays the status of both monitoring targets and metrics, while the SDK provides a couple of methods to  retrieve the status of the monitored entities programmatically.</p> SDK Example <p>You can visualize the status of a Monitoring Target using the  <code>get_monitoring_status</code> method of the <code>Client</code> object:</p> <pre><code>status = client.get_monitoring_status(\n    task_id=task_id, \n    monitoring_target=MonitoringTarget.INPUT,\n)\n</code></pre> <p>In the same way, you can retrieve the status of a Monitoring Metric:</p> <pre><code>status = client.get_monitoring_status(\n    task_id=task_id, \n    monitoring_target=MonitoringTarget.USER_INPUT,\n    monitoring_metric=MonitoringMetric.TEXT_TOXICITY,\n)\n</code></pre> <p>The method returns a BaseModel of type <code>MonitoringQuantityStatus</code> that contains the status of the  specified monitoring entity.</p>"},{"location":"user_guide/monitoring/detection_event/","title":"Detection Event","text":"<p>A Detection Event is raised by the ML cube Platform when a significant change is detected in one of the entities being monitored.</p> <p>An event is characterized by the following attributes:</p> Attribute Description Eventy Type The type of the event. It's possible values are: <ul><li> <code>Warning On</code>: the monitoring entity is experiencing slight changes that might lead to a drift.</li><li> <code>Warning Off</code>: the monitoring entity has returned to the reference distribution. </li><li> <code>Drift On</code>: the monitoring entity has drifted from the reference distribution.</li><li> <code>Drift Off</code>: the monitoring entity has returned to the reference distribution.</li> </ul> Severity The severity of the event. It's provided only for drift events and it can be <code>Low</code>, <code>Medium</code>, or <code>High</code>. Monitoring Target The Monitoring Target being monitored. Monitoring Metric The Monitoring Metric being monitored. Specification The specification of the Monitoring Metric being monitored, if existing. Model Name The name of the model that raised the event. It's present only if the event is related to a model. Model Version The version of the model that raised the event. It's present only if the event is related to a model. Insert datetime The time when the event was raised. Sample timestamp The timestamp of the sample that triggered the event. Sample customer ID The ID of the sample that triggered the event. User feedback The feedback provided by the user on whether the event was expected or not. Segment ID The ID of the Segment being monitored."},{"location":"user_guide/monitoring/detection_event/#retrieve-detection-events","title":"Retrieve Detection Events","text":"<p>You can access the detection events generated by the Platform in two ways:</p> <ul> <li>SDK: it can be used to retrieve all detection events for a specific task programmatically.</li> <li>WebApp: navigate to the <code>Detection</code> section located in the task page's sidebar. Here, all detection events are displayed in a table,     with multiple filtering options available for useful event management. Additionally, the latest detection events identified are shown in the Task homepage,    in the section named \"Latest Detection Events\".</li> </ul> SDK Example <p>The following code demonstrates how to retrieve all detection events for a specific task.</p> <pre><code>detection_events = client.get_detection_events(task_id='my-task-id')\n</code></pre>"},{"location":"user_guide/monitoring/detection_event/#user-feedback","title":"User Feedback","text":"<p>When a <code>Drift On</code> event is raised, you can provide feedback on whether the event was expected or not. This feedback is then used  to tune the monitoring algorithms and improve their performance. The feedback can be provided through the WebApp, in the <code>Detection</code> section of the task page, or through the SDK.</p>"},{"location":"user_guide/monitoring/detection_event/#detection-event-rules","title":"Detection Event Rules","text":"<p>To automate actions upon the reception of a detection event, you can set up Detection Event Rules.  You can learn more about how to configure them in the Detection Event Rules section.</p>"},{"location":"user_guide/monitoring/detection_event_rules/","title":"Detection Event Rules","text":"<p>This section outlines how to configure automation to receive notifications or start retraining after a Detection Event occurs.</p> <p>When a Detection Event is produced, the ML cube Platform reviews all the Detection Event Rules you have set  and triggers those matching the event.</p> <p>Rules are specific to a task and are characterized by the following attributes:</p> Attribute Description Name A descriptive label of the rule. Detection Event Type The type of event that triggers the rule. Severity The severity of the event that triggers the rule. It is only applicable to drift events. If not specified, the rule will be triggered by drift events of any severity. Monitoring Target The Monitoring Target whose event should trigger the rule. Monitoring Metric The Monitoring Metric whose event should trigger the rule. Specification The specification of the Monitoring Metric being monitored, if existing. Model name The name of the model to which the rule applies. This is only required when the monitoring target is related to a model (such as <code>ERROR</code> or <code>PREDICTION</code>). Actions A list of actions to be executed sequentially when the rule is triggered. Segment ID It refers to the ID of the Segment where the rule is set."},{"location":"user_guide/monitoring/detection_event_rules/#detection-event-actions","title":"Detection Event Actions","text":"<p>Three types of actions are currently supported: notification, plot configuration and retrain.</p>"},{"location":"user_guide/monitoring/detection_event_rules/#notifications","title":"Notifications","text":"<p>These actions send notifications to external services when a detection event is triggered. The following notification options are available:</p> Channel Description Slack Notification Sends a notification to a Slack channel via webhook. Discord Notification Sends a notification to a Discord channel via webhook. Email Notification Sends an email to the provided email address. Teams Notification Sends a notification to Microsoft Teams via webhook. Mqtt Notification Sends a notification to an MQTT broker."},{"location":"user_guide/monitoring/detection_event_rules/#plot-configuration","title":"Plot Configuration","text":"<p>This action consists in creating two plot configurations when a Detection Event is triggered: the first one includes data preceding the event, while the second one includes data following the event.</p>"},{"location":"user_guide/monitoring/detection_event_rules/#retrain","title":"Retrain","text":"<p>Retrain Action enables the automatic retraining of your model. Therefore, it is only available when the target of the rule is related to a model. The retrain action does not need any parameter because it is automatically inferred from the <code>model name</code> attribute of the rule. Of course, the model must already have a Retrain Trigger associated before setting up this action.</p> SDK Example <p>The following code demonstrates how to create a rule that matches high severity drift events on the error of a model.  When triggered, it first sends a notification to the <code>ml3-platform-notifications</code> channel on your Slack workspace, using the  provided webhook URL, and then starts the retraining of the model.</p> <pre><code>rule_id = client.create_detection_event_rule(\n    name='Retrain model with notification',\n    task_id='my-task-id',\n    model_name='my-model',\n    severity=DetectionEventSeverity.HIGH,\n    detection_event_type=DetectionEventType.DRIFT_ON,\n    monitoring_target=MonitoringTarget.ERROR,\n    actions=[\n        SlackNotificationAction(\n            webhook='https://hooks.slack.com/services/...',\n            channel='ml3-platform-notifications'\n        ),\n        RetrainAction()\n    ],\n)\n</code></pre>"},{"location":"user_guide/monitoring/drift_explainability/","title":"Drift Explainability","text":"<p>Monitoring  is a crucial aspect of the machine learning lifecycle, as it enables tracking the model's performance and its data over time, ensuring the model continues to function as expected. However, monitoring only is not enough when it comes to the adaptation phase.</p> <p>In order to make the right decisions, you need to understand what were the main factors that led to the drift in the first place, so that the correct actions can be taken to mitigate it. s The ML cube Platform supports this process by providing what we refer to as Drift Explainability Report,  automatically generated upon the detection of a drift and containing several elements that should help you diagnose the root causes  of the change occurred.</p> <p>You can access the reports in the WebApp, by navigating to the <code>Drift Explainability</code> tab in the sidebar of the Task page.</p>"},{"location":"user_guide/monitoring/drift_explainability/#structure","title":"Structure","text":"<p>A Drift Explainability Report consists in comparing the reference data and the portion of production data where the drift was identified, hence  those belonging to the new data distribution. Notice that these reports are generated after a sufficient amount of samples has been collected  after the drift, in order to ensure statistical reliability of the results. If the data distribution moves back to the reference before enough samples are collected, the report might not be generated.</p> <p>Each report is composed of several entities, each providing a different perspective on the data and the drift occurred.  Most of them are specific to a certain Data Structure, so they might not be available for all Tasks.</p> <p>These entities can take the form of tables, plots, or textual explanations.  Observed and analyzed together, they should provide a comprehensive understanding of the drift and its underlying causes. These are the entities currently available:</p> <ul> <li><code>Feature Importance</code>: it's a barplot that illustrates how the significance of each feature differs between the reference   and the production datasets. Variations in a feature's values might suggest that its contribution to the model's predictions   has changed over time. This entity is available only for tasks with tabular data.</li> </ul> <p> </p> Example of a feature importance plot. <ul> <li><code>Variable discriminative power</code>: it's also a bar plot displays the influence of each feature, as well as the target,   in differentiating between the reference and the production datasets.   The values represent how strongly a given feature helps to distinguish the datasets, with higher values representing stronger   separating power. This entity is available only for tasks with tabular data.</li> </ul> <p> </p> Example of a variable discriminative power plot. <ul> <li><code>Drift Score</code>: it's a line plot that shows the evolution of the drift score over time. The drift score is a    measure of the statistical distance between a sliding window of the production data and the reference data. It also shows the threshold,   which is the value that the drift score must exceed to raise a drift alarm, and all the Detection Events that were triggered in   the time frame of the report. This plot helps in understanding how the drift evolved over time and the moments in which the difference   between the two datasets was higher. Notice that some postprocessing is applied on the events to account for the functioning of the drift detection algorithms.    Specifically,   we shift back the drift on events by a certain offset, aiming to point at the precise time when the drift actually started. As a result,   drift on events might be shown before the threshold is exceeded. This explainability entity is available for all tasks.</li> </ul> <p> </p> Example of a drift score plot with detection events of increasing severity displayed. <ul> <li> <p><code>Clusters Count</code>: it's an histogram plot that shows the distribution of reference and production samples across clusters created over reference data. It is obtained by fitting a clustering algorithm over the reference data and then assigning production data to these identified clusters. This plot can help understanding the nature of the drift by distinguishing between two main scenarios:</p> <ol> <li><code>Internal dynamic change</code>: if all production data align with the reference clusters, with no point labeled as outlier, then the drift is likely reflecting a change in the internal dynamics of the same distribution. For instance, production data may concentrate within specific sub-domains of the reference distribution.</li> <li><code>Distribution shift</code>: if most production data points are labeled as noise, which means that the clusters found defined on reference data are not able to capture the production data, then an actual distribution shift has likely occurred.</li> </ol> </li> </ul> <p> </p> Example of a cluster count plot. In this case most production data are assigned to the reference clusters, but with different proportions, which suggests that the drift is likely due to an internal reorganization within the same distribution. <ul> <li><code>Clustering scatter</code>: it's a scatter plot showing reference and production data in a 2 dimensional space. The color of the points represents the cluster they belong to, while the shape distinguishes between reference and production data. The clusters shown in this plot are the ones identified over the reference data (also shown in the <code>Clusters Count</code> plot). This plot can help understanding how the production data are mapped to the reference data, and how the clusters are distributed in the 2D space.</li> </ul> <p> </p> Example of a clustering scatter plot. <ul> <li><code>Clustering heatmap</code>: it's a heatmap which aims at showing the difference between reference and production data. Along with the clustering over reference data, another clustering is executed, this time over production data only. It is important to notice that the production clusters found by analyzing reference data do not necessarily map 1 to 1 with the ones found by analyzing production data. Those two clustering outputs are then compared in this heatmap. Given a cell at row R and column C, the intensity indicates how many production samples are assigned to reference cluster R and production cluster C. When the data distribution did not shift, the expected output is a chess-like heatmap, in which each production cluster matches with a reference cluster. When the data distribution shifted, the expected output is that most of production data are assigned to the noise row of the reference cluster, while belonging to specific production clusters.</li> </ul> <p> </p> Example of a clustering heatmap. It looks like there is a direct mapping between reference and production clusters, which suggests that the drift is likely not due to a distribution shift."}]}