{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ML cube Platform","text":"<p>Welcome to the ML cube Platform documentation!</p> <p>This website is designed to help you get the most out of ML cube Platform by providing clear, detailed explanations about the main concepts of the product and guidance on setup, configuration and usage. You will find definitions, tutorials, practical examples and everything you need to make full use of our product.</p> <p>Dive in to explore everything ML cube Platform has to offer, and feel free to reach out if you need further assistance. Let\u2019s get started!</p> <ul> <li> <p> Basic concepts</p> <p>Are you new to the ML cube Platform and don't know where to start?</p> <p>Start reading the Company page.</p> <p> More info</p> </li> <li> <p> Modules</p> <p>Do you want to learn what ML cube Platform can offer?</p> <p>Go to the Modules page.</p> <p> More info</p> </li> <li> <p> Data</p> <p>Data are not all the same! </p> <p>Discover how ML cube Platform handles data in the Data page.</p> <p> More info</p> </li> <li> <p> Integrations</p> <p>ML cube Platform is natively integrated with cloud solutions!</p> <p>Check how to configure them.</p> <p> More info</p> </li> <li> <p> API</p> <p>ML cube Platform has been developed with the API first mind!</p> <p>Explore how to use our SDK.</p> <p> More info</p> </li> <li> <p> Notebook Examples</p> <p>First time using our SDK?</p> <p>Check the Examples page to learn from them.</p> <p> More info</p> </li> </ul>"},{"location":"api/","title":"ML cube Platform API","text":"<p>ML cube Platform is an API-first application that provides full coverage of its functionalities both as Web Application and as API. In particular, we provide a Python SDK facilitating the interaction using high-level objects and automated procedures.</p> <ul> <li> <p> Python SDK</p> <p>Documentation page of Python SDK with low level details of available methods.</p> <p> More info</p> </li> <li> <p> Notebook Examples</p> <p>Jupyter notebooks examples showing most of the basic usage of ML cube Platform.</p> <p> More info</p> </li> </ul>"},{"location":"api/examples/","title":"Examples","text":"<p>We prepared for you a set of notebooks that can help you to understand how to use ML cube Platform SDK:</p> <ul> <li>0 - Company and Project</li> <li>1 - Task and Model</li> <li>2 - Production</li> </ul> <p>In the following notebooks you can see examples for specific type of tasks:</p> <ul> <li>Text multiclass classification</li> <li>Image multiclass classification</li> <li>RAG</li> </ul>"},{"location":"api/python/","title":"ML3 platform client SDK","text":"<p>ML cube Platform SDK is a Python client that helps you interacting with ML cube Platform using high-level objects. The available sections are:</p> <ul> <li>Client:     the Client is the main class to use the SDK, it is initialized with the application url (https://api.platform.mlcube.com) and with the user API Key.     It exposes all the methods to use ML cube Platform programmatically.</li> <li>Models:     Pydantic models used as support objects in the SDK.</li> <li>Enums:     Enumerations available in the SDK.</li> <li>Exceptions:     Exceptions raised by the Client.</li> </ul>"},{"location":"api/python/#installation","title":"Installation","text":"<pre><code>pip install ml3-platform-sdk\n</code></pre>"},{"location":"api/python/client/","title":"Client","text":""},{"location":"api/python/client/#ml3platformclient","title":"ML3PlatformClient","text":"<pre><code>ML3PlatformClient(\n   url: str, api_key: str, timeout: int = 60\n)\n</code></pre> <p>Client class is the single point of interaction with ML cube Platform APIs, it is initialized providing the <code>url</code> and the User <code>api_key</code>. Optionally, you can specify the <code>timeout</code> for the operations. Every operation is performed verifying the API Key and the permissions associated to the User that own that key.</p>"},{"location":"api/python/client/#methods-categories","title":"Methods categories","text":"<p>There are the following types of methods:</p> <ul> <li>entity creation: create the entity and return its identifier. It is used in the other methods to indicate the entity.</li> <li>entity update: modify the entity but do not return anything.</li> <li>entity getters: return a Pydantic <code>BaseModel</code> with the required entity.</li> <li>entity show: print to the stdout the entity, but they do not return anything.</li> <li>entity delete: delete the entity</li> <li>job submission: submit a job on ML cube Platform that will take some time. They return the job identifier that can be used to monitor its state.</li> <li>job waiters: given a job id wait the until the job is completed</li> </ul>"},{"location":"api/python/client/#exceptions","title":"Exceptions","text":"<p>The Client class raises only exceptions that are subclasses of <code>SDKClientException</code>. The exception has two fields that you can share with ML cube Support to get help in identifying the problem:</p> <ul> <li>error_code: unique identifier of the error</li> <li>error_message: message that explain the error</li> </ul> <p>The page is structured in different blocks of methods, one for each entity.</p> <p>Methods:</p>"},{"location":"api/python/client/#create_company","title":".create_company","text":"<pre><code>.create_company(\n   name: str, address: str, vat: str\n)\n</code></pre> <p>Create a company for the User, this method works only is the User has not a company yet. After the Company is created the User is the Company Owner.</p> <p>Args</p> <ul> <li>name  : the name of the company</li> <li>address  : the address of the company</li> <li>vat  : the vat of the company</li> </ul> <p>Returns</p> <ul> <li>company_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateCompanyException</code></p>"},{"location":"api/python/client/#get_company","title":".get_company","text":"<pre><code>.get_company()\n</code></pre> <p>Returns the company of the User</p> <p>Returns</p> <ul> <li>company  : <code>Company</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#update_company","title":".update_company","text":"<pre><code>.update_company(\n   name: (str|None), address: (str|None), vat: (str|None)\n)\n</code></pre> <p>Update company information.</p> <p>Empty values will not be updated.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : new the of the company</li> <li>address  : new billing address for the company</li> <li>vat  : new vat of the company</li> </ul> <p>Raises</p> <p><code>UpdateCompanyException</code></p>"},{"location":"api/python/client/#get_all_company_subscription_plans","title":".get_all_company_subscription_plans","text":"<pre><code>.get_all_company_subscription_plans()\n</code></pre> <p>Returns all subscription plans associated with the user company.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Returns</p> <ul> <li>subscription_plan_list  : <code>List[SubscriptionPlanInfo]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_active_subscription_plan","title":".get_active_subscription_plan","text":"<pre><code>.get_active_subscription_plan()\n</code></pre> <p>Returns the current active subscription plan associated with the user company. Returns None if no active subscription plan found.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Returns</p> <ul> <li>subscription_plan  : <code>SubscriptionPlanInfo | None</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_project","title":".create_project","text":"<pre><code>.create_project(\n   name: str, description: (str|None), default_storage_policy: StoragePolicy\n)\n</code></pre> <p>Create a project inside the company. You don't need to specify the company because a User belongs only to one company and it is retrieved automatically.</p> <p>Allowed Roles</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : the name of the project</li> <li>description  : optional description of the project</li> <li>default_storage_policy  : represents the default policy to     use for storing data in ML cube Platform</li> </ul> <p>Returns</p> <ul> <li>project_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateProjectException</code></p>"},{"location":"api/python/client/#get_projects","title":".get_projects","text":"<pre><code>.get_projects()\n</code></pre> <p>Get the list of all projects in the company the User has permissions to view.</p> <p>Returns</p> <ul> <li>projects_list  : <code>List[Project]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_project","title":".get_project","text":"<pre><code>.get_project(\n   project_id: str\n)\n</code></pre> <p>Get a project with the given id</p> <p>Allowed Roles</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : project identifier</li> </ul> <p>Returns</p> <ul> <li>project  : <code>Project</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#update_project","title":".update_project","text":"<pre><code>.update_project(\n   project_id: str, name: (str|None), description: (str|None),\n   default_storage_policy: (StoragePolicy|None)\n)\n</code></pre> <p>Update project details.</p> <p>Empty values will not be updated.</p> <p>Allowed Roles</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : project identifier</li> <li>name  : new name of the project</li> <li>description  : new description of the project</li> <li>default_storage_policy  : represents the default policy to     use for storing data in ML cube Platform</li> </ul> <p>Returns</p> <ul> <li>project  : <code>Project</code></li> </ul> <p>Raises</p> <p><code>UpdateProjectException</code></p>"},{"location":"api/python/client/#show_projects","title":".show_projects","text":"<pre><code>.show_projects()\n</code></pre> <p>Show a list all projects printing to stdout.</p> <p>Example output: <pre><code>Project ID                Name\n------------------------  ----------\n6475f8c9ebac5081e529s63f  my project\n</code></pre></p>"},{"location":"api/python/client/#create_task","title":".create_task","text":"<pre><code>.create_task(\n   project_id: str, name: str, tags: list[str], task_type: TaskType,\n   data_structure: DataStructure, cost_info: (TaskCostInfoUnion|None) = None,\n   optional_target: bool = False, text_language: (TextLanguage|None) = None,\n   positive_class: (str|int|bool|None) = None,\n   rag_contexts_separator: (str|None) = None\n)\n</code></pre> <p>Create a task inside the project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>name  : the name of the task</li> <li>tags  : a list of tags associated with the task</li> <li>task_type  : the type of the task. See <code>TaskType</code>     documentation for more information</li> <li>data_structure  : type of data in the task</li> <li>cost_info  : optional argument that specify the cost     information of the task</li> <li>optional_target  : True if the target value in not always     available. This changes the behaviour and the detection     phase of ML cube Platform that will analyse production     data without considering the actual target</li> <li>text_language  : required for NLP tasks, it specifies the     language used in the task.</li> <li>positive_class  : required for binary classification tasks,     it specifies the positive class of the target.</li> <li>rag_contexts_separator  : Separator used to separate rag contexts     from different sources. If missing then only one source exists.</li> </ul> <p>Returns</p> <ul> <li>task_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateTaskException</code></p>"},{"location":"api/python/client/#update_task","title":".update_task","text":"<pre><code>.update_task(\n   task_id: str, name: (str|None) = None, tags: (list[str]|None) = None,\n   cost_info: (TaskCostInfoUnion|None) = None\n)\n</code></pre> <p>Update task attributes.</p> <p><code>None</code> parameters are ignored for the update.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>name  : the name of the task</li> <li>tags  : a list of tags associated with the task. To remove     all the tags then pass an empty list.</li> <li>cost_info  : optional argument that specify the cost     information of the task</li> </ul> <p>Raises</p> <p><code>UpdateTaskException</code></p>"},{"location":"api/python/client/#get_tasks","title":".get_tasks","text":"<pre><code>.get_tasks(\n   project_id: str\n)\n</code></pre> <p>Get the list of the Tasks inside the project.</p> <p>Args</p> <ul> <li>project_id  : identifier of the project</li> </ul> <p>Returns</p> <ul> <li>task_list  : <code>List[Task]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_task","title":".get_task","text":"<pre><code>.get_task(\n   task_id: str\n)\n</code></pre> <p>Get task by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>task  : <code>Task</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_tasks","title":".show_tasks","text":"<pre><code>.show_tasks(\n   project_id: str\n)\n</code></pre> <p>Show a list of tasks included in a project to stdout.</p> <p>Args</p> <ul> <li>project_id  : the identifier of a project</li> </ul> <p>Example output: <pre><code>Task ID                   Name     Type            Status     Status start date\n------------------------  -------  --------------  --------   -----------------\n6476040d583201813ab4539a  my task  classification  OK         03-02-2023 10:14:06\n</code></pre></p>"},{"location":"api/python/client/#create_model","title":".create_model","text":"<pre><code>.create_model(\n   task_id: str, name: str, version: str, with_probabilistic_output: bool,\n   metric_name: (ModelMetricName|None),\n   preferred_suggestion_type: (SuggestionType|None) = None,\n   retraining_cost: float = 0.0, resampled_dataset_size: (int|None) = None\n)\n</code></pre> <p>Create a model inside the task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>name  : the name of the model</li> <li>version  : the current version of the model</li> <li>metric_name  : performance or error metric associated with     the model</li> <li>retraining_cost  : estimated costs in the Task currency to     retrain the model. This information is used by the     retraining tool to show gain-cost information.     Default value is 0.0 meaning that the cost is negligible</li> <li>preferred_suggestion_type  : preferred type of suggestion that     will be computed to retrain the model</li> <li>resampled_dataset_size  : size of the resampled dataset that     will be proposed to retrain the model     note: this parameter is required if     <code>preferred_suggestion_type</code> is     <code>SuggestionType.RESAMPLED_DATASET</code></li> </ul> <p>Returns</p> <ul> <li>model_id  : <code>str</code> identifier of the created model</li> </ul> <p>Raises</p> <p><code>CreateModelException</code></p>"},{"location":"api/python/client/#get_models","title":".get_models","text":"<pre><code>.get_models(\n   task_id: str\n)\n</code></pre> <p>Get all models of a task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>models_list  : <code>List[Model]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_model","title":".get_model","text":"<pre><code>.get_model(\n   model_id: str\n)\n</code></pre> <p>Get model by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : identifier of the model</li> </ul> <p>Returns</p> <ul> <li>model  : <code>Model</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_model_by_name_and_version","title":".get_model_by_name_and_version","text":"<pre><code>.get_model_by_name_and_version(\n   task_id: str, model_name: str, model_version: str\n)\n</code></pre> <p>Get model by name and version.</p> <p>A Model can have multiple versions according to the updates and retraining done. This method allow to get the Model object by specifying its name and the version tag.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>model_name  : the name of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Returns</p> <ul> <li>model  : <code>Model</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_models","title":".show_models","text":"<pre><code>.show_models(\n   task_id: str\n)\n</code></pre> <p>Show a list of models included in a task to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Example output: <pre><code>Model Id                  Task Id                   Name                    Version    Status           Status start timestamp    Status insert date          Metric Name\n------------------------  ------------------------  ----------------------  ---------  ---------------  ------------------------  --------------------------  --------------------\n64fecf7d323311ab78f17280  64fecf7c323311ab78f17262  model_local_experiment  v0.0.1     ok                                         2023-09-11 08:27:41.431000  ModelMetricName.RMSE\n</code></pre></p>"},{"location":"api/python/client/#get_suggestions_info","title":".get_suggestions_info","text":"<pre><code>.get_suggestions_info(\n   model_id: str, model_version: str\n)\n</code></pre> <p>Retrieve suggestions associated with a model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Returns</p> <ul> <li>suggestion_info_list  : <code>List[SuggestionInfo]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_suggestions","title":".show_suggestions","text":"<pre><code>.show_suggestions(\n   model_id: str, model_version: str\n)\n</code></pre> <p>Show the list of suggestions associated with a model printing them to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>model_version  : the version of the model</li> </ul> <p>Example output: <pre><code>Suggestion Id                     Executed    Timestamp\n--------------------------------  ----------  --------------------------\n79a8710c351c4b6a9ece7322e153f200  True        2023-08-21 10:54:40.386189\n</code></pre></p> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_model_suggestion_type","title":".set_model_suggestion_type","text":"<pre><code>.set_model_suggestion_type(\n   model_id: str, preferred_suggestion_type: SuggestionType,\n   resampled_dataset_size: (int|None) = None\n)\n</code></pre> <p>Set model suggestion type.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the task</li> <li>preferred_suggestion_type  : preferred type of suggestion that     will be computed to retrain the model</li> <li>resampled_dataset_size  : size of the resampled dataset that     will be proposed to retrain the model     note: this parameter is required if     <code>preferred_suggestion_type</code> is     <code>SuggestionType.RESAMPLED_DATASET</code></li> </ul> <p>Raises</p> <p><code>SetModelSuggestionTypeException</code></p>"},{"location":"api/python/client/#update_model_version_by_suggestion_id","title":".update_model_version_by_suggestion_id","text":"<pre><code>.update_model_version_by_suggestion_id(\n   model_id: str, new_model_version: str, suggestion_id: str\n)\n</code></pre> <p>Update model version by suggestion id. To retrain the Model, ML cube Platform provides importance weights through a <code>SuggestionInfo</code>. After the retraining is completed, you use this method to create the new model version in ML cube Platform. By specifying the <code>suggestion_id</code>, ML cube Platform automatically knows which is the reference data the model is trained on.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model</li> <li>suggestion_id  : the identifier of the suggestion</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> job identifier of the pipeline in execution</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p>"},{"location":"api/python/client/#update_model_version","title":".update_model_version","text":"<pre><code>.update_model_version(\n   model_id: str, new_model_version: str\n)\n</code></pre> <p>Update model version with empty reference data. It is similar to create model but it does not actually create a different Model but a new version of the existent one. After this request, it is possible to upload data with add_historical_data. To start monitoring is required to set the reference of this new model with set_reference request.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model</li> </ul> <p>Returns</p> <ul> <li>model_id  : <code>str</code> job identifier of new model id</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p>"},{"location":"api/python/client/#update_model_version_from_time_range","title":".update_model_version_from_time_range","text":"<pre><code>.update_model_version_from_time_range(\n   model_id: str, new_model_version: str, from_timestamp: float,\n   to_timestamp: float\n)\n</code></pre> <p>Update model version by specifying the time range of uploaded data on ML cube Platform.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> <li>new_model_version  : the new version of the model from_timestamp to_timestamp</li> </ul> <p>Raises</p> <p><code>UpdateModelVersionException</code></p> <p>Returns the job_id associated to the pipeline</p>"},{"location":"api/python/client/#add_data_schema","title":".add_data_schema","text":"<pre><code>.add_data_schema(\n   task_id: str, data_schema: DataSchema\n)\n</code></pre> <p>Associate a data schema to a task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>data_schema  : the data schema that characterize your task</li> </ul> <p>Raises</p> <p><code>AddDataSchemaException</code></p>"},{"location":"api/python/client/#get_data_schema","title":".get_data_schema","text":"<pre><code>.get_data_schema(\n   task_id: str\n)\n</code></pre> <p>Get task data schema</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Returns</p> <ul> <li>data_schema  : <code>DataSchema</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_data_schema","title":".show_data_schema","text":"<pre><code>.show_data_schema(\n   task_id: str\n)\n</code></pre> <p>Show data schema of associated with a task</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : identifier of the task</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p> <p>Example output:</p> <pre><code>Column name       Role     Type      Nullable\n----------------  -------  --------  ----------\nsample_id         id       string    False\ntimestamp         time_id  string    False\nsepallength       input    float     False\nsepalwidth        input    float     False\npetallength       input    float     False\npetalwidth        input    float     False\nclass             target   category  False\n</code></pre>"},{"location":"api/python/client/#add_historical_data","title":".add_historical_data","text":"<pre><code>.add_historical_data(\n   task_id: str, inputs: Data, target: (Data|None) = None,\n   predictions: (list[tuple[str, Data]]|None) = None\n)\n</code></pre> <p>Add a batch of historical data for the Task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>inputs  : data object that contain input data source.     It can be None if you upload other kinds of data</li> <li>target  : data object that contains target data.     It can be None if you upload other kinds of data</li> <li>predictions  : list of data objects that contain prediction data.     Each element is a tuple with model_id and data object.     It can be None if you upload other kinds of data.     Predictions are mandatory for RAG tasks while not     permitted for the other TaskType.</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddHistoricalDataException</code></p>"},{"location":"api/python/client/#add_target_data","title":".add_target_data","text":"<pre><code>.add_target_data(\n   task_id: str, target: Data\n)\n</code></pre> <p>Add target samples for data already uploaded on the Task. This operation is used for Tasks with optional target which is manually labelled. For instance, fter the labelling process (maybe with our Active Learning module) you have a set of labelled samples spread over all the uploaded data. Indeed, they can belong to different data batches (historical or production consistent uploads) and can be a subset of the uploaded data.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>target  : data object that contains target data</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddHistoricalDataException</code></p>"},{"location":"api/python/client/#set_model_reference","title":".set_model_reference","text":"<pre><code>.set_model_reference(\n   model_id: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Specify data to use as reference for the model with time range. Data need to be already uploaded on ML cube Platform.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model from_timestamp to_timestamp</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddModelReferenceException</code></p>"},{"location":"api/python/client/#add_production_data","title":".add_production_data","text":"<pre><code>.add_production_data(\n   task_id: str, inputs: (Data|None) = None, target: (Data|None) = None,\n   predictions: (list[tuple[str, Data]]|None) = None\n)\n</code></pre> <p>Add a batch of production data associated with a given task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>inputs  : data object that contain input data source.     It can be None if you upload other kinds of data</li> <li>target  : data object that contains target data.     It can be None if you upload other kinds of data</li> <li>predictions  : list of data objects that contain prediction data.     Each element is a tuple with model_id and data object.     It can be None if you upload other kinds of data</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddProductionDataException</code></p>"},{"location":"api/python/client/#create_kpi","title":".create_kpi","text":"<pre><code>.create_kpi(\n   project_id: str, name: str\n)\n</code></pre> <p>Create a KPI.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>name  : the name of the kpi</li> </ul> <p>Returns</p> <ul> <li>kpi_id  : <code>str</code> identifier of the created kpi</li> </ul> <p>Raises</p> <p><code>CreateKpiException</code></p>"},{"location":"api/python/client/#get_kpi","title":".get_kpi","text":"<pre><code>.get_kpi(\n   kpi_id: str\n)\n</code></pre> <p>Get kpi by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>kpi_id  : identifier of the kpi</li> </ul> <p>Returns</p> <ul> <li>kpi  : <code>KPI</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_kpis","title":".get_kpis","text":"<pre><code>.get_kpis(\n   project_id: str\n)\n</code></pre> <p>Get all kpis of a project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : identifier of the project</li> </ul> <p>Returns</p> <ul> <li>kpis_list  : <code>List[KPI]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_kpis","title":".show_kpis","text":"<pre><code>.show_kpis(\n   project_id: str\n)\n</code></pre> <p>Show the list of KPIs included in a project to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> </ul> <p>Example output: <pre><code>KPI Id                    Project Id                Name                    Status           Status start timestamp    Status insert date\n------------------------  ------------------------  ----------------------  ---------------  ------------------------  --------------------------\n64fecf7d323311ab78f17280  64fecf7c323311ab78f17262  model_local_experiment  not_initialized                            2023-09-11 08:27:41.431000\n</code></pre></p>"},{"location":"api/python/client/#add_kpi_data","title":".add_kpi_data","text":"<pre><code>.add_kpi_data(\n   project_id: str, kpi_id: str, kpi: TabularData\n)\n</code></pre> <p>Add a batch of a given kpi with the given project.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the identifier of the project</li> <li>kpi_id  : the identifier of the kpi</li> <li>kpi  : data object that contains data source.</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>AddKPIDataException</code></p>"},{"location":"api/python/client/#compute_retraining_report","title":".compute_retraining_report","text":"<pre><code>.compute_retraining_report(\n   model_id: str\n)\n</code></pre> <p>Compute the retraining report for a given model</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p><code>ComputeRetrainingReportException</code></p>"},{"location":"api/python/client/#get_retraining_report","title":".get_retraining_report","text":"<pre><code>.get_retraining_report(\n   model_id: str\n)\n</code></pre> <p>For a given model id, get the sample weights computed and additional information about them included in the retraining report</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the identifier of the model</li> </ul> <p>Returns</p> <ul> <li>retraining_report  : <code>RetrainingReport</code></li> </ul> <p>Raises</p> <p><code>GetRetrainingReportException</code></p>"},{"location":"api/python/client/#compute_rag_evaluation_report","title":".compute_rag_evaluation_report","text":"<pre><code>.compute_rag_evaluation_report(\n   task_id: str, report_name: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Compute the RAG evaluation report for a given task.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles: - At least <code>PROJECT_EDIT</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>report_name  : the name of the report</li> <li>from_timestamp  : start timestamp of the samples to evaluate</li> <li>to_timestamp  : end timestamp of the samples to evaluate</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p>ComputeRagEvaluationReportException</p>"},{"location":"api/python/client/#get_rag_evaluation_reports","title":".get_rag_evaluation_reports","text":"<pre><code>.get_rag_evaluation_reports(\n   task_id: str\n)\n</code></pre> <p>For a given task id, get the computed RAG evaluation reports.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Returns</p> <ul> <li>rag_eval_reports  : list[TaskRagEvalReportItem]</li> </ul>"},{"location":"api/python/client/#export_rag_evaluation_report","title":".export_rag_evaluation_report","text":"<pre><code>.export_rag_evaluation_report(\n   report_id: str, folder: str, file_name: str\n)\n</code></pre> <p>Export a RAG evaluation report to a file.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>report_id  : the identifier of the report</li> <li>folder  : the folder where the report will be saved</li> <li>file_name  : the name of the file where the report will be saved</li> </ul> <p>Returns</p> <p>None</p>"},{"location":"api/python/client/#compute_topic_modeling_report","title":".compute_topic_modeling_report","text":"<pre><code>.compute_topic_modeling_report(\n   task_id: str, report_name: str, from_timestamp: float, to_timestamp: float\n)\n</code></pre> <p>Compute the topic modeling report for a given task. This functionality is available only for tasks based on text data. The data on which the report is computed is determined by the timestamps.</p> <p>This request starts an operation pipeline that is executed by ML cube Platform. Thus, the method returns the identifier of the job that you can monitor to know its status and proceed with the other work using the method <code>wait_job_completion(job_id)</code></p> <p>Allowed Roles: - At least <code>PROJECT_EDIT</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>report_name  : the name of the report</li> <li>from_timestamp  : start timestamp of the samples to evaluate</li> <li>to_timestamp  : end timestamp of the samples to evaluate</li> </ul> <p>Returns</p> <ul> <li>job_id  : <code>str</code> identifier of the submitted job</li> </ul> <p>Raises</p> <p>ComputeTopicModelingReportException</p>"},{"location":"api/python/client/#get_topic_modeling_reports","title":".get_topic_modeling_reports","text":"<pre><code>.get_topic_modeling_reports(\n   task_id: str\n)\n</code></pre> <p>For a given task id, get the computed topic modeling reports.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> </ul> <p>Returns</p> <ul> <li>topic_modeling_reports  : list[TaskTopicModelingReportItem]</li> </ul>"},{"location":"api/python/client/#get_topic_modeling_report","title":".get_topic_modeling_report","text":"<pre><code>.get_topic_modeling_report(\n   report_id: str\n)\n</code></pre> <p>For a given task id, get the computed topic modeling report for a specific report id.</p> <p>Allowed Roles: - At least <code>PROJECT_VIEW</code> for that project - <code>COMPANY_OWNER</code> - <code>COMPANY_ADMIN</code></p> <p>Args</p> <ul> <li>report_id  : the identifier of the report</li> </ul> <p>Returns</p> <ul> <li>topic_modeling_report_detail  : TaskTopicModelingReportDetails</li> </ul>"},{"location":"api/python/client/#get_monitoring_status","title":".get_monitoring_status","text":"<pre><code>.get_monitoring_status(\n   task_id: str, monitoring_target: MonitoringTarget,\n   monitoring_metric: (MonitoringMetric|None) = None\n)\n</code></pre> <p>Get the monitoring status of a monitoring target (or metric) in a task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : the identifier of the task</li> <li>monitoring_target  : the type of monitoring target to get the status</li> <li>monitoring_metric  : the type of monitoring metric to get the status     If <code>None</code>, only the monitoring target is considered</li> </ul> <p>Returns</p> <ul> <li>monitoring_quantity_status  : <code>MonitoringQuantityStatus</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_jobs","title":".get_jobs","text":"<pre><code>.get_jobs(\n   project_id: (str|None) = None, task_id: (str|None) = None,\n   model_id: (str|None) = None, status: (JobStatus|None) = None,\n   job_id: (str|None) = None\n)\n</code></pre> <p>Get current jobs information. Jobs can be filtered by project_id, task_id, model_id or status.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : the project_id to filter job.     If <code>None</code> job of every project will be returned</li> <li>task_id  : the task_id to filter job.     If <code>None</code> job of every task will be returned</li> <li>model_id  : the model_id to filter job.     If <code>None</code> job of every model will be returned</li> <li>status  : the status to filter job.     If <code>None</code> job with every status will be retrieved</li> <li>job_id  : id of the job to filter.     If <code>None</code> job with every id will be retrieved</li> </ul> <p>Returns</p> <ul> <li>jobs_list  : <code>List[Job]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_job","title":".get_job","text":"<pre><code>.get_job(\n   job_id: str\n)\n</code></pre> <p>Get current job information.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>job_id  : id of the job to retrieve</li> </ul> <p>Returns</p> <ul> <li>job  : <code>Job</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_jobs","title":".show_jobs","text":"<pre><code>.show_jobs()\n</code></pre> <p>Show current job information to stdout.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_events","title":".get_detection_events","text":"<pre><code>.get_detection_events(\n   task_id: str\n)\n</code></pre> <p>Get all detection event of a given task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : id of the task for which you want to retrieve the detection event</li> </ul> <p>Returns</p> <ul> <li>rules_list  : <code>List[DetectionEvent]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_event_rules","title":".get_detection_event_rules","text":"<pre><code>.get_detection_event_rules(\n   task_id: str\n)\n</code></pre> <p>Get all detection event rules of a given task.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>task_id  : id of the task for which you want to retrieve the detection event rules</li> </ul> <p>Returns</p> <ul> <li>rules_list  : <code>List[DetectionEventRule]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_detection_event_rule","title":".get_detection_event_rule","text":"<pre><code>.get_detection_event_rule(\n   rule_id: str\n)\n</code></pre> <p>Get a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : id of the rule</li> </ul> <p>Returns</p> <ul> <li>rule  : <code>DetectionEventRule</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_detection_event_user_feedback","title":".set_detection_event_user_feedback","text":"<pre><code>.set_detection_event_user_feedback(\n   detection_id: str, user_feedback: bool\n)\n</code></pre> <p>Get a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>detection_id  : id of the detection event</li> <li>user_feedback  : user feedback</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_detection_event_rule","title":".create_detection_event_rule","text":"<pre><code>.create_detection_event_rule(\n   name: str, task_id: str, severity: (DetectionEventSeverity|None),\n   detection_event_type: DetectionEventType, monitoring_target: MonitoringTarget,\n   actions: list[DetectionEventAction],\n   monitoring_metric: (MonitoringMetric|None) = None, model_name: (str|None) = None\n)\n</code></pre> <p>Create a detection event rule.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : the name of the rule</li> <li>task_id  : the id of the task to which the rule belongs.     The rule will only respond to detection events     generated by this task.</li> <li>model_name  : the name of the model, only required if     monitoring_target is set to MODEL.</li> <li>detection_event_type  : the type of detection event that     this rule should respond to.</li> <li>monitoring_target  : the type of monitoring target that     this rule should respond to.</li> <li>monitoring_metric  : additional metric extracted from     monitoring target that is monitored</li> <li>severity  : the level of severity of the detection event     that this rule should respond to. None means any     severity is matched.</li> <li>actions  : the list of actions to execute, in order,     when the conditions of the rule are matched.</li> </ul> <p>Returns</p> <ul> <li>rule_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>CreateDetectionEventRuleException</code></p>"},{"location":"api/python/client/#update_detection_event_rule","title":".update_detection_event_rule","text":"<pre><code>.update_detection_event_rule(\n   rule_id: str, name: (str|None) = None, model_name: (str|None) = None,\n   severity: (DetectionEventSeverity|None) = None,\n   detection_event_type: (DetectionEventType|None) = None,\n   monitoring_target: (MonitoringTarget|None) = None,\n   actions: (list[DetectionEventAction]|None) = None\n)\n</code></pre> <p>Update a detection event rule.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : the id of the rule to update</li> <li>name  : the name of the rule. If None, keeps the existing value.</li> <li>model_name  : the name of the model, only required if     monitoring_target is set to MODEL.     If None, keeps the existing value.</li> <li>detection_event_type  : the type of detection event that this     rule should respond to. If None, keeps the existing value.</li> <li>monitoring_target  : the type of monitoring target that this     rule should respond to. If None, keeps the existing value.</li> <li>severity  : the level of severity of the detection event that     this rule should respond to. If None, keeps the     existing value.</li> <li>actions  : the list of actions to execute, in order, when the     conditions of the rule are matched. If None,      keeps the existing value.</li> </ul> <p>Raises</p> <p><code>CreateDetectionEventRuleException</code></p>"},{"location":"api/python/client/#delete_detection_event_rule","title":".delete_detection_event_rule","text":"<pre><code>.delete_detection_event_rule(\n   rule_id: str\n)\n</code></pre> <p>Delete a detection event rule by id.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_EDIT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>rule_id  : id of the rule to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#wait_job_completion","title":".wait_job_completion","text":"<pre><code>.wait_job_completion(\n   job_id: str, max_wait_timeout: int = 3000\n)\n</code></pre> <p>Wait that the ML cube Platform job terminates successfully its execution.</p> <p>Note that this method stops the execution.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>PROJECT_VIEW</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>job_id  : identifier of the job</li> <li>max_wait_timeout  : maximum amount of seconds to wait before     launching <code>JobWaitTimeoutException</code></li> </ul> <p>Raises</p> <ul> <li><code>JobWaitTimeoutException</code> when the maximum timeout time     is reached</li> <li><code>JobNotFoundException</code> when the requested job does not     exist</li> <li><code>JobFailureException</code> when the requested job is failed</li> </ul>"},{"location":"api/python/client/#create_company_user","title":".create_company_user","text":"<pre><code>.create_company_user(\n   name: str, surname: str, username: str, password: str, email: str,\n   company_role: UserCompanyRole\n)\n</code></pre> <p>Creates a new User in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> </ul> <p>Args</p> <ul> <li>name  : name of the user</li> <li>surname  : surname of the user</li> <li>username  : username of the user</li> <li>password  : temporary password for the user. It will change     this at the first login</li> <li>email  : email of the user</li> <li>company_role  : role of the user inside the company</li> </ul> <p>Returns</p> <ul> <li>user_id  : <code>str</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_company_users","title":".get_company_users","text":"<pre><code>.get_company_users()\n</code></pre> <p>Returns the list of users in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>users_list  : <code>List[CompanyUser]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#change_user_company_role","title":".change_user_company_role","text":"<pre><code>.change_user_company_role(\n   user_id: str, company_role: UserCompanyRole\n)\n</code></pre> <p>Change the company role of a user in the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code>: can change the roles of all the Users     Users apart from other Admins and the Owner</li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which the role is updated</li> <li>company_role  : the new role to assign</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_company_users","title":".show_company_users","text":"<pre><code>.show_company_users()\n</code></pre> <p>Show company users to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_user_projects","title":".get_user_projects","text":"<pre><code>.get_user_projects(\n   user_id: str\n)\n</code></pre> <p>Returns a list of projects that the user can view.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> </ul> <p>Returns</p> <ul> <li>projects_list  : <code>List[Project]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_user_projects","title":".show_user_projects","text":"<pre><code>.show_user_projects(\n   user_id: str\n)\n</code></pre> <p>Shows the projects that the user can view to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#add_user_project_role","title":".add_user_project_role","text":"<pre><code>.add_user_project_role(\n   user_id: str, project_id: str, project_role: UserProjectRole\n)\n</code></pre> <p>Add a project role to the user for the given project.</p> <p>The User Project role can be assigned only to <code>COMPANY_USER</code> because Admin and Owner already have all the permission over projects.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> <li>project_id  : identifies the project</li> <li>project_role  : the project role to assign</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_project_role","title":".delete_project_role","text":"<pre><code>.delete_project_role(\n   user_id: str, project_id: str\n)\n</code></pre> <p>Delete the role of the user for the given project.</p> <p>The User Project role can be deleted only for <code>COMPANY_USER</code> because Admin and Owner have all the permission over projects.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user for which you want to see the list</li> <li>project_id  : identifies the project</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_api_keys","title":".get_api_keys","text":"<pre><code>.get_api_keys()\n</code></pre> <p>Returns a list of api keys the user has.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>api_keys_list  : <code>List[ApiKey]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_api_keys","title":".show_api_keys","text":"<pre><code>.show_api_keys()\n</code></pre> <p>Shows the list of api keys the user has to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_api_key","title":".create_api_key","text":"<pre><code>.create_api_key(\n   name: str, expiration_time: ApiKeyExpirationTime\n)\n</code></pre> <p>Create a new api key for the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Returns</p> <ul> <li>name  : the name of the api key</li> <li>expiration_time  : the expiration time of the api key</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_api_key","title":".delete_api_key","text":"<pre><code>.delete_api_key(\n   api_key: str\n)\n</code></pre> <p>Delete the api key of the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_USER</code></li> </ul> <p>Args</p> <ul> <li>api_key  : api key to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_user_api_keys","title":".get_user_api_keys","text":"<pre><code>.get_user_api_keys(\n   user_id: str\n)\n</code></pre> <p>Get the list of api keys a user has.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to get his api keys</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#show_user_api_keys","title":".show_user_api_keys","text":"<pre><code>.show_user_api_keys(\n   user_id: str\n)\n</code></pre> <p>Shows the list of api keys a user has to stdout.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to get his api keys</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_user_api_key","title":".create_user_api_key","text":"<pre><code>.create_user_api_key(\n   user_id: str, name: str, expiration_time: ApiKeyExpirationTime\n)\n</code></pre> <p>Create a new api key for the user.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user to create a new api key</li> <li>name  : the name of the api key</li> <li>expiration_time  : the expiration time of the api key</li> </ul> <p>Returns</p> <ul> <li>api_key  : the new created api key for the user</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_user_api_key","title":".delete_user_api_key","text":"<pre><code>.delete_user_api_key(\n   user_id: str, api_key: str\n)\n</code></pre> <p>Delete the api key of the user</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code>     Admin</li> </ul> <p>Args</p> <ul> <li>user_id  : the user to delete an api key</li> <li>api_key  : the api key to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#change_company_owner","title":".change_company_owner","text":"<pre><code>.change_company_owner(\n   user_id: str\n)\n</code></pre> <p>Change the company owner role from the requesting user to the other user.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> </ul> <p>Args</p> <ul> <li>user_id  : the user that become Company Owner</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_company_user","title":".delete_company_user","text":"<pre><code>.delete_company_user(\n   user_id: str\n)\n</code></pre> <p>Delete a user from the company.</p> <p>Allowed Roles:</p> <ul> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code>: cannot delete other company admins</li> </ul> <p>Args</p> <ul> <li>user_id  : the user to delete</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_integration_credentials","title":".get_integration_credentials","text":"<pre><code>.get_integration_credentials(\n   credentials_id: str\n)\n</code></pre> <p>Get the credentials with the given id for 3<sup>rd</sup> party service provider integration.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to retrieve.</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>IntegrationCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#get_all_project_integration_credentials","title":".get_all_project_integration_credentials","text":"<pre><code>.get_all_project_integration_credentials(\n   project_id: str\n)\n</code></pre> <p>Get the list of credentials for 3<sup>rd</sup> party service provider integrations that are currently configured in a project.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for that project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>project_id  : id of the project for which all configured credentials should be retrieved.</li> </ul> <p>Returns</p> <ul> <li>credentials_list  : <code>List[IntegrationCredentials]</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#delete_integration_credentials","title":".delete_integration_credentials","text":"<pre><code>.delete_integration_credentials(\n   credentials_id: str\n)\n</code></pre> <p>Delete credentials for the integration with a 3<sup>rd</sup> party  service provider.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code>: cannot delete other company admins</li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to delete.</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_integration_credentials_as_default","title":".set_integration_credentials_as_default","text":"<pre><code>.set_integration_credentials_as_default(\n   credentials_id: str\n)\n</code></pre> <p>Set the credentials with the given id as default for 3<sup>rd</sup> party  service provider integration.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project where the credentials have been configured</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>credentials_id  : id of the integration credentials to set as default.</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_aws_integration_credentials","title":".create_aws_integration_credentials","text":"<pre><code>.create_aws_integration_credentials(\n   name: str, default: bool, project_id: str, role_arn: str\n)\n</code></pre> <p>Create credentials to integrate with AWS. Returns an object that contains the external_id you will need to configure in your trust policy.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>role_arn  : the ARN of the IAM role that will be assumed by ML     cube Platform</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>SecretAWSCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_aws_compatible_integration_credentials","title":".create_aws_compatible_integration_credentials","text":"<pre><code>.create_aws_compatible_integration_credentials(\n   name: str, default: bool, project_id: str, access_key_id: str,\n   secret_access_key: str, endpoint_url: (str|None) = None\n)\n</code></pre> <p>Create credentials to integrate with AWS-compatible services.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>access_key_id  : the access key id</li> <li>secret_access_key  : the secret access key</li> <li>endpoint_url  : the endpoint url of the service. If None,     AWS itself is used</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>AWSCompatibleCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_gcp_integration_credentials","title":".create_gcp_integration_credentials","text":"<pre><code>.create_gcp_integration_credentials(\n   name: str, default: bool, project_id: str, service_account_info_json: str\n)\n</code></pre> <p>Create credentials to integrate with GCP.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>service_account_info_json  : the json-encoded string     containing the key of the service account</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>GCPCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#create_azure_integration_credentials","title":".create_azure_integration_credentials","text":"<pre><code>.create_azure_integration_credentials(\n   name: str, default: bool, project_id: str,\n   service_principal_credentials_json: str\n)\n</code></pre> <p>Create credentials to integrate with Azure.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>UPDATE_PROJECT_INFORMATION</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>name  : a simple name to identify this set of credentials</li> <li>default  : whether to use these credentials by default when     using an AWS integration</li> <li>project_id  : the project in which these credentials will     be configured</li> <li>service_principal_credentials_json  : the json-encoded string     containing the credentials of the service principal</li> </ul> <p>Returns</p> <ul> <li>credentials  : <code>AzureCredentials</code></li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#set_retrain_trigger","title":".set_retrain_trigger","text":"<pre><code>.set_retrain_trigger(\n   model_id: str, trigger: (RetrainTrigger|None)\n)\n</code></pre> <p>Set the retrain trigger for a given model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> <li>trigger  : the trigger to set. If you want to remove the     trigger, set it to None</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#test_retrain_trigger","title":".test_retrain_trigger","text":"<pre><code>.test_retrain_trigger(\n   model_id: str, trigger: RetrainTrigger\n)\n</code></pre> <p>Test the retrain trigger for a given model.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> <li>trigger  : the trigger to test</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/client/#retrain_model","title":".retrain_model","text":"<pre><code>.retrain_model(\n   model_id: str\n)\n</code></pre> <p>Retrain a model via the configured retrain trigger.</p> <p>Allowed Roles:</p> <ul> <li>At least <code>WORK_ON_PROJECT</code> for the project</li> <li><code>COMPANY_OWNER</code></li> <li><code>COMPANY_ADMIN</code></li> </ul> <p>Args</p> <ul> <li>model_id  : the id of the model</li> </ul> <p>Raises</p> <p><code>SDKClientException</code></p>"},{"location":"api/python/enums/","title":"Enums","text":""},{"location":"api/python/enums/#apikeyexpirationtime","title":"ApiKeyExpirationTime","text":"<pre><code>ApiKeyExpirationTime()\n</code></pre> <p>Fields:</p> <ul> <li>ONE_MONTH</li> <li>THREE_MONTHS</li> <li>SIX_MONTHS</li> <li>ONE_YEAR</li> <li>NEVER</li> </ul>"},{"location":"api/python/enums/#baseml3enum","title":"BaseML3Enum","text":"<pre><code>BaseML3Enum()\n</code></pre> <p>Base class for all enums in the ML3 Platform SDK</p>"},{"location":"api/python/enums/#columnrole","title":"ColumnRole","text":"<pre><code>ColumnRole()\n</code></pre> <p>Column role enum Describe the role of a column</p>"},{"location":"api/python/enums/#columnsubrole","title":"ColumnSubRole","text":"<pre><code>ColumnSubRole()\n</code></pre> <p>Column subrole enum Describe the subrole of a column</p>"},{"location":"api/python/enums/#currency","title":"Currency","text":"<pre><code>Currency()\n</code></pre> <p>Currency of to use for the Task</p>"},{"location":"api/python/enums/#datastructure","title":"DataStructure","text":"<pre><code>DataStructure()\n</code></pre> <p>Represents the typology of the data to send</p> <p>Fields:</p> <ul> <li>TABULAR</li> <li>IMAGE</li> <li>TEXT</li> <li>EMBEDDING</li> </ul>"},{"location":"api/python/enums/#datatype","title":"DataType","text":"<pre><code>DataType()\n</code></pre> <p>Data type enum Describe data type of input</p>"},{"location":"api/python/enums/#detectioneventactiontype","title":"DetectionEventActionType","text":"<pre><code>DetectionEventActionType()\n</code></pre> <p>Fields:</p> <ul> <li>DISCORD_NOTIFICATION</li> <li>SLACK_NOTIFICATION</li> <li>EMAIL_NOTIFICATION</li> <li>TEAMS_NOTIFICATION</li> <li>MQTT_NOTIFICATION</li> <li>RETRAIN</li> </ul>"},{"location":"api/python/enums/#detectioneventseverity","title":"DetectionEventSeverity","text":"<pre><code>DetectionEventSeverity()\n</code></pre> <p>Fields:</p> <ul> <li>LOW</li> <li>MEDIUM</li> <li>HIGH</li> </ul>"},{"location":"api/python/enums/#detectioneventtype","title":"DetectionEventType","text":"<pre><code>DetectionEventType()\n</code></pre> <p>Fields:</p> <ul> <li>DRIFT</li> </ul>"},{"location":"api/python/enums/#externalintegration","title":"ExternalIntegration","text":"<pre><code>ExternalIntegration()\n</code></pre> <p>An integration with a 3<sup>rd</sup> party service provider</p> <p>Fields: - AWS - GCP - AZURE - AWS_COMPATIBLE</p>"},{"location":"api/python/enums/#filetype","title":"FileType","text":"<pre><code>FileType()\n</code></pre> <p>Fields:</p> <ul> <li>CSV</li> <li>JSON</li> </ul>"},{"location":"api/python/enums/#foldertype","title":"FolderType","text":"<pre><code>FolderType()\n</code></pre> <p>Type of folder</p> <p>Fields</p> <ul> <li>UNCOMPRESSED</li> <li>TAR</li> <li>ZIP</li> </ul>"},{"location":"api/python/enums/#imagemode","title":"ImageMode","text":"<pre><code>ImageMode()\n</code></pre> <p>Image mode enumeration</p>"},{"location":"api/python/enums/#fields","title":"Fields","text":"<p>RGB: Red, Green, Blue RGBA: Red, Green, Blue, Alpha GRAYSCALE: Grayscale</p>"},{"location":"api/python/enums/#jobstatus","title":"JobStatus","text":"<pre><code>JobStatus()\n</code></pre> <p>Fields:</p> <ul> <li>IDLE</li> <li>STARTING</li> <li>RUNNING</li> <li>COMPLETED</li> <li>ERROR</li> </ul>"},{"location":"api/python/enums/#kpistatus","title":"KPIStatus","text":"<pre><code>KPIStatus()\n</code></pre> <p>Fields:</p> <ul> <li>NOT_INITIALIZED</li> <li>OK</li> <li>WARNING</li> <li>DRIFT</li> </ul>"},{"location":"api/python/enums/#modelmetricname","title":"ModelMetricName","text":"<pre><code>ModelMetricName()\n</code></pre> <p>Name of the model metrics that is associated with the model</p> <p>Fields: - RMSE - RSQUARE</p>"},{"location":"api/python/enums/#monitoringmetric","title":"MonitoringMetric","text":"<pre><code>MonitoringMetric()\n</code></pre> <p>Fields:</p> <ul> <li>FEATURE</li> <li>TEXT_TOXICITY</li> <li>TEXT_EMOTION</li> <li>TEXT_SENTIMENT</li> <li>MODEL_PERPLEXITY</li> </ul>"},{"location":"api/python/enums/#monitoringstatus","title":"MonitoringStatus","text":"<pre><code>MonitoringStatus()\n</code></pre> <p>Fields:</p> <ul> <li>OK</li> <li>WARNING</li> <li>DRIFT</li> </ul>"},{"location":"api/python/enums/#monitoringtarget","title":"MonitoringTarget","text":"<pre><code>MonitoringTarget()\n</code></pre> <p>Fields:</p> <ul> <li>ERROR</li> <li>INPUT</li> <li>CONCEPT</li> <li>PREDICTION</li> <li>USER_INPUT</li> <li>USER_INPUT_RETRIEVED_CONTEXT</li> <li>RETRIEVED_CONTEXT</li> <li>USER_INPUT_MODEL_OUTPUT</li> <li>MODEL_OUTPUT_RETRIEVED_CONTEXT</li> </ul>"},{"location":"api/python/enums/#productkeystatus","title":"ProductKeyStatus","text":"<pre><code>ProductKeyStatus()\n</code></pre> <p>Status of a product key</p>"},{"location":"api/python/enums/#fields_1","title":"Fields","text":"<p>NEW = generated but not yet used product key VALIDATING = validation requested from client IN_USE = validated product key, client activated</p>"},{"location":"api/python/enums/#retraintriggertype","title":"RetrainTriggerType","text":"<pre><code>RetrainTriggerType()\n</code></pre> <p>Enumeration of the possible retrain triggers</p>"},{"location":"api/python/enums/#storagepolicy","title":"StoragePolicy","text":"<pre><code>StoragePolicy()\n</code></pre> <p>Enumeration that specifies the storage policy for the data sent to ML cube Platform</p> <p>Fields:     cloud     it needs to read data</p>"},{"location":"api/python/enums/#storingdatatype","title":"StoringDataType","text":"<pre><code>StoringDataType()\n</code></pre> <p>Fields:</p> <ul> <li>HISTORICAL</li> <li>REFERENCE</li> <li>PRODUCTION</li> </ul>"},{"location":"api/python/enums/#subscriptiontype","title":"SubscriptionType","text":"<pre><code>SubscriptionType()\n</code></pre> <p>Type of subscription plan of a company</p>"},{"location":"api/python/enums/#fields_2","title":"Fields","text":"<p>CLOUD: subscription plan for web app or sdk access EDGE: subscription plan for edge deployment</p>"},{"location":"api/python/enums/#suggestiontype","title":"SuggestionType","text":"<pre><code>SuggestionType()\n</code></pre> <p>Enum to specify the preferred type of suggestion</p> <p>Fields: - SAMPLE_WEIGHTS - RESAMPLED_DATASET</p>"},{"location":"api/python/enums/#tasktype","title":"TaskType","text":"<pre><code>TaskType()\n</code></pre> <p>Fields:</p> <ul> <li>REGRESSION</li> <li>CLASSIFICATION_BINARY</li> <li>CLASSIFICATION_MULTICLASS</li> <li>CLASSIFICATION_MULTILABEL</li> <li>RAG</li> </ul>"},{"location":"api/python/enums/#textlanguage","title":"TextLanguage","text":"<pre><code>TextLanguage()\n</code></pre> <p>Enumeration of text language used in nlp tasks.</p>"},{"location":"api/python/enums/#fields_3","title":"Fields","text":"<p>ITALIAN ENGLISH MULTILANGUAGE</p>"},{"location":"api/python/enums/#usercompanyrole","title":"UserCompanyRole","text":"<pre><code>UserCompanyRole()\n</code></pre> <p>Fields:</p> <ul> <li>COMPANY_OWNER</li> <li>COMPANY_ADMIN</li> <li>COMPANY_USER</li> <li>COMPANY_NONE</li> </ul>"},{"location":"api/python/enums/#userprojectrole","title":"UserProjectRole","text":"<pre><code>UserProjectRole()\n</code></pre> <p>Fields:</p> <ul> <li>PROJECT_ADMIN</li> <li>PROJECT_USER</li> <li>PROJECT_VIEW</li> </ul>"},{"location":"api/python/exceptions/","title":"Exceptions","text":""},{"location":"api/python/exceptions/#adddataschemaexception","title":"AddDataSchemaException","text":"<pre><code>AddDataSchemaException()\n</code></pre> <p>AddDataSchemaException</p>"},{"location":"api/python/exceptions/#addhistoricaldataexception","title":"AddHistoricalDataException","text":"<pre><code>AddHistoricalDataException()\n</code></pre> <p>AddHistoricalDataException</p>"},{"location":"api/python/exceptions/#addkpidataexception","title":"AddKPIDataException","text":"<pre><code>AddKPIDataException()\n</code></pre> <p>AddKPIDataException</p>"},{"location":"api/python/exceptions/#addproductiondataexception","title":"AddProductionDataException","text":"<pre><code>AddProductionDataException()\n</code></pre> <p>AddProductionDataException</p>"},{"location":"api/python/exceptions/#addtargetdataexception","title":"AddTargetDataException","text":"<pre><code>AddTargetDataException()\n</code></pre> <p>AddTargetDataException</p>"},{"location":"api/python/exceptions/#computeragevaluationreportexception","title":"ComputeRagEvaluationReportException","text":"<pre><code>ComputeRagEvaluationReportException()\n</code></pre> <p>ComputeRagEvaluationReportException</p>"},{"location":"api/python/exceptions/#computeretrainingreportexception","title":"ComputeRetrainingReportException","text":"<pre><code>ComputeRetrainingReportException()\n</code></pre> <p>ComputeRetrainingReportException</p>"},{"location":"api/python/exceptions/#computetopicmodelingreportexception","title":"ComputeTopicModelingReportException","text":"<pre><code>ComputeTopicModelingReportException()\n</code></pre> <p>ComputeTopicModelingReportException</p>"},{"location":"api/python/exceptions/#createcompanyexception","title":"CreateCompanyException","text":"<pre><code>CreateCompanyException()\n</code></pre> <p>CreateCompanyException</p>"},{"location":"api/python/exceptions/#createdetectioneventruleexception","title":"CreateDetectionEventRuleException","text":"<pre><code>CreateDetectionEventRuleException()\n</code></pre> <p>CreateDetectionEventRuleException</p>"},{"location":"api/python/exceptions/#createkpiexception","title":"CreateKPIException","text":"<pre><code>CreateKPIException()\n</code></pre> <p>CreateKPIEventRuleException</p>"},{"location":"api/python/exceptions/#createmodelexception","title":"CreateModelException","text":"<pre><code>CreateModelException()\n</code></pre> <p>CreateModelException</p>"},{"location":"api/python/exceptions/#createprojectexception","title":"CreateProjectException","text":"<pre><code>CreateProjectException()\n</code></pre> <p>CreateProjectException</p>"},{"location":"api/python/exceptions/#createtaskexception","title":"CreateTaskException","text":"<pre><code>CreateTaskException()\n</code></pre> <p>CreateTaskException</p>"},{"location":"api/python/exceptions/#getragevaluationreportexception","title":"GetRagEvaluationReportException","text":"<pre><code>GetRagEvaluationReportException()\n</code></pre> <p>GetRagEvaluationReportException</p>"},{"location":"api/python/exceptions/#getretrainingreportexception","title":"GetRetrainingReportException","text":"<pre><code>GetRetrainingReportException()\n</code></pre> <p>GetRetrainingReportException</p>"},{"location":"api/python/exceptions/#gettopicmodelingreportexception","title":"GetTopicModelingReportException","text":"<pre><code>GetTopicModelingReportException()\n</code></pre> <p>GetTopicModelingReportException</p>"},{"location":"api/python/exceptions/#invalidactionlist","title":"InvalidActionList","text":"<pre><code>InvalidActionList()\n</code></pre> <p>Exception raised when the detection event actions in the rule are not valid</p>"},{"location":"api/python/exceptions/#jobfailureexception","title":"JobFailureException","text":"<pre><code>JobFailureException()\n</code></pre> <p>JobFailureException</p>"},{"location":"api/python/exceptions/#jobnotfoundexception","title":"JobNotFoundException","text":"<pre><code>JobNotFoundException()\n</code></pre> <p>JobNotFoundException</p>"},{"location":"api/python/exceptions/#jobwaittimeoutexception","title":"JobWaitTimeoutException","text":"<pre><code>JobWaitTimeoutException()\n</code></pre> <p>JobWaitTimeoutException</p>"},{"location":"api/python/exceptions/#sdkclientexception","title":"SDKClientException","text":"<pre><code>SDKClientException(\n   error_code: str = 'UNEXPECTED', error_message: str = 'Anunexpectederroroccurred'\n)\n</code></pre> <p>Base class for client sdk exceptions</p>"},{"location":"api/python/exceptions/#setmodelreferenceexception","title":"SetModelReferenceException","text":"<pre><code>SetModelReferenceException()\n</code></pre> <p>AddModelReferenceException</p>"},{"location":"api/python/exceptions/#setmodelsuggestiontypeexception","title":"SetModelSuggestionTypeException","text":"<pre><code>SetModelSuggestionTypeException()\n</code></pre> <p>SetModelSuggestionTypeException</p>"},{"location":"api/python/exceptions/#updatecompanyexception","title":"UpdateCompanyException","text":"<pre><code>UpdateCompanyException()\n</code></pre> <p>UpdateCompanyException</p>"},{"location":"api/python/exceptions/#updatedataschemaexception","title":"UpdateDataSchemaException","text":"<pre><code>UpdateDataSchemaException()\n</code></pre> <p>UpdateDataSchemaException</p>"},{"location":"api/python/exceptions/#updatedetectioneventruleexception","title":"UpdateDetectionEventRuleException","text":"<pre><code>UpdateDetectionEventRuleException()\n</code></pre> <p>UpdateDetectionEventRuleException</p>"},{"location":"api/python/exceptions/#updatemodelversionexception","title":"UpdateModelVersionException","text":"<pre><code>UpdateModelVersionException()\n</code></pre> <p>UpdateModelVersionException</p>"},{"location":"api/python/exceptions/#updateprojectexception","title":"UpdateProjectException","text":"<pre><code>UpdateProjectException()\n</code></pre> <p>UpdateProjectException</p>"},{"location":"api/python/exceptions/#updatetaskexception","title":"UpdateTaskException","text":"<pre><code>UpdateTaskException()\n</code></pre> <p>UpdateTaskException</p>"},{"location":"api/python/models/","title":"Models","text":""},{"location":"api/python/models/#awscompatiblecredentials","title":"AWSCompatibleCredentials","text":"<pre><code>AWSCompatibleCredentials()\n</code></pre> <p>AWS-compatible integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>access_key_id  : The access key id</li> <li>endpoint_url  : The endpoint url (if any)</li> </ul>"},{"location":"api/python/models/#awscredentials","title":"AWSCredentials","text":"<pre><code>AWSCredentials()\n</code></pre> <p>AWS integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>role_arn  : The ARN of the role that should be assumed via STS</li> </ul>"},{"location":"api/python/models/#awseventbridgeretraintrigger","title":"AWSEventBridgeRetrainTrigger","text":"<pre><code>AWSEventBridgeRetrainTrigger()\n</code></pre> <p>Base model to define an AWS EventBridge retrain trigger</p> <p>Fields: type credentials_id aws_region_name event_bus_name</p>"},{"location":"api/python/models/#apikey","title":"ApiKey","text":"<pre><code>ApiKey()\n</code></pre> <p>base model for api key</p> <p>Attributes</p> <ul> <li>api_key  : str</li> </ul>"},{"location":"api/python/models/#azureblobdatasource","title":"AzureBlobDataSource","text":"<pre><code>AzureBlobDataSource()\n</code></pre> <p>A source that identifies a blob in Azure Storage.</p> <p>Attributes</p> <ul> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#azurecredentials","title":"AzureCredentials","text":"<pre><code>AzureCredentials()\n</code></pre> <p>Azure integration credentials.</p> <p>Attributes</p> <ul> <li>app_id  : The id of the service principal</li> </ul>"},{"location":"api/python/models/#azureeventgridretraintrigger","title":"AzureEventGridRetrainTrigger","text":"<pre><code>AzureEventGridRetrainTrigger()\n</code></pre> <p>Base model to define an Azure EventGrid retrain trigger</p> <p>Fields: type credentials_id topic_endpoint</p>"},{"location":"api/python/models/#binaryclassificationtaskcostinfo","title":"BinaryClassificationTaskCostInfo","text":"<pre><code>BinaryClassificationTaskCostInfo()\n</code></pre> <p>Binary classification cost info is expressed in two terms: - cost of false positive - cost of false negative</p>"},{"location":"api/python/models/#columninfo","title":"ColumnInfo","text":"<pre><code>ColumnInfo()\n</code></pre> <p>Column base model</p> <p>Attributes</p> <ul> <li>name  : str</li> <li>role  : ColumnRole</li> <li>is_nullable  : bool</li> <li>data_type  : DataType</li> <li>predicted_target  : Optional[str] = None</li> <li>possible_values  : Optional[list[str | int | bool]] = None</li> <li>model_id  : Optional[str] = None</li> <li>dims  : Optional[tuple[int]] = None     it is mandatory when data_type is Array</li> <li>classes_names  : Optional[list[str]] = None     it is mandatory when the column is the target     in multilabel classification tasks</li> <li>subrole  : Optional[ColumnSubRole] = None     Indicates the subrole of the column. It's used in     RAG tasks to define the role of the input columns     (e.g. user input or retrieved context)</li> <li>image_mode  : Optional[ImageMode] = None     Indicates the mode of the image. It must be provided     when the data type is an image</li> </ul>"},{"location":"api/python/models/#company","title":"Company","text":"<pre><code>Company()\n</code></pre> <p>Company model</p> <p>Attributes</p> <ul> <li>company_id  : str</li> <li>name  : str</li> <li>address  : str</li> <li>vat  : str</li> </ul>"},{"location":"api/python/models/#companyuser","title":"CompanyUser","text":"<pre><code>CompanyUser()\n</code></pre> <p>base model for company user</p> <p>Attributes</p> <ul> <li>user_id  : str</li> <li>company_role  : UserCompanyRole</li> </ul>"},{"location":"api/python/models/#data","title":"Data","text":"<pre><code>Data()\n</code></pre> <p>Generic data model that contains all information about a data</p> <p>Attributes</p> <ul> <li>data_structure  : DataStructure</li> <li>source  : DataSource</li> </ul>"},{"location":"api/python/models/#dataschema","title":"DataSchema","text":"<pre><code>DataSchema()\n</code></pre> <p>Data schema base model</p> <p>Attributes</p> <ul> <li>columns  : List[ColumnInfo]</li> </ul>"},{"location":"api/python/models/#datasource","title":"DataSource","text":"<pre><code>DataSource()\n</code></pre> <p>Generic data source.</p>"},{"location":"api/python/models/#detectionevent","title":"DetectionEvent","text":"<pre><code>DetectionEvent()\n</code></pre> <p>An event created during the detection process.</p> <p>Attributes</p> <ul> <li>event_id  : str</li> <li>event_type  : DetectionEventType</li> <li>monitoring_target  : MonitoringTarget</li> <li>severity_type  : Optional[DetectionEventSeverity]</li> <li>insert_datetime  : str</li> <li>sample_timestamp  : float</li> <li>model_id  : Optional[str]</li> <li>model_name  : Optional[str]</li> <li>model_version  : Optional[str]</li> <li>user_feedback  : Optional[bool]</li> </ul>"},{"location":"api/python/models/#detectioneventaction","title":"DetectionEventAction","text":"<pre><code>DetectionEventAction()\n</code></pre> <p>Generic action that can be performed</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType</li> </ul>"},{"location":"api/python/models/#detectioneventrule","title":"DetectionEventRule","text":"<pre><code>DetectionEventRule(\n   **kwargs\n)\n</code></pre> <p>A rule that can be triggered by a detection event, and executes a series of actions.</p> <p>Attributes</p> <ul> <li>rule_id  : str</li> <li>name  : str</li> <li>task_id  : str</li> <li>model_name  : Optional[str]</li> <li>severity  : DetectionEventSeverity</li> <li>detection_event_type  : DetectionEventType</li> <li>monitoring_target  : MonitoringTarget</li> <li>actions  : List[DetectionEventAction]</li> </ul>"},{"location":"api/python/models/#discordnotificationaction","title":"DiscordNotificationAction","text":"<pre><code>DiscordNotificationAction()\n</code></pre> <p>Action that sends a notification to a Discord server through a webhook that you configure</p> <p>Attributes</p> <ul> <li>webhook  : str type = DetectionEventActionType.DISCORD_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#emailnotificationaction","title":"EmailNotificationAction","text":"<pre><code>EmailNotificationAction()\n</code></pre> <p>Base Model for Email Notification Action</p> <p>Attributes</p> <ul> <li>address  : str type = DetectionEventActionType.EMAIL_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#embeddingdata","title":"EmbeddingData","text":"<pre><code>EmbeddingData()\n</code></pre> <p>Embedding data model i.e., a data that can be represented via DataFrame and is stored in formats like: csv, parquet, json. There is only one input that has type array_1</p>"},{"location":"api/python/models/#gcpcredentials","title":"GCPCredentials","text":"<pre><code>GCPCredentials()\n</code></pre> <p>GCP integration credentials.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>gcp_project_id  : The id of the project on GCP</li> <li>client_email  : The email that identifies the service account</li> <li>client_id  : The client id</li> </ul>"},{"location":"api/python/models/#gcppubsubretraintrigger","title":"GCPPubSubRetrainTrigger","text":"<pre><code>GCPPubSubRetrainTrigger()\n</code></pre> <p>Base model to define a GCP PubSub retrain trigger</p> <p>Fields: type credentials_id topic_name</p>"},{"location":"api/python/models/#gcsdatasource","title":"GCSDataSource","text":"<pre><code>GCSDataSource()\n</code></pre> <p>A source that identifies a file in a GCS bucket.</p> <p>Attributes</p> <ul> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_1","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type_1","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#imagedata","title":"ImageData","text":"<pre><code>ImageData()\n</code></pre> <p>Image data model i.e., images, text or other. Since it is composed of multiple files, it needs a mapping between customer ids and those files</p> <p>Attributes</p> <ul> <li>mapping_source  : DataSource</li> </ul>"},{"location":"api/python/models/#integrationcredentials","title":"IntegrationCredentials","text":"<pre><code>IntegrationCredentials()\n</code></pre> <p>Credentials to authenticate to a 3<sup>rd</sup> party service provider via an integration.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> </ul>"},{"location":"api/python/models/#job","title":"Job","text":"<pre><code>Job()\n</code></pre> <p>Job information item model</p> <p>Attributes</p> <ul> <li>job_id  : str</li> <li>job_group  : str</li> <li>project_id  : str</li> <li>project_name  : str</li> <li>task_id  : str</li> <li>task_name  : str</li> <li>model_id  : Optional[str]</li> <li>model_name  : Optional[str]</li> <li>status  : str</li> <li>error  : Optional[str]</li> </ul>"},{"location":"api/python/models/#kpi","title":"KPI","text":"<pre><code>KPI()\n</code></pre> <p>KPI base model</p> <p>Attributes</p> <ul> <li>kpi_id  : str</li> <li>name  : str</li> <li>status  : ModelStatus</li> <li>status_kpi_start_timestamp  : Optional[datetime]</li> <li>status_insert_datetime  : datetime</li> </ul>"},{"location":"api/python/models/#localdatasource","title":"LocalDataSource","text":"<pre><code>LocalDataSource()\n</code></pre> <p>Use this data source if you want to upload a file from your local disk to the ML cube platform cloud.</p> <p>Attributes</p> <ul> <li>file_path  : str</li> </ul>"},{"location":"api/python/models/#model","title":"Model","text":"<pre><code>Model()\n</code></pre> <p>Base model to define model item</p> <p>Attributes</p> <ul> <li>model_id  : str</li> <li>task_id  : str</li> <li>name  : str</li> <li>version  : str</li> <li>metric_name  : performance or error metric associated with     the model</li> <li>creation_datetime  : Optional[datetime]</li> <li>retrain_trigger  : Optional[RetrainTrigger]</li> </ul>"},{"location":"api/python/models/#monitoringquantitystatus","title":"MonitoringQuantityStatus","text":"<pre><code>MonitoringQuantityStatus()\n</code></pre> <p>Base model to store the monitoring status of a monitoring quantity (target or metric)</p>"},{"location":"api/python/models/#mqttnotificationaction","title":"MqttNotificationAction","text":"<pre><code>MqttNotificationAction()\n</code></pre> <p>Base Model for Mqtt Notification Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.MQTT_NOTIFICATION</li> <li>connection_string  : str</li> <li>topic  : str</li> <li>payload  : str</li> </ul>"},{"location":"api/python/models/#multiclassclassificationtaskcostinfo","title":"MulticlassClassificationTaskCostInfo","text":"<pre><code>MulticlassClassificationTaskCostInfo()\n</code></pre> <p>Multiclass classification cost info is expressed in terms of the misclassification costs for each class</p>"},{"location":"api/python/models/#multilabelclassificationtaskcostinfo","title":"MultilabelClassificationTaskCostInfo","text":"<pre><code>MultilabelClassificationTaskCostInfo()\n</code></pre> <p>Multilabel classification cost info is expressed in terms of false positive and false negative costs for each class</p>"},{"location":"api/python/models/#project","title":"Project","text":"<pre><code>Project()\n</code></pre> <p>Project model</p> <p>Attributes</p> <ul> <li>project_id  : str</li> <li>name  : str</li> </ul>"},{"location":"api/python/models/#regressiontaskcostinfo","title":"RegressionTaskCostInfo","text":"<pre><code>RegressionTaskCostInfo()\n</code></pre> <p>Regression cost info is expressed in two terms: - cost due to overestimation - cost due to underestimation</p> <p>Fields: currency overestimation_cost underestimation_cost</p>"},{"location":"api/python/models/#remotedatasource","title":"RemoteDataSource","text":"<pre><code>RemoteDataSource()\n</code></pre> <p>A source that identifies where data is stored.</p> <p>Attributes</p> <ul> <li>credentials_id  : The id of the credentials to use to authenticate to the remote data source. If None, the default will be used</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_2","title":".get_path","text":"<pre><code>.get_path()\n</code></pre> <p>Return the path of the object</p>"},{"location":"api/python/models/#get_source_type_2","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre> <p>Returns raw data source type</p>"},{"location":"api/python/models/#resampleddatasetsuggestion","title":"ResampledDatasetSuggestion","text":"<pre><code>ResampledDatasetSuggestion()\n</code></pre> <p>ResampledDatasetSuggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> <li>sample_ids  : List[str]</li> <li>sample_counts  : List[int]</li> </ul>"},{"location":"api/python/models/#retrainaction","title":"RetrainAction","text":"<pre><code>RetrainAction()\n</code></pre> <p>Base Model for Retrain Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.RETRAIN</li> <li>model_name  : str</li> </ul>"},{"location":"api/python/models/#retraintrigger","title":"RetrainTrigger","text":"<pre><code>RetrainTrigger()\n</code></pre> <p>Base model to define a retrain trigger</p> <p>Fields: type credentials_id</p>"},{"location":"api/python/models/#retrainingreport","title":"RetrainingReport","text":"<pre><code>RetrainingReport()\n</code></pre> <p>base model for Retraining Report</p> <p>Attributes</p> <ul> <li>report_id  : str</li> <li>suggestion  : Suggestion</li> <li>effective_sample_size  : float</li> <li>model_metric_name  : str</li> <li>performance_upper_bound  : float</li> <li>performance_lower_bound  : float</li> <li>cost_upper_bound  : float</li> <li>cost_lower_bound  : float</li> </ul>"},{"location":"api/python/models/#s3datasource","title":"S3DataSource","text":"<pre><code>S3DataSource()\n</code></pre> <p>A source that identifies a file in an S3 bucket.</p> <p>Attributes</p> <ul> <li>object_path  : str</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#get_path_3","title":".get_path","text":"<pre><code>.get_path()\n</code></pre>"},{"location":"api/python/models/#get_source_type_3","title":".get_source_type","text":"<pre><code>.get_source_type()\n</code></pre>"},{"location":"api/python/models/#sampleweightssuggestion","title":"SampleWeightsSuggestion","text":"<pre><code>SampleWeightsSuggestion()\n</code></pre> <p>SampleWeightsSuggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> <li>sample_ids  : List[str]</li> <li>sample_weights  : List[float]</li> </ul>"},{"location":"api/python/models/#secretawscredentials","title":"SecretAWSCredentials","text":"<pre><code>SecretAWSCredentials()\n</code></pre> <p>AWS integration credentials, that also include the external_id you need to set up the trust policy on AWS.</p> <p>Attributes</p> <ul> <li>credentials_id  : str</li> <li>name  : str</li> <li>default  : bool</li> <li>type  : ExternalIntegration</li> <li>role_arn  : The ARN of the IAM role that should be assumed</li> <li>external_id  : Secret key used to assume the IAM role via STS</li> </ul> <p>Methods:</p>"},{"location":"api/python/models/#generate_trust_policy","title":".generate_trust_policy","text":"<pre><code>.generate_trust_policy()\n</code></pre> <p>Generates a JSON trust policy that you can copy into the IAM role on AWS.</p>"},{"location":"api/python/models/#slacknotificationaction","title":"SlackNotificationAction","text":"<pre><code>SlackNotificationAction()\n</code></pre> <p>Action that sends a notification to a Slack channel through a webhook that you configure.</p> <p>Attributes</p> <ul> <li>webhook  : str</li> <li>channel  : str type = DetectionEventActionType.SLACK_NOTIFICATION</li> </ul>"},{"location":"api/python/models/#subscriptionplaninfo","title":"SubscriptionPlanInfo","text":"<pre><code>SubscriptionPlanInfo()\n</code></pre> <p>Data model for a subscription plan Permission limit set to None means no limit is set Expiration date set to None means no expiration is set Product key data are set only if a product key is associated to the subscription plan</p> <p>Attributes</p> <ul> <li>subscription_id  : str</li> <li>type  : SubscriptionType</li> <li>max_tasks  : int | None</li> <li>max_users  : int | None</li> <li>monitoring  : bool</li> <li>explainability  : bool</li> <li>retraining  : bool</li> <li>is_active  : bool</li> <li>start_date  : date</li> <li>expiration_date  : date | None</li> <li>product_key  : str | None</li> <li>product_key_status  : ProductKeyStatus | None</li> </ul>"},{"location":"api/python/models/#suggestion","title":"Suggestion","text":"<pre><code>Suggestion()\n</code></pre> <p>Suggestion base model</p> <p>Attributes</p> <ul> <li>suggestion_id  : str</li> <li>suggestion_type  : SuggestionType</li> </ul>"},{"location":"api/python/models/#suggestioninfo","title":"SuggestionInfo","text":"<pre><code>SuggestionInfo()\n</code></pre> <p>SuggestionInfo base model</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>executed  : bool</li> <li>timestamp  : float</li> </ul>"},{"location":"api/python/models/#tabulardata","title":"TabularData","text":"<pre><code>TabularData()\n</code></pre> <p>Tabular data model i.e., a data that can be represented via DataFrame and is stored in formats like: csv, parquet, json</p>"},{"location":"api/python/models/#task","title":"Task","text":"<pre><code>Task()\n</code></pre> <p>Task model</p> <p>Attributes</p> <ul> <li>task_id  : str</li> <li>name  : str</li> <li>type  : TaskType</li> </ul>"},{"location":"api/python/models/#taskcostinfo","title":"TaskCostInfo","text":"<pre><code>TaskCostInfo()\n</code></pre> <p>Base class for task cost info. It depends on TaskType because classification is different from regression in terms of business costs due to errors</p>"},{"location":"api/python/models/#taskragevalreportitem","title":"TaskRagEvalReportItem","text":"<pre><code>TaskRagEvalReportItem()\n</code></pre> <p>base model for Rag Evaluation Report</p> <p>Attributes</p> <ul> <li>id  : str</li> <li>creation_datetime  : datetime</li> <li>name  : str</li> <li>status  : JobStatus</li> <li>from_datetime  : datetime</li> <li>to_datetime  : datetime</li> </ul>"},{"location":"api/python/models/#tasktopicmodelingreportdetails","title":"TaskTopicModelingReportDetails","text":"<pre><code>TaskTopicModelingReportDetails()\n</code></pre> <p>Task Topic Modeling Report Details base model</p>"},{"location":"api/python/models/#tasktopicmodelingreportitem","title":"TaskTopicModelingReportItem","text":"<pre><code>TaskTopicModelingReportItem()\n</code></pre> <p>Task Topic Modeling Report Item base model</p>"},{"location":"api/python/models/#teamsnotificationaction","title":"TeamsNotificationAction","text":"<pre><code>TeamsNotificationAction()\n</code></pre> <p>Base Model for Teams Notification Action</p> <p>Attributes</p> <ul> <li>type  : DetectionEventActionType.TEAMS_NOTIFICATION</li> <li>webhook  : str</li> </ul>"},{"location":"api/python/models/#textdata","title":"TextData","text":"<pre><code>TextData()\n</code></pre> <p>Text data model for nlp tasks.</p>"},{"location":"api/rest/","title":"REST API","text":"<p>Page under construction</p>"},{"location":"user_guide/","title":"User Guide","text":"<p>ML cube Platform is built following the API first principle, by which it can be used both via Web Application and REST API. In this first guide, we explain the basic concepts in ML cube Platform. In order to find easily every information we suggest you to use the search bar, moreover, you can go to Glossary to find the key concepts and definition we use along this site.</p> <p>Example Company</p> <p>Every time you find a section like this you will see a examples with a fictional company that will help you to better understand the concepts and the entities in ML cube Platform. We will use a fictional company called Delta Energy that is a producer of Photovoltaic Modules that own Photovoltaic fields and trades the produced energy to the market.</p>"},{"location":"user_guide/#company","title":"Company","text":"<p>To use ML cube Platform you need to belong to a Company that is created during your first login in the Web Application. Everything will be done inside it, for example, if you want your colleagues to work with you in ML cube Platform you need to create new Users inside the company. Users has assigned a specific Role that defines their privileges and what they are able to do (refer to User Roles for more information).</p>"},{"location":"user_guide/#project","title":"Project","text":"<p>Your work on ML cube Platform is organized through Projects, Task and Models. A Project is an artificial intelligence application that uses a set of algorithms to reach a common set of KPIs, usually a Project contains several Tasks.</p> <p>Delta Energy inc</p> <p>Delta Energy, created the Energy Revenue Project to enhance their revenue from energy trading. They invested in four AI algorithms:</p> <ul> <li>Fault detection</li> <li>Fault diagnosis</li> <li>Soiling detection</li> <li>Trading</li> </ul> <p>In ML cube Platform, these four algorithms define four Tasks inside the Project Energy Revenue. They have been placed into the same Project because they share the business goal i.e., the net revenue, the data and are interdependent.</p>"},{"location":"user_guide/#task","title":"Task","text":"<p>As you may have guessed, in ML cube Platform a Task corresponds to an AI Algorithm. To be more precise, a Task is an AI Problem with a dataset composed of input features and a target. A Task can have more than one AI Model that uses the input features estimates the target.</p> <p>Delta Energy inc</p> <p>In out example company, the Fault Detection Task has as input features the PV modules and weather data and the target is the presence of a fault. The Task has two Models: logistic regression and random forest. Both models use the same data and predict the same target but are different in the techniques used to perform the estimation.</p> <p>A Task is specified by several attributes, the most important are:</p> <ul> <li><code>type</code>: regression, classification, object detection ...</li> <li><code>data structure</code>: tabular data, image data, ...</li> <li><code>optional target</code>: if the target is not always available. This happens when input samples are labeled and the most part of production data do not have a label</li> <li><code>data schema</code>: specifies the inputs and the target of the task, see Data Schema section for more details</li> <li><code>cost info</code>: information about the economic costs of the error on the target</li> </ul> <p>Delta Energy inc</p> <p>The Task of Fault Detection has clear costs due to false positives or false negatives. Every time a fault is not detected the false negative cost is proportional to the power reduction. While, false positives determine costs for the maintenance team that will go on the field for nothing.</p>"},{"location":"user_guide/#model","title":"Model","text":"<p>The last entity is the Model that is the actual AI model that predicts the target. A Model has a Version that defines the training data used to train it. All model's data will be uploaded specifying the model version in order to track each prediction with the right model instance. The model version is updated whenever a new training of the model is done.</p> <p>A Model has the following attributes:</p> <ul> <li><code>name</code>: uniquely identifies the model inside the Task</li> <li><code>version</code>: specifies different trained instance of the model</li> <li><code>metric name</code>: metric to use to measure the model performance</li> <li><code>suggestion type</code>: type of retraining action to use for that model, see Retraining section for more details</li> <li><code>retraining cost</code>: how much model training cost. This information is used by the Business module</li> </ul> <p>Note</p> <p>It's worth nothing to note that in ML cube Platform you do not actually need to upload the model on the application. We just need to know its training data and its predictions for the production data. In this way, ML cube Platform is considered as model agnostic.</p> <p>Now that you have clear the basic concepts we invite you to explore the other ML cube Platform pages:</p> <ul> <li> Modules</li> </ul> <p>Explore available features in ML cube Platform</p> <p> More info</p> <ul> <li> Integrations</li> </ul> <p>We are part of MLOps ecosystem and natively integrated with other solutions.</p> <p> More info</p> <ul> <li> Automation rules</li> </ul> <p>Discover how to setup automation rules to increase your reactivity.</p> <pre><code>[:octicons-arrow-right-24: More info](monitoring/detection_event_rules.md)\n</code></pre> <ul> <li> Roles and access</li> </ul> <p>Get more info about RBAC inside ML cube Platform</p> <p> More info</p>"},{"location":"user_guide/company/","title":"Company and Subscriptions","text":"<p>A Company is the fundamental organizational entity inside ML cube Platform. Users belong to a single Company and billing, licenses, and projects are all managed and created inside a Company.</p> <p>The Company has an Owner with maximum privileges including billing administration. The owner can create new User accounts inside the Company assigning specific roles at the company and project level.</p> <p>The information required that describe the Company are:</p> <ul> <li>Company name</li> <li>Address</li> <li>VAT</li> </ul> <p>The Company is created by ML cube team during the onboarding, then, during first login the Owner needs to complete the remaining information.</p>"},{"location":"user_guide/company/#subscriptions","title":"Subscriptions","text":"<p>A Subscription is the payment arrangement where a Company pays a periodical fee to access ML cube Platform services. A Subscription has a start and an expiration date and contains the modules and quotas the Company can handle.</p> QuotasModules Quota Description Users Maximum number of Users per Company Tasks Maximum number of Tasks per Company excluding those in demo projects Modules Description Monitoring Data drift monitoring and detection for several targets and metrics. Alerts are raised when drifts are detected allowing automated the response. Retraining Generation of retraining datasets to update AI models according to identified data drifts. Dataset is based on data distributions and leverages all the past available data. Explainability Explainability of detected drifts to better understand what happened and how to tackle it. <p>Moreover, there are two types of subscriptions depending on where ML cube Platform is used:</p> <ul> <li> <p> Cloud </p> <p>Standard SaaS plan to use ML cube Platform hosted on ML cube cloud infrastructure. With Cloud subscription the users can use Web Application and SDK to interact with ML cube Platform and to use its services. Data can be stored on ML cube Private Cloud Storage.</p> </li> <li> <p> Edge </p> <p>Edge subscriptions are used, as the name suggests, for edge devices that hosts ML cube Platform Edge. A common use case is an industrial machinery computer that runs AI algorithms. Edge subscriptions are validated via Product Key and are uniquely linked to a specific edge device.</p> </li> </ul>"},{"location":"user_guide/data/","title":"Data","text":"<p>To use ML cube Platform, you need to upload the data that your artificial intelligence algorithms deal with. They are represented by the Data logical entity, which is a complex object composed of many different elementary units.</p> <p>According to the context, the Data object has some required entities to be considered valid: for instance, training data require both input and target, while production data are still valid with only inputs.</p> <p>Each elementary unit in the Data object belongs to a Data Category, which represents an consistent group of data. Available categories are:</p> Data Category Description Input The set of input data like features in Tabular Task or the Image in Object Detection Task. Target The ground truth data representing the actual class in a Classification Task or the value to predict in Regression. Prediction The AI model's prediction using the Input data."},{"location":"user_guide/data/#data-category-and-data-schema-columns-role","title":"Data Category and Data Schema Column's Role","text":"<p>The Data Schema created for the Task contains a list of Column objects, each of which has a Role. Naturally, there is a relationship between the Column's Role and the Data Category. In fact, each Data Category comprises a set of Column objects with certain Roles. When you upload samples belonging to a Data Category, they must contain all the Columns objects declared on the Data Schema to be considered valid.</p> <p>The following table shows these relationships:</p> Data Category ID Time ID Input Input additional embedding Target Target additional embedding Prediction Prediction additional embedding Input Target Prediction <p>Note</p> <p>As you can see Column's Roles ID and Time Id are always required because are used by ML cube Platform to correctly link data units together.</p> <p>Example</p> <p>Consider a regression task with three inputs variables \\(X_0, X_1\\) and \\(X_2\\). In the Data Schema they are represented by three Column objects with Input Role. When uploading Input Data Category, the csv file must contain the three Input Columns, the Time ID and the ID like: <pre><code>sample_id,time_id,x_0,x_1,x_2\nabc123,1234,3.5,100,23.0\n</code></pre></p>"},{"location":"user_guide/data/#data-batch","title":"Data Batch","text":"<p>ML cube Platform supports batch-based data processing, which means that data samples are uploaded together as a batch. Therefore, the Data object described above must contain, for each Data Category, the same set of samples to form a consistent batch. The only exception is for Target data that, given the fact that they may be obtained through labeling, can be present partially.</p> <p>Warning</p> <p>Even if Target data can be uploaded partially, they must be a subset of samples of Input or Prediction.</p> <p>In any case, the presence of a Data Category in a Data Batch depends on the context in which it is uploaded. In the next section we explain what is meant by the Historical, Reference and Production data.</p> <p>Info</p> <p>You can see your uploaded data batches in its own page. With information about the origin of data, the data categories it contains and what monitoring has been done.</p> <p> </p> Examples of two valid and two invalid data batches."},{"location":"user_guide/data/#data-contexts","title":"Data contexts","text":"<p>Data samples have different meanings depending on how you have used them to train, validate, or infer from your artificial intelligence model. Some data may be too old and, therefore, not used, others may belong to a training set, and, lastly, the newest one comes directly from production.</p> <p>To handle all these scenarios, the ML cube Platform allows the upload of data specifying how they were used. Indeed, the operations of the ML cube Platform will be different to each context.</p> <p>The main difference in the data usage is the split between the ones present before the model was in production and the others. For that reason, the ML cube Platform defines two data contexts: historical and production.</p> <p>Note</p> <p>Note that when an artificial intelligence model is retrained with new data, the previous contexts change and the old production data become historical ones for the new model version.</p> <p> </p> Different data batches and they data context."},{"location":"user_guide/data/#historical","title":"Historical","text":"<p>Historical data are all data samples that existed before the model was deployed in production. They include the training, validation and test sets that were used during model experimentation and training, or older data that were discarded.</p> <p>Historical data can be uploaded in ML cube Platform anytime through different data batches.</p> <p>Data categories that a batch of historical data contains depend on the Task Type, here is the summary:</p> <ul> <li>Supervised tasks like regression, classification and object detection requires Input and Target</li> <li>Retrieval Augmented Generation task requires Inputs and Predictions</li> </ul> <p>The reason why it is not possible to upload the model's predictions in historical data of supervised tasks is that they may contain the training bias of the model. On the other hand, in RAG tasks there is no target or training dataset.</p> SDK Example <p>You can upload historical data as follow:</p> <pre><code>job_id = client.add_historical_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=target_data,\n)\n</code></pre>"},{"location":"user_guide/data/#reference","title":"Reference","text":"<p>As mentioned before, to historical data belong to both training and old data, but it is important distinguish them. The Monitoring module requires the definition of a Reference dataset representing training, validation and test data. The reference data is initially loaded as historical data and then marked as reference data by providing the timestamp interval.</p> <p>It is worth mentioning that also RAG Tasks can have reference data even if they do not have a proper training dataset. Indeed, reference data are used by the Monitoring module to initialize drift detection algorithms, therefore, the reference data definition is a mandatory step to enable this feature. For RAG Tasks, reference data can be used to indicate the type of data expected in production.</p> <p>Warning</p> <p>There is only one limitation to upload historical data: the production boundary. Model data reference defines a rigid boundary separating historical and production data. This boundary is defined as the last sample timestamp of the reference data. After the model data reference is defined, it is no longer possible to upload historical data that are more recent than that date. Indeed, data with sample timestamps higher than the production boundary are considered production data and no longer historical.</p> SDK Example <p>You can set reference data as follow:</p> <pre><code>job_id = client.set_model_reference(\n    model_id=model_id,\n    from_timestamp=from_timestamp,\n    to_timestamp=to_timestamp,\n)\n</code></pre>"},{"location":"user_guide/data/#production","title":"Production","text":"<p>After the artificial intelligence model is deployed in a production environment, incoming data belongs to the Production context. They have different characteristics with respect to historical data because they are online.</p> <p>While the assumption for historical data is that they belong to the past and are complete (with both input and target as an example), production data can be partial or different Data Categories are available at different times. For that reason, they can be uploaded asynchronously at different moments in time.</p> <p>However, there are some constraints about the Production data upload. Since data samples are grouped in Data Batches they must be consistent in all the Data Categories. ML cube Platform allows to upload each category separately at different moments but the uploads must contains always the samples in the Data Batch.</p> <p>Info</p> <p>After data are uploaded to ML cube Platform their ID are analyzed to determine if they are already present or not. If not then a new Data Batch is created containing the set of samples ID just uploaded. If the samples are already present in ML cube Platform they belong to a existing Data Batch and the Data Category is attached to it.</p> <p>Let's do an example with a Regression Task about forecasting the weekly produced power from a solar plant. In this case, inputs come from electrical and meteorological measurements from the past week and are immediately available; prediction is available as soon as the model makes an inference and is uploaded from the model inference service. Instead, target data are only available at the end of the week.</p> <ul> <li>Input samples are the first uploaded Data Category and they define the samples that belong to this Data Batch.</li> <li>Predictions are uploaded after the model inference is completed, in order to be valid they must match entirely the input samples already uploaded.</li> <li>Target data are uploaded at the end of the week and, as described before they can be a subset of already uploaded input samples.</li> </ul> SDK Example <p>You can upload production data in one method call:</p> <pre><code>job_id = client.add_production_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=target_data,\n    predictions=[\n        (\n            model_id,\n            prediction_data,\n        )\n    ],\n)\n</code></pre> <p>or in different times:</p> <pre><code>job_id = client.add_production_data(\n    task_id=task_id,\n    inputs=inputs_data,\n    target=None,\n    predictions=None\n)\n# ...\njob_id = client.add_production_data(\n    task_id=task_id,\n    inputs=None,\n    target=target_data,\n    predictions=[\n        (\n            model_id,\n            prediction_data,\n        )\n    ],\n)\n</code></pre>"},{"location":"user_guide/data/#post-labeling","title":"Post labeling","text":"<p>In some use cases, target data are obtained only by human labeling, and since this is an expensive procedure, not all available samples are labelled but usually only a selected subset. Moreover, it is possible that labeling is done occasionally for samples that belong to different data batches uploaded in ML cube Platform.</p> <p>Obviously, this scenario does not belong either to historical or production contexts and it has its own specificity. Hence, ML cube Platform provides another way to upload newly labelled samples in a single moment. In this case, there are no constraints about data batches or production boundary but you are allowed to upload all data freely. ML cube Platform will take care of assigning the new target to the right Data Batch.</p> <p>Warning</p> <p>The only constraint is that you cannot upload the label for a sample that already has target uploaded.</p> <p> </p> Labelled target data distributed in their data batches. SDK Example <p>You can upload labelled target samples via the add_target_data method:</p> <pre><code>job_id = client.add_target_data(\n    task_id=task_id,\n    target=target_data,\n)\n</code></pre>"},{"location":"user_guide/data/#data-sources","title":"Data sources","text":"<p>The previous sections of this page described what a Data object is, how it is represented by Data Batch and its uploading contexts. To have more information about how to actually upload data we suggest to read the page Data Sources.</p>"},{"location":"user_guide/data_schema/","title":"Data Schema","text":"<p>Data schema contains all the information about the data in the Task, it is created at the beginning and is immutable.</p> <p>Tip</p> <p>Data Schema can be easily created starting from a template from the Web App. Go in Data Schema page after you created a Task and see the precompiled version of Data Schema, update and insert new Columns to create your custom version.</p> <p>A Data schema is composed of a list of objects named Column that represent each data entity in the Task. The number and type of Column objects depend on the task type and task data structure.</p> <p>A Column object has some mandatory attributes and others that depends on its role or data type:</p> Attribute Description Mandatory Name Name of the entity used to read it from raw data. For instance, in Tabular tasks, it represents the name of the column of the CSV file. Mandatory Data type Data format of the entity. Possible values are <ul><li>Float: numeric value</li><li>Categorical: entity that can assume a only specified values. A Categorical Column requires the attribute possible_values to be specified.</li><li>String: generic textual data like text input or customer id. To not be used for categorical columns.</li><li>Array 1: one-dimensional array. Requires dims attribute to be defined like a list of 1 element [n] that specifies the number of elements of the array.</li><li>Array 2: two-dimensional array. Requires dims attribute to be defined like a list of 2 elements [n, m] that specifies the number elements of the each dimension of the array.</li><li>Array 3: three-dimensional array. Requires dims attribute to be defined like a list of 3 elements [n, m, k] that specifies the number elements of the each dimension of the array.</li></ul> Mandatory Role Defines the role the Column object has in the Task. According to the Task type some roles are required or not allowed. More information in the following sections. Mandatory Subrole Additional specification of the role in the Task. Some entities belong to the same Role but have different meanings, the Subroles allows to distinguish between them. More information in the following sections. Depends on Task Type Is Nullable If the entity allows missing values. Mandatory Dims List with the number of elements each dimension of the array has. The value -1 indicates that that dimension can have an arbitrary number of elements. Required when Data Type is Array Possible values List of values the categorical variable can assume. They can be either strings or numbers. When Task Type is Classification Multilabel and Role is Target, possibile values must be [0, 1] indicating the presence or not of that class. Mandatory when Column Data Type is Categorical Classes Names Names of the classes in the Task. The length of this list must match the length of the Dims of the array. Required when Column Role is Target and Task Type is Classification Multilabel. Image Mode Type of image, it can be RGB, RGBA, GRAYSCALE. It also determines the Data Type, which is Array 3 for RBG and RGBA and Array 2 for GRAYSCALE. Required when Column Role is Input and Data Structure is Image."},{"location":"user_guide/data_schema/#role","title":"Role","text":"<p>The Role defines what the Column object represents for the Task. Roles are used by ML cube Platform to correctly use provided data. Some Roles are needed to uniquely identify a sample, other to retrieve the correct information. Moreover, some Roles must be inserted by you when creating the Data Schema the first time, while others, like the model predictions, are created automatically by ML cube Platform.</p> <p>User defined roles are:</p> Role Data Type Description Mandatory ID String Unique identifier of the sample. It is used during data validation to avoid duplicates of data and to communicate information about data with you without sending the actual data It must be always present when sending data to ML cube Platform. Time ID Float Timestamp of the sample expressed in seconds (for that reason it is a Float). It is used to temporally order samples maintaining coherence in the analysis of ML cube Platform. It must be always present when sending data to ML cube Platform. Input Any available Data Type Represents input data like a single feature for Tabular tasks or image in Image tasks or text in Text tasks According to Task Type the number of Input Column object varies from 1 to illimitate. See Section Data schema templates Target Any available Data Type. It must be coherent with Task Type Represents the true value of the sample in supervised tasks. It is mandatory for supervised tasks. Input additional embedding Array 1 Embedding vector of the Input Column. It is allowed only then Data Structure of Task is Image or Text. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm. It is optional since it depends on your choice to share with ML cube Platform this type of data. Target additional embedding Array 1 Embedding vector of the Target Column. It is allowed only then Task Type is RAG. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm. It is optional since it depends on your choice to share with ML cube Platform this type of data. <p>ML cube Platform defined roles are:</p> Role Data Type Description Prediction Same Data Type of Target Column Prediction Column object automatically created when the Task Model is created. The name has the fixed template: &lt;MODEL_NAME&gt;@&lt;MODEL_VERSION&gt; Prediction additional embedding Array 1 Embedding vector of the Prediction Column. It is allowed only then Task Type is RAG. When this Column object is present, ML cube Platform uses it as numerical representation of the data, otherwise, it uses an internal embedding algorithm."},{"location":"user_guide/data_schema/#subrole","title":"Subrole","text":"<p>Some tasks can have different data entities for the same Role, the Column object's attribute Subrole helps to specify the correct type of data.</p> Subrole Associated Role Data Type Description RAG User Input INPUT String In RAG Tasks it is the user query submitted to the system. RAG Retrieved Context INPUT String In RAG Tasks it is the retrieved contexts (separated with the Task attribute context separator) that the retrieval system has selected to answer the query. Model probability PREDICTION Depends on Task Type:<ul><li>RAG: Array 1</li><li>Classification Binary: Float</li><li>Classification Multiclass: Array 1</li><li>Classification Multilabel: Array 1</li></ul> It is automatically created by ML cube Platform when the created Model has the flag additional probabilistic output set as True. The name has fixed template: &lt;MODEL_NAME&gt;_probability@&lt;MODEL_VERSION&gt;. Object detection prediction label PREDICTION Array 1 It is automatically created when Task Type is Object detection. It is an array with length equal to the number of predicted bounding boxes where each element contains the class label assigned to the bounding box. The name has a fixed template: &lt;MODEL_NAME&gt;_predicted_labels@&lt;MODEL_VERSION&gt;. Object detection target label TARGET Array 1 It is mandatory when Task Type is Object detection. It is an array with length equal to the number of actual bounding boxes where each element contains the class label assigned to the bounding box."},{"location":"user_guide/data_schema/#data-schema-constraints","title":"Data schema constraints","text":"<p>Each combination of Task Type and Data Structure leads to different Data Schema requirements that must be satisfied when it is created for the Task. For instance, image binary classification tasks requires only one input column object with image data type and target column object must be categorical with only two possible values.</p> <p>Here the list of constraints about quantities for each Role:</p> TaskType DataStructure ID TIME_ID INPUT TARGET INPUT_ADDITIONAL_EMBEDDING TARGET_ADDITIONAL_EMBEDDING USER_INPUT RETRIEVED_CONTEXT OBJECT_DETECTION_LABEL_TARGET Regression Tabular 1 1 &gt;=1 1 0 0 0 0 0 Regression Embedding 1 1 1 1 0 0 0 0 0 Regression Image 1 1 1 1 &lt;= 1 0 0 0 0 Regression Text 1 1 1 1 &lt;= 1 0 0 0 0 Classification Binary Tabular 1 1 &gt;=1 1 0 0 0 0 0 Classification Binary Embedding 1 1 1 1 0 0 0 0 0 Classification Binary Image 1 1 1 1 &lt;= 1 0 0 0 0 Classification Binary Text 1 1 1 1 &lt;= 1 0 0 0 0 Classification Multiclass Tabular 1 1 &gt;=1 1 0 0 0 0 0 Classification Multiclass Embedding 1 1 1 1 0 0 0 0 0 Classification Multiclass Image 1 1 1 1 &lt;= 1 0 0 0 0 Classification Multiclass Text 1 1 1 1 &lt;= 1 0 0 0 0 Classification Multilabel Tabular 1 1 &gt;=1 1 0 0 0 0 0 Classification Multilabel Embedding 1 1 1 1 0 0 0 0 0 Classification Multilabel Image 1 1 1 1 &lt;= 1 0 0 0 0 Classification Multilabel Text 1 1 1 1 &lt;= 1 0 0 0 0 RAG Text 1 1 2 0 0 | 2 0 1 1 nan <p>Here the list of constraints about Data Types for each Role:</p> TaskType DataStructure ID TIME_ID INPUT TARGET INPUT_ADDITIONAL_EMBEDDING TARGET_ADDITIONAL_EMBEDDING USER_INPUT RETRIEVED_CONTEXT OBJECT_DETECTION_LABEL_TARGET Regression Tabular STRING FLOAT FLOAT, CATEGORY FLOAT - - - - - Regression Embedding STRING FLOAT ARRAY_1 FLOAT - - - - - Regression Image STRING FLOAT ARRAY_3 FLOAT ARRAY_1 - - - - Regression Text STRING FLOAT STRING FLOAT ARRAY_1 - - - - Classification Binary Tabular STRING FLOAT FLOAT, CATEGORY CATEGORY - - - - - Classification Binary Embedding STRING FLOAT ARRAY_1 CATEGORY - - - - - Classification Binary Image STRING FLOAT ARRAY_3 CATEGORY ARRAY_1 - - - - Classification Binary Text STRING FLOAT STRING CATEGORY ARRAY_1 - - - - Classification Multiclass Tabular STRING FLOAT FLOAT, CATEGORY CATEGORY - - - - - Classification Multiclass Embedding STRING FLOAT ARRAY_1 CATEGORY - - - - - Classification Multiclass Image STRING FLOAT ARRAY_3 CATEGORY ARRAY_1 - - - - Classification Multiclass Text STRING FLOAT STRING CATEGORY ARRAY_1 - - - - Classification Multilabel Tabular STRING FLOAT FLOAT, CATEGORY ARRAY_1 - - - - - Classification Multilabel Embedding STRING FLOAT ARRAY_1 ARRAY_1 - - - - - Classification Multilabel Image STRING FLOAT ARRAY_3 ARRAY_1 ARRAY_1 - - - - Classification Multilabel Text STRING FLOAT STRING ARRAY_1 ARRAY_1 - - - - RAG Text STRING FLOAT STRING - ARRAY_1 - STRING STRING - Object Detection Image STRING FLOAT ARRAY_3 ARRAY_2 ARRAY_1 - - - ARRAY_1"},{"location":"user_guide/glossary/","title":"Glossary","text":""},{"location":"user_guide/glossary/#general-terms","title":"General Terms","text":"<ul> <li>User: registered user with username and password that interact with ML cube Platform.  A User can create API keys that inherits his/her permissions and that the he/she uses to communicate with ML cube Platform though API. </li> <li>Company: collection of Users that work with ML cube Platform.  Subscription plan and contracts with ML cube are managed at Company level. A Company has one owner that has all the privileges and that can assign admin role to other Users in the Company.</li> <li>Project: collection of AI Tasks that belong to the same business domain.</li> <li>Task: it is the standard AI problem with a dataset, a target and a set of AI models that predicts the target. All AI models inside the AI Task predicts the same target quantity and they are considered as champion and challengers or deployed and shadow models. Data drift detection is done at Task level because the models uses the same dataset.</li> <li>Model: AI model inside a Task that makes predictions over the Task's dataset.</li> </ul>"},{"location":"user_guide/glossary/#data-terms","title":"Data Terms","text":"<ul> <li>Historical data: data not used for the newest retraining but that belong to the Task. ML cube Platform uses these data during the retraining dataset selection to exploit all the available information. They are not mandatory, if they are not present then the Retraining Tool selects data from the reference set and the production data.</li> <li>Reference data: dataset used as reference for the current model version. It can be the training dataset, the test set or both. Reference data represents the current view of the model over the Task</li> <li>Production data: data the model encounter during production, the production data are monitored by the ML cube Platform detectors to detect drifts</li> <li>Data schema: represents the schema of the data that ML cube Platform uses to know the features and the target columns.</li> </ul>"},{"location":"user_guide/glossary/#drifts-terms","title":"Drifts Terms","text":"<ul> <li>Input drift: statistically significant change in the input data P(X)</li> <li>Concept drift: statistically significant change in the input and target data P(X, y)</li> <li>Model drift: statistically significant change in the model error P(y \u2013 y_pred).</li> <li>Detection Event: event generated by a detector, it has a monitoring target, a severity and a type (warning, drift)</li> </ul>"},{"location":"user_guide/glossary/#actions-terms","title":"Actions Terms","text":"<ul> <li>Importance weights: the retraining dataset is given in form of a set of importance weights associated to every data available for the Task. This importance score will be used during the training pipeline of the customer to weights samples. In particular, the ML model will use the form of the sample weighted loss instead of the standard loss during its retraining phase</li> <li>Dataset boostrapping: if the ML model does not support the sample weighted loss then ML cube Platform can provide a dataset extracted from the available data using sampling with replacement based on the importance weights of the data. The customer can specify the size of the retraining dataset and ML cube Platform provides the best retraining bootstapped dataset of that size</li> <li>Relabeling: in case of concept drift in a classification Task, old labels are no meaningful, given a budget/size constraint ML cube Platform provides the subset of data to be relabelled</li> <li>Active Learning: ML cube Platform provides a set of new synthetic data to label or it provides indication where to collect new real data from the environment</li> </ul>"},{"location":"user_guide/model/","title":"Model","text":"<p>In the ML cube Platform, a Model is a representation of the actual artificial intelligence model used for making predictions. The data used for its training usually represent the reference data distribution, while production data comprises the data on which the model  performs inference. For more information about reference and production data see the Data page.</p> <p>A Model is uniquely associated with a Task and it can be created both through the WebApp and the Python SDK.  Currently, we support only one model per Task.</p> <p>A Model is defined by a name and a version. The version is updated whenever the model is retrained, allowing to  track the latest version of the model and the data used for its training. When predictions are uploaded to the platform, the model version needs to be appropriately specified, following the guidelines in the Data Schema page, to ensure that the predictions are associated to the correct model version.</p> <p>Note</p> <p>You don't need to upload the real model on the Platform. We only require its training data and predictions. The entity you create on the Platform serves more as a placeholder for the actual model. For this reason, the ML cube Platform is considered model agnostic.</p>"},{"location":"user_guide/model/#rag-model","title":"RAG Model","text":"<p>RAG Tasks represent an exception to the model framework presented before. In this type of Tasks, the model is a Large Language Model (LLM), that is used to generate responses to user queries. The model is not trained on a specific dataset but is rather a pre-trained model, sometimes finetuned on custom domain data, which means that the classic process of training and retraining does not apply. </p> <p>To maintain a coherent Model definition across task types, the RAG model is also represented as a Model,  but an update of its version represents an update of the reference data distribution and not necessarily a retraining of the model itself. Moreover, most of the attributes which will be described in the following sections are not applicable, as they are related to the retraining module, which is not available for RAG tasks.</p>"},{"location":"user_guide/model/#probabilistic-output","title":"Probabilistic output","text":"<p>When creating a model, you can specify if you want to provide also the probabilistic output of the model along with the predictions.  The probabilistic output represents the probability or confidence score associated with the model's predictions. If provided,  the ML cube Platform will use this information to compute additional metrics and insights.</p> <p>It is optional and currently supported only for Classification and RAG tasks. If specified, the probabilistic output must be provided  as a new column in the predictions file, following the guidelines in the Data Schema page.</p> <p>Example</p> <p>For example, Logistic Regression classification model provides both the probability of belonging to the positive class and the predicted class using a threshold. In this case, you can upload to ML cube Platform the predicted class as principal prediction and the probability as probabilistic output.</p>"},{"location":"user_guide/model/#model-metric","title":"Model Metric","text":"<p>A Model Metric represents the evaluation metric used to assess the performance of the model.  It can both represent a performance or an error. The chosen metric will be used in the various views of the WebApp to provide insights on the model's performance and in the Performance View section of the Retraining Module.</p> <p>Note</p> <p>Note that model metrics can only be computed when target data are available.</p> <p>The available options are:</p> Metric Task Type Accuracy Classification tasks RMSE Regression tasks R2 Regression tasks Average Precision For Object Detection tasks <p>RAG tasks have no metric, as in that case the model is an LLM for which classic definitions of metrics are not applicable.</p> <p>Warning</p> <p>Model Metrics should not be confused with Monitoring Metrics, which are entities being monitoring by the ML cube Platform and not necessarily related to a Model.</p>"},{"location":"user_guide/model/#suggestion-type","title":"Suggestion Type","text":"<p>The Suggestion Type represents the type of suggestion that the ML cube Platform should provide when computing the  Retraining Dataset. The available options are provided in the related section.</p>"},{"location":"user_guide/model/#retraining-cost","title":"Retraining Cost","text":"<p>The Retraining Cost represents the cost associated with retraining the model. This information is used by the Retraining Module to provide gain-cost analysis and insights on the retraining process. The cost is expressed in the same currency as the one used in the Task cost information. The default value is 0.0, which means that the cost is negligible.</p>"},{"location":"user_guide/model/#retrain-trigger","title":"Retrain Trigger","text":"<p>You can associate a Retrain Trigger to your Model in order to enable the automatic initiation of your retraining pipeline  from the ML cube Platform. More information on how to set up a retrain trigger can be found in the related section.</p>"},{"location":"user_guide/project/","title":"Project","text":"<p>A Project is the secondary organizational entity in ML cube Platform. A Project groups together a set of artificial intelligence algorithms that share a common goal measured by a set of KPIs. For this reason, it is composed of several Tasks that collaborate to reach the Project's goal.</p> <p>Users in the Company can access to one or more Projects according to their roles.</p>"},{"location":"user_guide/project/#creation","title":"Creation","text":"<p>When a Project is created, the User specifies its name, description, and selects the default storage policy.</p> <p>Storage Policy defines the default behavior the Platform follows to access data that are shared with it. Indeed, data shared with ML cube Platform can either be duplicated and stored in ML cube private cloud storage or stay only on customer's cloud and accessed as a remote data source.</p>"},{"location":"user_guide/project/#demo-projects","title":"Demo Projects","text":"<p>To better explore ML cube Platform modules and features, it is possible to create Demo Projects that are not taken into account by subscription quotas. ML cube Platform provides different Demo Projects that cover all the possible use cases (regression, classification, text data, image data, RAG, object detection and so on). To create a Demo Project, you need to check the \"Demo Project\" checkbox and select the one you prefer.</p>"},{"location":"user_guide/project/#kpi-monitoring","title":"KPI Monitoring","text":"<p>A Key Performance Indicator is a measure of performance over time for a specific objective.  While artificial intelligence algorithms try to minimize their loss function, artificial intelligence based solutions and applications look at KPIs. Therefore, it is essential to monitor Project's KPIs along with algorithm performance to have a complete view of the current situation.</p> <p>ML cube Platform offers the possibility to upload Project's KPIs to monitor them via drift detection algorithms. That enables the detection of potentially dangerous trends in what really matters from the business point of view. KPI Monitoring page in the Project sidebar shows the registered KPIs, their trends and drift events ML cube Platform detected during time.</p>"},{"location":"user_guide/project/#integrations","title":"Integrations","text":"<p>ML cube Platform is part of the artificial intelligence and cloud ecosystem and provides connectors to interact with Cloud Providers and MLOps solutions. The Integrations page allows to create and manage credentials that will be used by the Project's Tasks.</p> <p>For instance, if data are stored in a Google Cloud Storage bucket, adding the Google Cloud Platform credentials with the right permissions, allows ML cube Platform to read data from it.</p> <p>Another example is to trigger a Sage Maker pipeline to retrain your artificial intelligence model with a dataset provided by ML cube Platform. In this case, you can create Amazon Web Services credentials with permission to create an event on Amazon Event Bridge. See the Integrations page for more information about credentials setup, data sources and retraining triggers.</p>"},{"location":"user_guide/project/#jobs-monitoring","title":"Jobs Monitoring","text":"<p>Operations like sharing data to ML cube Platform, submitting the creation of a retraining dataset or reports like RAG evaluation, trigger the execution of asynchronous pipelines in ML cube Platform cloud infrastructure. Each pipeline is associated with an identifier named job id that can be used to monitor its execution status. This monitoring can be done both from Web App in the Job Status page and, with specific SDKs method allowing automation. A job failure can be either due to bad requests or internal errors, you can check the error message information via the same page.</p>"},{"location":"user_guide/rbac/","title":"User Roles","text":"<p>The Company owner can create new Users assigning to them a Role. The Role defines a set of actions a User has inside the application. Each User has associated a company role and one or more project roles.</p> <p>Delta Energy inc</p> <p>Delta Energy has one dedicated AI Team to each Task. Hence, they assigned specific Project Administrator Role to each Team leader; while the other Data Scientists have the Project Edit Role for the project they are working on.</p> <p> </p> User Roles in ML cube Platform. <p>The following tables shows the User roles:</p>"},{"location":"user_guide/rbac/#company-permissions","title":"Company Permissions","text":"Role DELETE_COMPANY CHANGE_COMPANY_OWNER MANAGE_COMPANY_ADMIN MANAGE_COMPANY_USER CHANGE_COMPANY_USER_ROLE UPDATE_COMPANY_INFORMATION READ_COMPANY CREATE_PROJECT COMPANY_OWNER COMPANY_ADMIN COMPANY_USER COMPANY_NONE"},{"location":"user_guide/rbac/#project-permissions","title":"Project Permissions","text":"Role DELETE_PROJECT MANAGE_PROJECT_ADMIN UPDATE_PROJECT_INFORMATION MANAGE_PROJECT_USER CHANGE_PROJECT_USER_ROLE WORK_ON_PROJECT READ_PROJECT COMPANY_OWNER COMPANY_ADMIN COMPANY_USER COMPANY_NONE PROJECT_ADMIN PROJECT_USER PROJECT_VIEW"},{"location":"user_guide/task/","title":"Task","text":"<p>A Task is the third and last organizational entity in ML cube Platform. A Task represents an ordinary artificial intelligence task like regression, classification, text generation or object detection.</p> <p>A Task is associated with a Model and a Data schema that describes all the information about the data in the Task.</p> <p>A Task is associated with a unique identifier that will be used by SDK to operate on it. The identifier can be retrieved from the Task homepage or by looking at the url.</p> <p>A Task has a status that summarizes the health of its AI model. The status depends on the monitoring module and changes from Ok, Warning or Drift when the Monitoring modules detect drifts on monitored quantities.</p> <p>Moreover, in the Task homepage is present the section named \"Data events\" which shows the most recent detection events generated by the monitoring module. It is possible to click on view to see more details or discard the notification (the event will remain available for future analysis on the Detection page).</p>"},{"location":"user_guide/task/#attributes","title":"Attributes","text":"<p>A Task is described by a set of attributes specified during its creation. Some attributes are common for every Task, while others vary according to its type. Generic attributes are:</p> Attribute Description Name Name of the Task, unique for the Project. Tags Optional customizable list of tags. They are used to better describe the Task and to improve search. Task type Artificial intelligence type of Task. Possible values are:<ul><li>Regression</li><li>Binary classification</li><li>Multiclass classification</li><li>Multilabel classification</li><li>Retrieval Augmented Generation</li><li>Object Detection</li></ul> Data structure Type of input data the Task uses. Possible values are:<ul><li>Tabular: standard table based data used in contexts like regression or classification.</li><li>Image: images in their different formats and channels.</li><li>Text: textual data expressed as strings. When data structure is Text, attribute Text Language is required.</li><li>Embedding: input data are arrays that could represent embedding either image or text data. This data structure is used when raw data are not shared with ML cube Platform.</li></ul> Optional target Boolean value that specifies if the ground truth is always available or not. In some Tasks, the actual value is not present until explicit labeling is done. In this cases, the Task is marked as with optional target so that ML cube Platform works accordingly. Cost info Optional information about costs that depend on Task Type. Text language Which language is used in the Task when input data structure is text. Positive class Required when Task Type is Binary Classification, it indicates the positive class to be predicted. Context separator Available when Task Type is RAG, it specifies the string separator to split retrieved context into different chunks. <p>Warning</p> <p>Some Task's attributes are immutable: type, data structure and optional target flag cannot be modified after the creation of the Task.</p>"},{"location":"user_guide/task/#platform-modules-and-task-type-compatibility","title":"Platform modules and Task Type compatibility","text":"<p>Most of ML cube Platform operations are done at Task level: monitoring, retraining, analytics and other features are specific to AI models and data that belong to a Task. Indeed, each Task Type has a set of ML cube Platform modules:</p> Module Regression Classification RAG Object Detection Monitoring Explainability Retraining Topic Modeling RAG Evaluation LLM Security <p>Tip</p> <p>On the left side of the web app page the Task men\u00f9 is present, with links to the above mentioned modules and Task settings.</p>"},{"location":"user_guide/task/#task-type","title":"Task Type","text":"<p>ML cube Platform supports several Task Types providing specific features for each of them. Not all Task Types are compatible with data structures, in the table below are shown which data structure is supported by which Task Type:</p> Task Type Tabular Image Text Embedding Regression Classification RAG Object Detection <p>In the following sections, you can find a description of each Task Type with its specific information.</p>"},{"location":"user_guide/task/#regression","title":"Regression","text":"<p>Supervised regression Task with continuous target.</p>"},{"location":"user_guide/task/#cost-information","title":"Cost information","text":"<p>Cost information is expressed by two proportional coefficients \\(c_{o}\\) and \\(c_{u}\\):</p> <ul> <li>\\(c_{o}\\) is the cost of overestimating the target value, i.e., when \\(\\hat{y} &gt; y\\)</li> <li>\\(c_{u}\\) is the cost of underestimating the target value. i.e., when \\(\\hat{y} &lt; y\\)</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{i | \\delta_i &lt; 0} |\\delta_i| \\times c_{o} + \\sum_{i | \\delta_i &gt; 0} \\delta_i \\times c_{u}}{N} $$ where \\(\\delta_i = y_i - \\hat{y}_i\\) is the different between the target and the estimated value.</p>"},{"location":"user_guide/task/#classification","title":"Classification","text":"<p>Supervised classification Task with discrete target. Classification Tasks divides in:</p> <ul> <li>Binary: when then target is a binary variable. For binary classification tasks additional positive class attribute must be specified indicating which value is considered as the positive one. For instance, in fraud detection classification task \"1\" can represent that the sample is a fraud, while \"0\" when it is not. In that case positive class attribute is \"1\".</li> <li>Multiclass: when the target is a categorical variable with more than two possible values but only one value can be assigned.</li> <li>Multilabel: when the target is an array indicating which of the possible categories are present. In this case, each element can be either 0 or 1, and more than one element of the array can be 1.</li> </ul>"},{"location":"user_guide/task/#cost-information_1","title":"Cost information","text":"<p>Cost information differs from each of the three classification types, however, the concept is similar. A cost is associated to every misclassification possibility:</p> <ul> <li> <p>Binary:</p> <ul> <li>\\(c_{FP}\\) is the cost of classifying a negative sample as positive</li> <li>\\(c_{FN}\\) is the cost of classifying a positive sample as negative</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{N_{FP} \\times c_{FP} + N_{FN} \\times c_{FN}}{N} $$ where \\(N_{FP}\\) and \\(N_{FN}\\) are the number of false positives and false negatives respectively.</p> </li> <li> <p>Multiclass:</p> <ul> <li>\\(c_{k}\\) is the cost of misclassifying a sample, whose actual class is \\(k\\), with another class</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{k} N_{k} \\times c_{k} }{N} $$ where \\(N_{k}\\) is the number of misclassified samples of class \\(k\\).</p> </li> <li> <p>Multilabel:</p> <ul> <li>\\(c_{FP}^{k}\\) is the cost of classifying a sample as class \\(k\\) when the actual class \\(k\\) is not present</li> <li>\\(c_{FN}^{k}\\) is the cost of not classifying a sample as class \\(k\\) when the actual class \\(k\\) is present</li> </ul> <p>Given a data batch, the mean cost \\(\\bar{C}\\) is expressed as  $$ \\bar{C} = \\frac{\\sum_{k} N_{FP}^{k} \\times c_{FP}^{k} + N_{FN}^{k} \\times c_{FN}^{k}}{N} $$ where \\(N_{FP}^{k}\\) and \\(N_{FN}^{k}\\) are the number of false positives and false negatives of class \\(k\\) respectively</p> </li> </ul>"},{"location":"user_guide/task/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>Retrieval Augmented Generation is a particular AI task for Text data based on Large Language Models, in which they are used to generate responses of user query using a set of retrieved documents as context to provide a precise and more focused response.</p> <p>RAG Tasks, do not have a Target therefore, the attribute optional target is always set to True. Moreover, in this Task, the Prediction is a text as well and the input is composed of two entities:</p> <ul> <li>User Input: the user query that the model needs to answer</li> <li>Retrieved Context: the set of documents the retrieval engine selected to help the model</li> </ul> <p>RAG tasks has additional the attribute context separator which is a string used to separate different retrieved contexts into chunks. Context data is sent as a single string, however, in RAG settings multiple documents can be retrieved. In this case, context separator is used to distinguish them. It is optional since a single context can be provided.</p> <p>Example</p> <p>Context separator: &lt;&gt; <p>Context data: The capital of Italy is Rome.&lt;&gt;Rome is the capital of Italy.&lt;&gt;Rome was the capital of Roman Empire. <p>Contexts:</p> <pre><code>- The capital of Italy is Rome.\n- Rome is the capital of Italy.\n- Rome was the capital of Roman Empire.\n</code></pre>"},{"location":"user_guide/task/#object-detection","title":"Object Detection","text":"<p>Object Detection task processes images and provides as output a list of bounding boxes with associated label indicating the type of identified entity. Therefore, target is a list of four elements tuples indicating the vertex of the box and a string label for the entity type.</p>"},{"location":"user_guide/integrations/","title":"Integrations","text":"<p>Below, you will find a guide that will help you create the credentials and configure the permissions that ML cube will use to access your resources on the supported cloud providers.</p>"},{"location":"user_guide/integrations/#creating-the-credentials","title":"Creating the credentials","text":"Amazon Web ServicesGoogle Cloud PlatformMicrosoft AzureAWS Compatible <p>The ML cube Platform can assume an IAM Role on your AWS Account, that can be used to authorize actions on specific resources. To create this, log into your AWS account and open the AWS console. Here, go to the IAM service, navigate to the Roles section and create a new role. When asked, select the Custom trust policy option and paste the following json:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::883313729965:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;EXTERNAL_ID&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p><code>883313729965</code> is the ID of the AWS Account used by the ML cube Platform. It is important that this value is not changed. We will populate the value of <code>&lt;EXTERNAL_ID&gt;</code> in a later step. Give your role a name and save it.</p> <p>Now, you will need to create the credentials through the ML cube Platform SDK or the web application.</p> <p>Example</p> <p>The following code will create a set of AWS credentials from the IAM Role we just created.</p> <pre><code>aws_creds = client.create_aws_integration_credentials(\n    name='AWS_01',\n    default=True,  # Set these credentials as the default to use when not specified\n    project_id='your_project_id',\n    role_arn='arn:aws:iam::{{ YOUR_AWS_ACCOUNT_ID }}:role/{{ YOUR_ROLE_NAME }}',\n)\n\ntrust_policy = aws_creds.generate_trust_policy()\nprint(trust_policy)\n</code></pre> <p>You can call the <code>generate_trust_policy</code> function on the created credentials to obtain the trust policy. Edit your IAM Role and change the trust policy to the one you just obtained.</p> <p>Right now, your IAM Role grants no permissions. Please refer to the next sections that will explain how to set up IAM Policies for S3, Event Bridge and so on.</p> <p>To revoke access, simply delete the role or change the trust policy.</p> <p></p> <p>The ML cube Platform can operate in your GCP Account through the creation of a dedicated Service Account. You will then be able to assign one or more IAM Roles to this Service Account, to allow the ML cube Platform to perform specific actions.</p> <p>To configure a Service Account for ML cube Platform, log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create the Service Account with the required permissions. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change this according to your project\nexport GCP_PROJECT=my-project\n\n# Creates a service account called ml3PlatformServiceAccount\ngcloud iam service-accounts create ml3PlatformServiceAccount --display-name \"ML3 Platform Service Account\"\n\n# Generates the access key that will be used to authenticate as the service account\ngcloud iam service-accounts keys create ml3-platform-key.json --iam-account=ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com\n\n# Displays the access key to the terminal screen\ncat ml3-platform-key.json\n</code></pre> <p>Copy the JSON object containing the key and save it to a file with the same name on your disk. Now, you will need to create the GCP credentials, either through the SDK or the web application, and provide the contents of the JSON file you just created.</p> <p>Example</p> <p>The following code will create a set of GCP credentials that will be able to access the service account.</p> <pre><code>with open('path/to/ml3-platform-key.json', 'r') as f:\n    creds_json = f.read()\n\n    gcp_creds = client.create_gcp_integration_credentials(\n        name='GCP_01',\n        default=True,  # Set these credentials as the default to use when not specified\n        project_id='your_project_id',\n        service_account_info_json=creds_json\n    )\n</code></pre> <p>Right now, your IAM Role grants no permissions. Please refer to the next sections that will explain how to set up IAM Policies for Google Cloud Storage, Pub/Sub and so on.</p> <p></p> <p>The ML cube Platform can operate in your Azure Account through the creation of a dedicated Service Principal. You will then be able to assign one or more Roles to this Service Principal, to allow the ML cube Platform to perform specific actions.</p> <p>To configure a Service Principal for ML cube Platform, log into your Azure account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Set the Cloud Shell to use bash instead of powershell. Now we will enter the following command that will create the Service Principal.</p> <pre><code>az ad sp create-for-rbac --name ML3PlatformSP --skip-assignment\n</code></pre> <p>Once the operation finishes running, it will output a JSON object with the following fields: <code>appId</code>, <code>displayName</code>, <code>password</code> and <code>tenant</code>. Copy this object and save it to a file on your disk, for example <code>azure-credentials.json</code>.</p> <p>Now, you will need to create the Azure credentials, either through the SDK or the web application, and provide the contents of the JSON file you just created.</p> <p>Example</p> <p>The following code will create a set of Azure credentials that will be able to access the service account.</p> <pre><code>with open('path/to/azure-credentials.json', 'r') as f:\n    creds_json = f.read()\n\n    azure_creds = client.create_azure_integration_credentials(\n        name='AZURE_01',\n        default=True,  # Set these credentials as the default to use when not specified\n        project_id='your_project_id',\n        service_principal_credentials_json=creds_json\n    )\n</code></pre> <p></p> <p>The ML cube Platform can be connected to AWS compatible services through security credentials.</p> <p>For example, to use min.io's S3-compatible storage, you can use the username and password of your min.io's user as the security credentials. You also need to provide the endpoint where your min.io instance can be reached. You will need to create the credentials through the ML cube Platform SDK or the web application.</p> <p>Example</p> <p>The following code will create a set of AWS-compatible credentials.</p> <pre><code>aws_compatible_creds = client.create_aws_compatible_integration_credentials(\n    name='AWS_COMPATIBLE_01',\n    default=True,  # Set these credentials as the default to use when not specified\n    project_id='your_project_id',\n    access_key_id='username',\n    secret_access_key='password',\n    endpoint_url='https://my-service.com:1234'\n)\n</code></pre> <p>If <code>endpoint_url</code> is set to <code>None</code>, these credentials can be used to access AWS itself as an IAM User.</p> <p>Warning</p> <p>Using AWS-compatible credentials to access AWS itself is discouraged, and should only be used in special cases where you are not able to use an IAM Role.</p>"},{"location":"user_guide/integrations/#storage-integration","title":"Storage integration","text":"AWS S3Google Cloud StorageAzure Blob StorageAWS S3 Compatible <p>Log into your AWS account and open the AWS console, then go to the IAM service and navigate to the Policies section. Here, we will create an IAM Policy.</p> <p>Example</p> <p>The following policy will grant read access to objects in the <code>my-company-data-bucket</code> to the IAM entity it is attached to.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketTagging\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-company-data-bucket\",\n                \"arn:aws:s3:::my-company-data-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Once the IAM Policy has been created, navigate to the Roles section, select the IAM Role you previously created and finally attach the IAM Policy to it.</p> <p></p> <p>Log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create an IAM Role with the required permissions, bind it to the service account you previously created, and grant access to the bucket. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change these according to your project and bucket\nexport GCP_PROJECT=my-project\nexport GCP_BUCKET=my-company-data-bucket\n\n# Creates an IAM Role called ml3PlatformStorageRole for your project, with read permissions on buckets and objects in the storage service\ngcloud iam roles create ml3PlatformStorageRole --project=$GCP_PROJECT --title=\"ML3 Platform Storage Role\" --description=\"Role that allows the ML cube Platform to access storage resources in a project\" --permissions=storage.buckets.get,storage.buckets.list,storage.objects.get,storage.objects.list --stage=ALPHA\n\n# Adds the IAM Role to the previously created service account\ngcloud projects add-iam-policy-binding $GCP_PROJECT --member=serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com --role=projects/$GCP_PROJECT/roles/ml3PlatformStorageRole\n\n# Allows the service account we created to access the given bucket with the objectViewer role\ngsutil iam ch serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com:roles/storage.objectViewer gs://$GCP_BUCKET\n</code></pre> <p></p> <p>Log into your Azure account and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. The following command will associate the previously created Service Principal with a role that is able to read data from a given blob container.</p> <pre><code># Change these according to your project and storage configuration\nexport SERVICE_PRINCIPAL_APP_ID=my-sp-app-id\nexport SUBSCRIPTION_ID=my-azure-subscription-id\nexport RESOURCE_GROUP=my-azure-resource-group\nexport STORAGE_ACCOUNT=my-storage-account\nexport BLOB_CONTAINER=my-blob-container\n\naz role assignment create --assignee $SERVICE_PRINCIPAL_APP_ID --role \"Storage Blob Data Reader\" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT/blobServices/default/containers/$BLOB_CONTAINER\n</code></pre> <p></p> <p>Log into your AWS S3 compatible service and set up a policy that allows to access objects in the bucket of your choice.</p> <p>Example</p> <p>The following policy will grant read access to objects in the <code>my-company-data-bucket</code>.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketTagging\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-company-data-bucket\",\n                \"arn:aws:s3:::my-company-data-bucket/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Adjust the policy according to the specifics of the S3-compatible service you are using.</p> <p>Make sure the policy is attached to the user whose credentials you are going to configure as AWS-compatible credentials in the ML cube platform.</p>"},{"location":"user_guide/integrations/#retrain-events-integration","title":"Retrain events integration","text":"Amazon EventBridgeGCP Pub/SubAzure Event Grid <p>Log into your AWS account and open the AWS console, then go to the IAM service and navigate to the Policies section. Here, we will create an IAM Policy.</p> <p>Example</p> <p>The following policy will allow an IAM Entity to put events in a specific event bus.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Statement1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"events:PutEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:events:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:event-bus/&lt;EVENT_BUS_NAME&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Replace <code>&lt;REGION&gt;</code>, <code>&lt;ACCOUNT_ID&gt;</code> and <code>&lt;EVENT_BUS_NAME&gt;</code> with your desired values.</p> <p>Once the IAM Policy has been created, navigate to the Roles section, select the IAM Role you previously created and finally attach the IAM Policy to it.</p> <p></p> <p>Log into your GCP account, select the correct project and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. Now we will enter some commands that will create an IAM Policy with the required permissions, bind it to the service account you previously created, and grant access to the Pub/Sub topic. A description of each command is provided to help you understand its purpose.</p> <pre><code># Change these according to your project and topic\nexport GCP_PROJECT=my-project\nexport GCP_TOPIC=my-topic\n\n# Adds a new IAM Policy to the previously created service account, granting publish access to the Pub/Sub topic\ngcloud pubsub topics add-iam-policy-binding $GCP_TOPIC --member=serviceAccount:ml3PlatformServiceAccount@$GCP_PROJECT.iam.gserviceaccount.com --role=roles/pubsub.publisher\n</code></pre> <p></p> <p>Log into your Azure account and open the Cloud Shell. You can find the button to open it in the upper-right corner of the page. The following command will associate the previously created Service Principal with a role that is able to publish events to an Event Grid topic.</p> <pre><code># Change these according to your project and storage configuration\nexport SERVICE_PRINCIPAL_APP_ID=my-sp-app-id\nexport SUBSCRIPTION_ID=my-azure-subscription-id\nexport RESOURCE_GROUP=my-azure-resource-group\nexport TOPIC_ID=my-topic\n\naz role assignment create --assignee $SERVICE_PRINCIPAL_APP_ID --role \"EventGrid Data Sender\" --scope /subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.EventGrid/topics/$TOPIC_ID\n</code></pre>"},{"location":"user_guide/integrations/data_sources/","title":"Data Sources","text":"<p>This page provides guidance on integrating external data sources into your ML cube Platform project. There are three types of data sources you can use to enable the ML cube Platform to access your data:</p> <ol> <li>Local data source</li> <li>Remote data source with ML cube Storage</li> <li>Remote data source with Customer Storage</li> </ol>"},{"location":"user_guide/integrations/data_sources/#local-data-source","title":"Local Data Source","text":"<p>This is the easiest way to send data to the ML cube Platform. You can simply specify the path to the local file you want to upload, and it will be uploaded to the ML cube Platform Secure Storage, where it is automatically processed.</p> <p>Example</p> <pre><code>data = TabularData(\n    source=LocalDataSource(\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n        file_path=file_path,\n    )\n)\n</code></pre>"},{"location":"user_guide/integrations/data_sources/#remote-data-source","title":"Remote Data Source","text":"<p>While using a local data source can be a good way to try out the features that the ML cube Platform can offer, in a production scenario you will usually want to integrate your remote data sources with your project. You can decide where you want your data to be stored.</p>"},{"location":"user_guide/integrations/data_sources/#ml-cube-storage","title":"ML cube Storage","text":"<p>This method should be used when the ML cube Platform is allowed to perform a one-time read of the raw data from your data source, and make a copy of it inside the ML cube Platform Secure Storage. This allows the ML cube Platform to store a server-side copy of the processed dataset for quicker access and analysis. Remember that you can request the deletion of your data at any time.</p>"},{"location":"user_guide/integrations/data_sources/#customer-storage","title":"Customer Storage","text":"<p>This method should be used when, due to compliance requirements, the ML cube Platform is only allowed to read data from your data sources when it needs to access it, without storing any sensitive information in its systems. The ML cube Platform will only store a set of anonymous identifiers that are used for data mapping. This gives you the ability to be in full control of when the ML cube Platform can access your data, but it comes at the expense of performance and cost, since constant data transfers can be slow and expensive.</p>"},{"location":"user_guide/integrations/data_sources/#integration-credentials","title":"Integration Credentials","text":"<p>To allow the ML cube Platform to access your external data sources, you need to configure credentials for them. Once the credentials have been configured, they will be available across your entire project, so multiple tasks in the same project will be able to access the same credentials.</p> <p>You can configure multiple credentials for the same provider, for example you could have two sets of credentials to access two different S3 buckets on two different AWS accounts.</p> <p>Below, you can find the configuration steps required to integrate the data source of your choice</p> Amazon S3Google Cloud StorageAzure Blob StorageMinIO <p></p> <p>To integrate Amazon S3, you will need to create a set of AWS credentials, and add a policy that grants read access to objects in your bucket. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify an <code>S3DataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=S3DataSource(\n        object_path='s3://my-company-data-bucket/historical/features.csv',\n        credentials_id=aws_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate Google Cloud Storage, you will need to create a set of GCP credentials, and add a policy that grants read access to objects in your bucket. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify a <code>GCSDataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=GCSDataSource(\n        object_path='gs://my-company-data-bucket/historical/features.csv',\n        credentials_id=gcp_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate Azure Blob Storage, you will need to create a set of Azure credentials, and add a policy that grants read access to objects in your blob container. Please refer to this page to know more.</p> <p>Once you have some credentials, you will be able to specify an <code>AzureBlobDataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=AzureBlobDataSource(\n        object_path='https://mystorageaccount.blob.core.windows.net/my-container/historical/features.csv',\n        credentials_id=azure_creds.credentials_id\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> <p></p> <p>To integrate MinIO, follow these steps:</p> <ol> <li>Create a set of AWS-compatible credentials:<ul> <li>Set <code>access_key_id</code> to the username of the MinIO user you want to use.</li> <li>Set <code>secret_access_key</code> to the password of that MinIO user.</li> <li>Set <code>endpoint_url</code> to the URL of your MinIO instance.</li> </ul> </li> <li>Ensure accessibility:<ul> <li>The MinIO instance must be reachable from the ML cube Platform.</li> <li>If you're using the SaaS version, the MinIO instance must be accessible over the internet.</li> </ul> </li> <li> <p>Set appropriate permissions:</p> <ul> <li>The specified MinIO user must have read access to the bucket and its objects.</li> </ul> <p>Then, you will be able to specify an <code>S3DataSource</code> when adding your data to a task.</p> <p>Example</p> <p>Note that, if you don't specify the <code>credentials_id</code>, the default ones will be used.</p> <pre><code>data = TabularData(\n    source=S3DataSource(\n        object_path='s3://my-company-data-bucket/historical/features.csv',\n        credentials_id=aws_creds.credentials_id,\n        data_structure=DataStructure.TABULAR,\n        file_type=FileType.CSV,\n    )\n)\n</code></pre> </li> </ol>"},{"location":"user_guide/integrations/retrain_trigger/","title":"Retrain Trigger","text":"<p>This section offers an overview of how you can set up a retrain trigger for your model.  Retrain triggers enable the automatic initiation of your retraining pipeline from the ML cube Platform. They are designed as  integrations with external services and thus require credentials with the appropriate privileges to be executed.</p> <p>A Retrain Trigger can be utilized within a Detection Event Rule. Alternatively, it can be manually activated from the WebApp, in the Retraining section.</p>"},{"location":"user_guide/integrations/retrain_trigger/#supported-triggers","title":"Supported Triggers","text":"<p>The following retrain triggers are supported:</p> <ul> <li><code>Amazon EventBridge</code>: puts an event in an Event Bus.</li> <li><code>GCP Pub/Sub</code>: puts an event in a Pub/Sub Topic.</li> <li><code>Azure Event Grid</code>: puts an event in an Event Grid Topic.</li> </ul> Amazon EventBridgeGoogle Cloud PlatformAzure Event Grid <p></p> <p>If your MLOps pipelines are set up in the AWS ecosystem, then you probably need the Amazon EventBridge retrain trigger.</p> <p>The trigger, when activated, will create an event in a Event Bus of your AWS account with custom metadata.</p> <p>You need to create an Event Bus rule that recognizes the ML cube Platform event pattern and attach the target action you want. Examples of targets are:</p> <ul> <li>launching a Lambda function;</li> <li>launching a SageMaker pipeline;</li> <li>sending a message to a SQS Queue with a retraining request.</li> </ul> <p></p> <p>If your MLOps pipelines are set up in the Google Cloud Platform ecosystem, then you probably need the GCP Pub/Sub retrain trigger.</p> <p>The trigger, when activated, will create an event in a Pub/Sub topic of your GCP project with custom metadata.</p> <p></p> <p>If your MLOps pipelines are set up in the Microsoft Azure ecosystem, then you probably need the Azure Event Grid retrain trigger.</p> <p>The trigger, when activated, will create an event in an Event Grid topic of your Azure resource group with custom metadata.</p>"},{"location":"user_guide/integrations/retrain_trigger/#event-bus-setup","title":"Event Bus Setup","text":"<ol> <li> <p>In the AWS console, open the Amazon EventBridge service and select the <code>Event buses</code> option in the left-side menu      </p> </li> <li> <p>In the <code>Custom event bus</code> tab, create a new event bus with the default settings</p> </li> <li> <p>Select the <code>Rules</code> section on the left-side menu and click the <code>Create Rule</code> button      </p> </li> <li> <p>Insert the rule name and the created Event Bus in the Event Bus section. Click <code>Next</code></p> <ol> <li><code>Event source</code>: select the voice AWS events or EventBridge partner events </li> <li><code>Sample event</code>: copy and paste this: <pre><code>{\n    \"version\": \"0\",\n    \"id\": \"fcdd87c7-f56e-c722-4f85-4cb6ba85a00a\",\n    \"detail-type\": \"retrain_trigger\",\n    \"source\": \"ml3_platform\",\n    \"account\": \"123456789\",\n    \"time\": \"2023-11-02T14:16:23Z\",\n    \"region\": \"eu-west-3\",\n    \"resources\": [],\n    \"detail\": {\n        \"first_param\": \"hi\",\n        \"second_param\": \"bye\"\n    }\n}\n</code></pre> </li> <li><code>Creation method</code>: select Custom pattern (JSON editor)  </li> <li><code>Event pattern</code>: copy and paste this:  <pre><code>{\n    \"source\": [{\n        \"prefix\": \"ml3_platform\"\n    }],\n    \"detail-type\": [{\n        \"prefix\": \"retrain_trigger\"\n    }]\n}\n</code></pre></li> <li>click the button Test pattern to check the match and then click next</li> </ol> </li> <li> <p>Select the target that will handle the events. If you want to test the rule, you can add a CloudWatch target that stores the event to a new Log Group.      </p> </li> <li> <p>Create the rule</p> </li> </ol> <p>Retrain Trigger Setup</p> <p>To integrate Amazon Event Bridge, you need to create a set of AWS credentials, and add a policy that allows to put events in  your event bus. Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK  or the web application.</p> SDK Example <p>Here is an example of how to set up an AWS Event Bridge Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=AWSEventBridgeRetrainTrigger(\n        credentials_id='your_credentials_id',\n        aws_region_name='e.g. eu-west-3',\n        event_bus_name='your_event_bus',\n    ),\n)\n</code></pre>"},{"location":"user_guide/integrations/retrain_trigger/#topic-setup","title":"Topic Setup","text":"<ol> <li> <p>In the Google Cloud console, open the Pub/Sub service and select the <code>Topics</code> option in the left-side menu      </p> </li> <li> <p>Click on the <code>Create topic</code> button. Then give it a unique id and create it.      </p> </li> <li> <p>Configure your subscriptions as needed to configure the service that will handle the events.</p> </li> </ol> <p>The ML cube platform will send events with this format: <pre><code>{\n    \"project_id\": \"your gcp project id\",\n    \"topic_name\": \"your unique topic id\",\n    \"source\": \"ml3_platform\",\n    \"event_type\": \"retrain_trigger\",\n    \"payload\": {\n        \"model_id\": \"id of the model on ml cube platform\"\n    }\n}\n</code></pre></p> <p>Retrain Trigger Setup</p> <p>To integrate GCP Pub/Sub, you need to create a set of GCP credentials, and add a policy that allows to put events  in your Pub/Sub topic. Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK or  the web application.</p> SDK Example <p>Here is an example of how to set up a GCP Pub/Sub Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=GCPPubSubRetrainTrigger(\n        credentials_id='your_credentials_id',\n        topic_name='your-topic'\n    ),\n)\n</code></pre>"},{"location":"user_guide/integrations/retrain_trigger/#topic-setup_1","title":"Topic Setup","text":"<ol> <li> <p>In the Azure console, open the Event Grid service and select the <code>Topics</code> option in the left-side menu      </p> </li> <li> <p>Click on the <code>Create</code> button. Select an active subscription and the resource group, then give the topic a unique name, select your preferred region and create it.      </p> </li> <li> <p>Configure your subscriptions as needed to configure the service that will handle the events.</p> </li> </ol> <p>The ML cube platform will send events with this format: <pre><code>{\n    \"subject\": \"ml3_platform\",\n    \"event_type\": \"retrain_trigger\",\n    \"data_version\": \"1.0\",\n    \"data\": {\n        \"model_id\": \"id of the model on ml cube platform\"\n    }\n}\n</code></pre></p> <p>Retrain Trigger Setup</p> <p>To integrate Azure Event Grid, you need to create a set of Azure credentials, and add a role that allows to publish events in your Event Grid topic.  Please refer to this page for more information.</p> <p>Once the credentials and the policy have been created, you can set up the retrain trigger for your model through the SDK or the web application.</p> SDK Example <p>Here is an example of how to set up an Azure Event Grid Retrain Trigger using the SDK:</p> <pre><code>client.set_retrain_trigger(\n    model_id='your_model_id',\n    trigger=AzureEventGridRetrainTrigger(\n        credentials_id='your_credentials_id',\n        topic_endpoint='https://your-retrain-topic.italynorth-1.eventgrid.azure.net/api/events'\n    ),\n)\n</code></pre>"},{"location":"user_guide/modules/","title":"Modules","text":"<p>ML cube Platform is a suite of solutions for AI model supervision, observability and maintenance covering all the aspects of the post deployment phase in the life cycle of your AI systems.</p> <p>Modules can be always active or on-demand: Monitoring module and Drift Explainability automatically analyses data and generate reports as soon as new data arrive or new drifts are detected; instead, Retraining report, RAG Evaluation, LLM Security and Topic Modeling reports are explicitly requested by the user.</p> <ul> <li> <p> Monitoring</p> <p>Data drift detection over data.</p> <p> More info</p> </li> <li> <p> Drift Explainability</p> <p>Understand the nature of detected drift.</p> <p> More info</p> </li> <li> <p> Retraining</p> <p>Update your model to handle the drift.</p> <p> More info</p> </li> <li> <p> RAG Evaluation</p> <p>Check the quality of your RAG system.</p> <p> More info</p> </li> <li> <p> LLM Security</p> <p>Verify robustness of your solution.</p> <p> More info</p> </li> <li> <p> Topic Modeling</p> <p>Identify sub-domains in your data.</p> <p> More info</p> </li> </ul>"},{"location":"user_guide/modules/business/","title":"Business mode","text":"<p>AI projects have some KPIs to improve and that are used to measure their effectiveness. Business module of ML cube Platform aims at provide a different view over AI projects that is more related to the impact the algorithms have on the company.</p> <p>Differently from the monitoring and retraining modules, this module is at Project level.</p>"},{"location":"user_guide/modules/business/#kpi-monitoring","title":"KPI Monitoring","text":"<p>You can log the project KPIs on ML cube Platform to monitor them like the model performance. Our detection algorithms will analyze each KPI showing its trend and raising alarms whenever a drift is detected.</p>"},{"location":"user_guide/modules/labeling/","title":"Labeling","text":"<p>We are sorry, this page is under construction...</p>"},{"location":"user_guide/modules/llm_security/","title":"LLM Security","text":""},{"location":"user_guide/modules/monitoring/","title":"Monitoring","text":"<p>Data and model monitoring is fundamental to guarantee performance of your ML models. With ML cube Platform you can log and monitor different aspects of your ML Task by uploading data batches. Before entering in details about what ML cube Platform monitors and analyses, it is worth to mention how data are represented. Data are shared as batches</p>"},{"location":"user_guide/modules/monitoring/#data-taxonomy","title":"Data Taxonomy","text":"<p>A Batch of data is composed of four types categories:</p> <ul> <li>input: set of input features the AI model uses to predict the output.   ML cube Platform uses the input data that comes at the end of the processing data pipeline and not the raw data.   This is due to the fact that ML cube Platform detects drifts in what the AI model uses and not in the general data the customer has.</li> <li>target: target quantity predicted by the AI models.   It is present in the training data but can be not available for production data.</li> <li>models' predictions: predicted target for each AI model in the AI Task.</li> <li>metadata: additional information that AI models do not use as input but that is important to define the data or the samples.   Mandatory for this category are the <code>sample-id</code>, a unique identifier for each sample used to avoid confusion and misinterpretation; and the   <code>sample-timestamp</code>, a timestamp associated with each sample used for ordering.   Moreover, the User can provide additional data used to segment the data space.   For instance, sensitive information like zip code or country are not used by AI models to prevent bias, however, ML cube Platform can use them to   check and prevent bias in the suggested retraining dataset or to perform segmented drift detection.</li> </ul>"},{"location":"user_guide/modules/monitoring/#data-categories","title":"Data Categories","text":"<p>ML cube Platform are present three categories of data:</p> <ul> <li>Reference: represents the dataset used to train the model.   Each model version has a reference dataset.   Detection algorithms use reference data during their initialization.</li> <li>Production: represents data that comes from the production environment in which the AI model is operating.   Detection algorithms analyze production data to detect the presence of drifts.</li> <li>Historical: represents additional data that ML cube Platform can use to define the retraining dataset after a drift.</li> </ul> <p>Each data category is uploaded to the application with its specific API call, however, they share the same structure. When a data batch is uploaded the data source for each each data type (input, target, prediction) is specified.</p> <p>The peculiarity of Production upload is that data types can be sent asynchronously with different API calls. That's why during production data arrive at different times, usually input and prediction are available together and target after a while.</p> <p> </p> ML cube Platform data categories. <p>Delta Energy inc</p> <p>Delta Energy company trained its models using the data in the year 2022 and used the algorithms starting from the 2023. This means that the data in the 2022 are the reference data and every data from the january first 2023 are considered as production data. Data previous 2022 are historical data instead.</p>"},{"location":"user_guide/modules/monitoring/#drift-detection","title":"Drift Detection","text":"<p>ML cube Platform provides a set of Detectors for each AI Task. These detectors are used to monitor the task at different levels. The choice over the types of detectors to be instantiated depends on the type of task (classification or regression) and on the type of data available for that task (input, output, model predictions).</p> <p>There are mainly two classes of detectors:</p> <ul> <li>Data Detectors: they take into account data associated with the task. They may be input only   data or input and ground truth data. These detectors are independent from the models trained on the   task as they do not either consider their predictions or performances. These detectors are responsible for the identification of   input and concept drifts. According to the type of the used detector, changes in data are either monitored at feature   level or using a multivariate monitoring strategy.</li> <li>Model Detectors: they monitor the performances associated with the models related to the task.   In cases where the user has multiple models trained for a single task, a single detector is created for each model.</li> </ul> <p>Each detector is initially created using Reference data provided by the user. Every time a new batch of data is uploaded, the detectors observe the batch and update their statistics. Each detector updates its statistics independently from the others and each of them presents a double-level alarm scheme in order to either signal a Warning or a Drift for the monitored task.</p> <p>The detectors may be in three different states:</p> <ul> <li>Regular: the detector is monitoring data that are similar to the reference data,</li> <li>Warning: the detector has fired a Warning alarm since the data has started to change. From this zone, it is possible   to either go into the Drift status or to go back to the Regular one, depending on the monitored data.</li> <li>Drift: the detector has fired a Drift alarm and a change has been established between the reference data and the last   ones. After a drift, the detector is usually reset by defining a new set of reference data. The reset process is different   according to what has been monitored by the detector.</li> </ul> <p>All the alarms generated during this process are shown in the application like Detection Events available in the Task homepage or in the Detection page. You can create automation rules based on those events to be notified on specific channels or start retraining, see Detection automation rules for more details.</p>"},{"location":"user_guide/modules/rag_evaluation/","title":"RAG Evaluation","text":""},{"location":"user_guide/modules/retraining/","title":"Retraining","text":"<p>The Retraining module of ML cube Platform plays a fundamental role in dealing with data drifts and should be used as consequence of drift detection alarms. Indeed, usually, a data drift determines a drop in the model's predictive capacity that starts providing bad predictions and therefore, degrading its performance. As soon as a data drift has been detected, action must be taken to avoid excessive performance degradation and thus business issues.</p> <p>Before entering in the details about the retraining module, it is important to describe what happen after the model is in production. In fact, the model artifact deployed in production does not last forever but further versions will be generated using newly fresh data that will be up to date with the last data distribution. The model update with retraining can be done on temporal basis or as soon as a data drift is detected.</p> <p> </p> AI model lifecycle."},{"location":"user_guide/modules/retraining/#retraining-dataset","title":"Retraining dataset","text":"<p>The main outcome of the Retraining module is the retraining dataset that you should use to retrain your AI model adapting it to the new discovered data distributions. The dataset computation is based on transfer learning techniques and leverages all the uploaded data to maximize the available information. In fact, after a drift has been detected, production data belonging to the new data distribution could be not sufficient for good model training. Hence, even if previous data come from another distribution, they can be used for retraining if properly transformed.</p> <p>The retraining dataset can have two formats:</p> <ul> <li>Sample weights:     each sample uploaded in ML cube Platform is assigned a weight that can be used as sample weight in a weighted loss function.     The higher the weight, the greater the importance of the sample for the new retraining.</li> <li>Data samples:     a list of sample ids (using data schema column object with role ID) is provided indicating which data form the retraining dataset.     This format can be used when the training procedure does not support weighted loss or when a fixed size retraining dataset is preferred.     Note that samples ids can appear more than once, this could happen when a sample is particular importante for the new retraining.</li> </ul> <p>The retraining report contains additional indicators and information that helps you to better understand the provided retraining dataset:</p> <ul> <li>Performance view</li> <li>Cost view</li> <li>Dataset info</li> </ul>"},{"location":"user_guide/modules/retraining/#performance-view","title":"Performance view","text":"<p>Performance view provides performance interval for three key moments:</p> <ul> <li>Last Concept: the performance the model has before the drift</li> <li>Current: the performance the model has after the drift</li> <li>Forecast: the estimated performance of the model if it was retrained with dataset contained in the retraining report</li> </ul> <p>The performance are shown as an interval, this interval is measured from production data for Last concept and Current while is estimated for Forecast. In particular, the Forecast upper bound is a theoretical optimistic bound that expressed the best scenario given the actual data distribution; the lower bound instead, is an empirical estimation done with a simple ML model.</p>"},{"location":"user_guide/modules/retraining/#cost-view","title":"Cost view","text":"<p>If the Task has cost information it is possible to show this view. It is similar to the performance view but the quantity shown is economic costs of the model with that level of performance.</p>"},{"location":"user_guide/modules/retraining/#dataset-info","title":"Dataset info","text":"<p>Information about the last training dataset and the proposed one:</p> <ul> <li>From detected drift: the number of available samples received from the last drift provides information about the amount of data available from the latest concept.</li> <li>Effective sample size: quantifies the amount of information conveyed by our suggestion. It represents the hypothetical number of independent observations in a sample that would provide the same information as the computed suggestion. As the magnitude of the drift increases, this value decreases because less information from the past can be reused.</li> <li>Last reference: the number of samples used in the last reference data.</li> <li>Whole data: the percentage of data considered in providing the suggestion relative to the total available data.</li> </ul>"},{"location":"user_guide/modules/topic_modeling/","title":"Topic Modeling","text":"<p>The Topic Modeling module allow you to categorize documents based on their content. The goal is to represent each document as a set of topics, where a topic is composed by a list of words that commonly appear together. The percentage of topics in a document varies, suggesting the themes it covers and in what proportion.</p> <p>Example</p> <p>Consider the case of a sport magazine. Words like \"team,\" \"game,\" and \"score\" would come up a lot, while words like \"market\" or \"technology\" would show up less frequently. This would suggest that the magazine's topics are centered around sports.</p>"},{"location":"user_guide/modules/topic_modeling/#key-concepts","title":"Key Concepts","text":"Term Description Topic A theme represented by a set of words that commonly appear together. Document Distribution Each document shows a spread of topics, indicating the themes it covers and in what proportion."},{"location":"user_guide/modules/topic_modeling/#supported-tasks-and-data-structures","title":"Supported Tasks and Data Structures","text":"<p>ML cube Platform supports the following tasks and data structures for Topic Modeling:</p> Task Type Tabular Image Text Embedding Regression Classification RAG Topic Modeling Timeseries: visualization of topic distribution over time."},{"location":"user_guide/monitoring/","title":"Monitoring","text":"<p>The monitoring module is a key feature of the ML cube Platform.  It enables continuous tracking of AI models performance over time, helping to identify potential issues.  It also implements the monitoring of production data to preemptively detect distribution changes, ensuring that the model continues to perform as expected and aligns with business requirements.</p>"},{"location":"user_guide/monitoring/#why-do-you-need-monitoring","title":"Why do you need Monitoring?","text":"<p>Machine Learning algorithms are based on the assumption that the distribution of the data used for training is the same as the one from which production data are drawn from. This assumption never holds in practice, as the real world is characterized by dynamic and ever-changing conditions. These distributional changes, if not addressed properly, can cause a drop in the model's performance, leading to bad estimations or predictions, which in turn can have a negative impact on the business.</p> <p>Monitoring, also known as Drift Detection in the literature, refers the process of continuously tracking the performance of a model  and the distribution of the data it is operating on to identify significant changes.</p>"},{"location":"user_guide/monitoring/#how-does-the-ml-cube-platform-perform-monitoring","title":"How does the ML cube Platform perform Monitoring?","text":"<p>The ML cube platform performs monitoring by employing statistical techniques to compare a certain reference (for instance, data used for training or the  performance of a model on the test set) to incoming production data. </p> <p>These statistical techniques, also known as monitoring algorithms, are tailored to the type of data being observed; for instance, univariate data requires different monitoring techniques than multivariate data.  However, you don't need to worry about the specifics of these algorithms, as the ML cube Platform takes care of selecting the most appropriate ones for your Task.</p> <p>If a significant difference between reference and production data is detected, an alarm is raised, signaling that the monitored entity is drifting away from the expected behavior and that corrective actions should be taken.</p> <p>In practical terms, you can use the SDK to specify the time period where the reference of a given model should be placed. As a consequence, all algorithms associated with the specified model (not just those monitoring the performance, but also those operating  on the data used by the model) will be initialized on the specified reference. Of course, you should provide to the  Platform the data you want to use as a reference before setting the reference itself. This can be done through the SDK as well.</p> <p>After setting the reference, you can send production data to the platform, still using the SDK. This data will be analyzed by the monitoring algorithms and, if a significant difference is detected, an alarm will be raised, in the form of a Detection Event.  You can explore more about detection events and how you can set up automatic actions upon their reception in the Detection Event  and the Detection Event Rule sections respectively.</p>"},{"location":"user_guide/monitoring/#targets-and-metrics","title":"Targets and Metrics","text":"<p>After explaining why monitoring is so important in modern AI systems and detailing how it is performed in the ML cube Platform,  we can introduce the concepts of Monitoring Targets and Monitoring Metrics. They both represent quantities that the ML cube Platform monitors,  but they differ in their nature. The figure below provides an overview of how Monitoring Targets and Metrics are  related to each other and to the entities of the Task.</p> <p> </p>  Monitoring Targets and Metrics overview <p>Monitoring Targets and Metrics are defined by the ML cube platform based on the Task attributes, such as the Task Type and the Data Structure,  and their monitoring is automatically enabled upon the Task creation. The idea underlying defining many entities to monitor,  rather than monitoring only the model error, is to provide a comprehensive view of the model's performance and the data distribution, easing the identification of the root causes of a drift and thus facilitating the corrective actions.</p>"},{"location":"user_guide/monitoring/#monitoring-targets","title":"Monitoring Targets","text":"<p>A Monitoring Target is a relevant entity involved in a Task. They represent the main quantities monitored by the platform, those whose variation can have a significant impact on the AI task success. </p> <p>The ML cube platform supports the following monitoring targets:</p> Monitoring Target Description INPUT the input distribution, \\(P(X)\\). CONCEPT the joint distribution of input and target, \\(P(X, Y)\\). PREDICTION the prediction of the model, \\(P(\\hat{Y})\\). INPUT PREDICTION the joint distribution of input and prediction, \\(P(X, \\hat{Y})\\). ERROR the error of the model, whose computation depends on the task type. USER INPUT the input provided by the user, usually in the form of a query. This target is only available in tasks of type RAG. USER INPUT RETRIEVED CONTEXT the similarity between the user input and the context retrieved by the RAG system. This target is only available in tasks of type RAG. USER INPUT MODEL OUTPUT the similarity between the user input and the response of the Large Language Model. This target is only available in tasks of type RAG. MODEL OUTPUT RETRIEVED CONTEXT the similarity between the response of the Large Language Model and the context retrieved by the RAG system. This target is only available in tasks of type RAG. <p>As mentioned, some targets are available only for specific Task types. The following table shows all the available monitoring targets in relation with the Task Type.  While some targets were specifically designed for a certain Task Type, others are more general and can be used in different contexts.  Nonetheless, the platform might not support yet all possible combinations. The table will be updated as new targets are added to the product.</p> Monitoring Target REGRESSION CLASSIFICATION BINARY CLASSIFICATION MULTICLASS CLASSIFICATION MULTILABEL OBJECT DETECTION RAG INPUT CONCEPT PREDICTION INPUT PREDICTION ERROR USER INPUT USER INPUT RETRIEVED CONTEXT USER INPUT MODEL OUTPUT MODEL OUTPUT RETRIEVED CONTEXT"},{"location":"user_guide/monitoring/#monitoring-metrics","title":"Monitoring Metrics","text":"<p>A Monitoring Metric is a generic quantity that can be computed on a Monitoring Target. They enable the monitoring of specific aspects of an entity, which might help in identifying the root cause of a drift, as well as defining the corrective actions to be taken.</p> <p>The following table displays the Monitoring Metrics supported, along with their Monitoring Target and the conditions under which they are actually computed and monitored. The possible values that each metric can assume are also provided. This table is subject to changes, as new metrics will be added in the future.</p> Monitoring Metric Description Monitoring Target Conditions Possible values TEXT TOXICITY The toxicity of the text INPUT, USER INPUT, PREDICTION When the data structure is text Either neutral or toxic. TEXT EMOTION The emotion of the text INPUT, USER INPUT When the data structure is text If the Task text language is Italian, one between these: anger, joy, sadness, fear.   Otherwise, one between these: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise, neutral. TEXT SENTIMENT The sentiment of the text INPUT, USER INPUT When the data structure is text If the Task text language is Italian, one between these: POSITIVE, NEGATIVE. Otherwise, one between these: negative, neutral, positive TEXT LENGTH The length of the text INPUT, USER INPUT, RETRIEVED CONTEXT, PREDICTION When the data structure is text An integer value. MODEL PERPLEXITY A measure of the uncertainty of an LLM when predicting the next words PREDICTION When the task type is RAG A floating point value. IMAGE BRIGHTNESS The brightness of the image INPUT When the data structure is image A floating point value. IMAGE CONTRAST The contrast of the image INPUT When the data structure is image A floating point value. BBOXES AREA The average area of the predicted bounding boxes PREDICTION When the task type is Object Detection A floating point value. BBOXES QUANTITY The average number of predicted bounding boxes per image PREDICTION When the task type is Object Detection An integer value."},{"location":"user_guide/monitoring/#monitoring-status","title":"Monitoring Status","text":"<p>All the entities being monitored are associated with a status, which can be one of the following:</p> Status Description OK the entity is behaving as expected. WARNING the entity has shown signs of drifts, but it is still within the acceptable range. DRIFT the entity has experienced a significant change and corrective actions should be taken. <p>The following state diagram illustrates the possible transitions between the statuses, as well as the events that trigger them.</p> <pre><code>stateDiagram-v2\n\n    [*] --&gt; OK : Initial State\n\n    OK --&gt; WARNING : Warning On\n    WARNING --&gt; OK : Set new reference \n    WARNING --&gt; OK : Warning Off\n\n\n    WARNING --&gt; DRIFT : Drift On\n    DRIFT --&gt; WARNING : Drift Off\n\n    DRIFT --&gt; OK : Set new reference\n    DRIFT --&gt; OK : Drift Off</code></pre> <p>Notice that a Drift Off event can either bring the entity back to the <code>OK</code> status or to the <code>WARNING</code> status,  depending on the velocity and intensity of the change and the monitoring algorithm's sensitivity. The same applies to the Drift On events, which can both occur when the entity is in the <code>WARNING</code> status or in the <code>OK</code> status.</p> <p>The only transitions which are not due to Detection Events are the ones caused by the specification of a new reference.  In this case, the status of the entity is reset to <code>OK</code> for every entity as all the monitoring algorithms are reinitialized on the new reference.</p> <p>You can check the status of the monitored entities both through the WebApp and the SDK. In particular, the homepage of the Task  displays the status of both monitoring targets and metrics, while the SDK provides a couple of methods to  retrieve the status of the monitored entities programmatically.</p> SDK Example <p>You can visualize the status of a Monitoring Target using the  <code>get_monitoring_status</code> method of the <code>Client</code> object:</p> <pre><code>status = client.get_monitoring_status(\n    task_id=task_id, \n    monitoring_target=MonitoringTarget.INPUT,\n)\n</code></pre> <p>In the same way, you can retrieve the status of a Monitoring Metric:</p> <pre><code>status = client.get_monitoring_status(\n    task_id=task_id, \n    monitoring_target=MonitoringTarget.USER_INPUT,\n    monitoring_metric=MonitoringMetric.TEXT_TOXICITY,\n)\n</code></pre> <p>The method returns a BaseModel of type <code>MonitoringQuantityStatus</code> that contains the status of the  specified monitoring entity.</p>"},{"location":"user_guide/monitoring/detection_event/","title":"Detection Event","text":"<p>A Detection Event is raised by the ML cube Platform when a significant change is detected in one of the entities being monitored.</p> <p>An event is characterized by the following attributes:</p> Attribute Description Eventy Type The type of the event. It's possible values are: <ul><li> <code>Warning On</code>: the monitoring entity is experiencing slight changes that might lead to a drift.</li><li> <code>Warning Off</code>: the monitoring entity has returned to the reference distribution. </li><li> <code>Drift On</code>: the monitoring entity has drifted from the reference distribution.</li><li> <code>Drift Off</code>: the monitoring entity has returned to the reference distribution.</li> </ul> Severity The severity of the event. It's provided only for drift events and it can be <code>Low</code>, <code>Medium</code>, or <code>High</code>. Monitoring Target The Monitoring Target being monitored. Monitoring Metric The Monitoring Metric being monitored. Model Name The name of the model that raised the event. It's present only if the event is related to a model. Model Version The version of the model that raised the event. It's present only if the event is related to a model. Insert datetime The time when the event was raised. Sample timestamp The timestamp of the sample that triggered the event. Sample customer ID The id of the sample that triggered the event. User feedback The feedback provided by the user on whether the event was expected or not."},{"location":"user_guide/monitoring/detection_event/#retrieve-detection-events","title":"Retrieve Detection Events","text":"<p>You can access the detection events generated by the Platform in two ways:</p> <ul> <li>SDK: it can be used to retrieve all detection events for a specific task programmatically.</li> <li>WebApp: navigate to the <code>Detection</code> section located in the task page's sidebar. Here, all detection events are displayed in a table,     with multiple filtering options available for useful event management. Additionally, the latest detection events identified are shown in the Task homepage,    in the section named \"Latest Detection Events\".</li> </ul> SDK Example <p>The following code demonstrates how to retrieve all detection events for a specific task.</p> <pre><code>detection_events = client.get_detection_events(task_id='my-task-id')\n</code></pre>"},{"location":"user_guide/monitoring/detection_event/#user-feedback","title":"User Feedback","text":"<p>When a <code>Drift On</code> event is raised, you can provide feedback on whether the event was expected or not. This feedback is then used  to tune the monitoring algorithms and improve their performance. The feedback can be provided through the WebApp, in the <code>Detection</code> section of the task page, or through the SDK.</p>"},{"location":"user_guide/monitoring/detection_event/#detection-event-rules","title":"Detection Event Rules","text":"<p>To automate actions upon the reception of a detection event, you can set up Detection Event Rules.  You can learn more about how to configure them in the Detection Event Rules section.</p>"},{"location":"user_guide/monitoring/detection_event_rules/","title":"Detection Event Rules","text":"<p>This section outlines how to configure automation to receive notifications or start retraining after a Detection Event occurs.</p> <p>When a Detection Event is produced, the ML cube Platform reviews all the Detection Event Rules you have set  and triggers those matching the event.</p> <p>Rules are specific to a task and are characterized by the following attributes:</p> Attribute Description Name A descriptive label of the rule. Detection Event Type The type of event that triggers the rule. Severity The severity of the event that triggers the rule. It is only applicable to drift events. If not specified, the rule will be triggered by drift events of any severity. Monitoring Target The Monitoring Target whose event should trigger the rule. Monitoring Metric The Monitoring Metric whose event should trigger the rule. Model name The name of the model to which the rule applies. This is only required when the monitoring target is related to a model (such as <code>ERROR</code> or <code>PREDICTION</code>). Actions A list of actions to be executed sequentially when the rule is triggered."},{"location":"user_guide/monitoring/detection_event_rules/#detection-event-actions","title":"Detection Event Actions","text":"<p>Three types of actions are currently supported: notification, plot configuration and retrain.</p>"},{"location":"user_guide/monitoring/detection_event_rules/#notifications","title":"Notifications","text":"<p>These actions send notifications to external services when a detection event is triggered. The following notification options are available:</p> Channel Description Slack Notification Sends a notification to a Slack channel via webhook. Discord Notification Sends a notification to a Discord channel via webhook. Email Notification Sends an email to the provided email address. Teams Notification Sends a notification to Microsoft Teams via webhook. Mqtt Notification Sends a notification to an MQTT broker."},{"location":"user_guide/monitoring/detection_event_rules/#plot-configuration","title":"Plot Configuration","text":"<p>This action consists in creating two plot configurations when a Detection Event is triggered: the first one includes data preceding the event, while the second one includes data following the event.</p>"},{"location":"user_guide/monitoring/detection_event_rules/#retrain","title":"Retrain","text":"<p>Retrain Action enables the automatic retraining of your model. Therefore, it is only available when the target of the rule is related to a model. The retrain action does not need any parameter because it is automatically inferred from the <code>model name</code> attribute of the rule. Of course, the model must already have a Retrain Trigger associated before setting up this action.</p> SDK Example <p>The following code demonstrates how to create a rule that matches high severity drift events on the error of a model.  When triggered, it first sends a notification to the <code>ml3-platform-notifications</code> channel on your Slack workspace, using the  provided webhook URL, and then starts the retraining of the model.</p> <pre><code>rule_id = client.create_detection_event_rule(\n    name='Retrain model with notification',\n    task_id='my-task-id',\n    model_name='my-model',\n    severity=DetectionEventSeverity.HIGH,\n    detection_event_type=DetectionEventType.DRIFT_ON,\n    monitoring_target=MonitoringTarget.ERROR,\n    actions=[\n        SlackNotificationAction(\n            webhook='https://hooks.slack.com/services/...',\n            channel='ml3-platform-notifications'\n        ),\n        RetrainAction()\n    ],\n)\n</code></pre>"},{"location":"user_guide/monitoring/drift_explainability/","title":"Drift Explainability","text":"<p>Monitoring  is a crucial aspect of the machine learning lifecycle, as it enables tracking the model's performance and its data over time, ensuring the model continues to function as expected. However, monitoring only is not enough when it comes to the adaptation phase.</p> <p>In order to make the right decisions, you need to understand what were the main factors that led to the drift in the first place, so that the correct actions can be taken to mitigate it. s The ML cube Platform supports this process by providing what we refer to as Drift Explainability Report,  automatically generated upon the detection of a drift and containing several elements that should help you diagnose the root causes  of the change occurred.</p> <p>You can access the reports in the WebApp, by navigating to the <code>Drift Explainability</code> tab in the sidebar of the Task page.</p>"},{"location":"user_guide/monitoring/drift_explainability/#structure","title":"Structure","text":"<p>A Drift Explainability Report consists in comparing the reference data and the portion of production data where the drift was identified, hence  those belonging to the new data distribution. Notice that these reports are generated after a sufficient amount of samples has been collected  after the drift, in order to ensure statistical reliability of the results. If the data distribution moves back to the reference before enough samples are collected, the report might not be generated.</p> <p>Each report is composed of several entities, each providing a different perspective on the data and the drift occurred.  Most of them are specific to a certain Data Structure, so they might not be available for all Tasks.</p> <p>These entities can take the form of tables, plots, or textual explanations.  Observed and analyzed together, they should provide a comprehensive understanding of the drift and its underlying causes. These are the entities currently available:</p> <ul> <li><code>Feature Importance</code>: it's a barplot that illustrates how the significance of each feature differs between the reference   and the production datasets. Variations in a feature's values might suggest that its contribution to the model's predictions   has changed over time. This entity is available only for tasks with tabular data.</li> </ul> <p> </p> Example of a feature importance plot. <ul> <li><code>Variable discriminative power</code>: it's also a bar plot displays the influence of each feature, as well as the target,   in differentiating between the reference and the production datasets.   The values represent how strongly a given feature helps to distinguish the datasets, with higher values representing stronger   separating power. This entity is available only for tasks with tabular data.</li> </ul> <p> </p> Example of a variable discriminative power plot. <ul> <li><code>Drift Score</code>: it's a line plot that shows the evolution of the drift score over time. The drift score is a    measure of the statistical distance between a sliding window of the production data and the reference data. It also shows the threshold,   which is the value that the drift score must exceed to raise a drift alarm, and all the Detection Events that were triggered in   the time frame of the report. This plot helps in understanding how the drift evolved over time and the moments in which the difference   between the two datasets was higher. Notice that some postprocessing is applied on the events to account for the functioning of the drift detection algorithms.    Specifically,   we shift back the drift on events by a certain offset, aiming to point at the precise time when the drift actually started. As a result,   drift on events might be shown before the threshold is exceeded. This explainability entity is available for all tasks.</li> </ul> <p> </p> Example of a drift score plot with detection events of increasing severity displayed."}]}