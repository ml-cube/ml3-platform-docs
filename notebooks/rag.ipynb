{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "This notebook shows how to use ML cube Platform with a Retrieval Augmented Generation task.\n",
    "\n",
    "We will use a dataset available in the Hugging Face hub as an example.\n",
    "\n",
    "Each sample in the dataset is composed by 3 elements:\n",
    "- user_input: the question asked by the user\n",
    "- context: the context retrieved by the retrieval system\n",
    "- answer: the output of the model that tries to answer the question\n",
    "\n",
    "The presence of these 3 elements allows us to simulate a full RAG system without actually setting up the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook you will learn:**\n",
    "- how to create a Rag task\n",
    "- how to define a data schema\n",
    "- how to create a model\n",
    "- how to upload historical data\n",
    "- how to set reference for the model\n",
    "- how to upload production data\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "These are the dependencies your Python environment is required to have in order to properly run this notebook.\n",
    "```\n",
    "ml3-platform-sdk>=0.0.22\n",
    "torch==2.2.0\n",
    "datasets==2.15.0\n",
    "sentence-transformers==3.0.1\n",
    "polars==0.20.31\n",
    "json==2.0.9\n",
    "numpy==1.26.4\n",
    "tqdm==4.66.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml3_platform_sdk import enums as ml3_enums\n",
    "from ml3_platform_sdk import models as ml3_models\n",
    "from ml3_platform_sdk.client import ML3PlatformClient\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import polars as pl\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://api.platform.mlcube.com'\n",
    "API_KEY = \"\"\n",
    "PROJECT_ID = ''\n",
    "model_name = 'mymodel'\n",
    "model_version = 'v0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, model and predictions\n",
    "Download dataset and model using Huggingface api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_INPUT_COL_NAME = 'question'\n",
    "CONTEXT_COL_NAME = 'context'\n",
    "ANSWER_COL_NAME = 'answer'\n",
    "\n",
    "complete_dataset = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train[:10%]\").filter(lambda x: all(x[col] is not None for col in [USER_INPUT_COL_NAME, CONTEXT_COL_NAME, ANSWER_COL_NAME]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, reference_portion=0.5, first_production_portion=0.5, seed=42):\n",
    "    sampled_dataset = DatasetDict()\n",
    "    \n",
    "    # Split the dataset into reference and production\n",
    "\n",
    "    split = dataset.train_test_split(test_size=reference_portion, seed=seed)\n",
    "\n",
    "    sampled_dataset['reference'] = split['train']\n",
    "\n",
    "    split_2 = split['test'].train_test_split(test_size=first_production_portion, seed=seed)\n",
    "\n",
    "    sampled_dataset['first_production'] = split_2['train']\n",
    "    sampled_dataset['second_production'] = split_2['test']\n",
    "\n",
    "    return sampled_dataset\n",
    "\n",
    "# Perform the sampling\n",
    "dataset = sample_dataset(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"reference\"]), len(dataset[\"first_production\"]), len(dataset[\"second_production\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data objects for ML cube Platform\n",
    "We use local data sources to upload data, hence, we need to create local files that will be shared with ML cube Platform.\n",
    "\n",
    "Uploading data coming from a RAG system is equivalent to uploading text data (refer to this [notebook](https://colab.research.google.com/github/ml-cube/ml3-platform-docs/blob/main/notebooks/text_classification.ipynb) for further information). \n",
    "\n",
    "Data needs to be stored in a json file as a list of objects. Each object must contain two mandatory fields, namely the timestamp and the sample-id, along with other the other fields that represent the data (e.g. question and context for input data, answer for prediction data).\n",
    "\n",
    "When dealing with unstructured data like text it is possible to send them in three ways:\n",
    "1. By sending only embeddings i.e., a numerical representation of the text sample as a vector, using `EmbeddingData`;\n",
    "2. By sending only unstructured text, using `TextData`. In this case ML cube Platform will create the numerical representation using internal encoders;\n",
    "3. By sending ustructured text along with embeddings using `TextData` with `embedding_source` attribute. This more complete option has two benefits: first, the usage of a personal embedder that usually is focused on the domain instead of a general one. Secondly, providing the text allows the platform to extract additional metrics thus enabling more insights and more comprehensive views in the web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are uploaded separately for each category:\n",
    "- **inputs:** TextData object in json format\n",
    "- **predictions:** TextData object in json format\n",
    "\n",
    "In Rag tasks, we deem the answer of the model as its prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_objects(\n",
    "    dataset,\n",
    "    embedder,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    starting_id: int,\n",
    "    starting_timestamp: float,\n",
    "    prefix: str,\n",
    "    with_prediction: bool = True\n",
    ") -> tuple[\n",
    "    ml3_models.TextData,\n",
    "    ml3_models.TextData | None,\n",
    "    int,\n",
    "    float\n",
    "]:\n",
    "    \"\"\"Builds data objects for inputs, target and predictions.\n",
    "    \n",
    "    Since each sample has associated an id and a timestamp we generate \n",
    "    them as incremental counters.\n",
    "    Therefore, we need a starting point for both.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    dataset: huggingface dataset\n",
    "    embedder: sentence transformer model used to encode text\n",
    "    model_name: name of the model\n",
    "    model_version: version of the model\n",
    "    starting_id: sample id to start from\n",
    "    starting_timestamp: timestamp to start from\n",
    "    prefix: prefix for the files\n",
    "    with_prediction: whether to include the prediction or not\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    input text_data, prediction text_data, last_sample_id, last_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(dataset[USER_INPUT_COL_NAME])\n",
    "    next_starting_sample_id = starting_id + n_samples\n",
    "    sample_ids = list(map(lambda x: f'sample_{x}', range(starting_id, starting_id + n_samples)))\n",
    "    # each sample has a time delta of 2 minutes\n",
    "    next_starting_timestamp = starting_timestamp + n_samples * 120\n",
    "    timestamps = list(np.arange(starting_timestamp, starting_timestamp + n_samples * 120, 120))\n",
    "    \n",
    "    input_samples_filename = f'{prefix}_input_samples.json'\n",
    "    prediction_samples_filename = f'{prefix}_prediction_samples.json'\n",
    "    input_embeddings_filename = f'{prefix}_input_embeddings.parquet'\n",
    "    prediction_embeddings_filename = f'{prefix}_prediction_embeddings.parquet'\n",
    "\n",
    "    # Create inputs embedding file and save it\n",
    "    print('Creating text files')\n",
    "    input_text_samples = []\n",
    "    prediction_text_samples = []\n",
    "    for (i, sample) in enumerate(dataset):\n",
    "        input_text_samples.append({\n",
    "            USER_INPUT_COL_NAME: sample[USER_INPUT_COL_NAME],\n",
    "            CONTEXT_COL_NAME: sample[CONTEXT_COL_NAME],\n",
    "            'sample-id': sample_ids[i],\n",
    "            'timestamp': timestamps[i]\n",
    "        })\n",
    "        \n",
    "        if with_prediction:\n",
    "            prediction_text_samples.append({\n",
    "                'sample-id': sample_ids[i],\n",
    "                'timestamp': timestamps[i],\n",
    "                f'{model_name}@{model_version}': sample[ANSWER_COL_NAME]\n",
    "            })\n",
    "\n",
    "    with open(input_samples_filename, 'w') as f:\n",
    "        json.dump(input_text_samples, f)\n",
    "    \n",
    "    if with_prediction:\n",
    "        with open(prediction_samples_filename, 'w') as f:\n",
    "            json.dump(prediction_text_samples, f)\n",
    "            \n",
    "    # Create embedding dataframe\n",
    "    print('Creating embedding file')\n",
    "    embeddings_input = pl.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'sample-id': sample_ids,\n",
    "        f'{USER_INPUT_COL_NAME}_embeddings': embedder.encode(dataset[USER_INPUT_COL_NAME]).tolist(),\n",
    "        f'{CONTEXT_COL_NAME}_embeddings': embedder.encode(dataset[CONTEXT_COL_NAME]).tolist(),        \n",
    "    })\n",
    "    embeddings_input.write_parquet(input_embeddings_filename)\n",
    "    \n",
    "    if with_prediction:\n",
    "        embeddings_prediction = pl.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'sample-id': sample_ids,\n",
    "            f'{model_name}_embeddings@{model_version}': embedder.encode(dataset[ANSWER_COL_NAME]).tolist(),\n",
    "        })\n",
    "        \n",
    "        embeddings_prediction.write_parquet(prediction_embeddings_filename)\n",
    "\n",
    "    print('Creating inputs data')\n",
    "    inputs_data = ml3_models.TextData(\n",
    "        source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.JSON,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=input_samples_filename\n",
    "        ),\n",
    "        embedding_source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.PARQUET,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=input_embeddings_filename\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    predictions_data = None\n",
    "    if with_prediction:\n",
    "        print('Creating predictions data')\n",
    "        predictions_data = ml3_models.TextData(\n",
    "            source=ml3_models.LocalDataSource(\n",
    "                file_type=ml3_enums.FileType.JSON,\n",
    "                is_folder=False,\n",
    "                folder_type=None,\n",
    "                file_path=prediction_samples_filename\n",
    "            ),\n",
    "            embedding_source=ml3_models.LocalDataSource(\n",
    "                file_type=ml3_enums.FileType.PARQUET,\n",
    "                is_folder=False,\n",
    "                folder_type=None,\n",
    "                file_path=prediction_embeddings_filename,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return (\n",
    "        inputs_data,\n",
    "        predictions_data,\n",
    "        next_starting_sample_id,\n",
    "        next_starting_timestamp\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_initial_sample_id = 0\n",
    "historical_initial_timestamp = datetime.datetime.now().timestamp()\n",
    "\n",
    "(\n",
    "    historical_inputs_data,\n",
    "    historical_prediction_data,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['reference'],\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    historical_initial_sample_id,\n",
    "    historical_initial_timestamp,\n",
    "    prefix='reference',\n",
    "    with_prediction=True\n",
    ")\n",
    "historical_end_timestamp = starting_timestamp - 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    production_0_inputs_data,\n",
    "    production_0_prediction_data,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['first_production'],\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    "    'prod_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    production_1_inputs_data,\n",
    "    production_1_prediction_data,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['second_production'],\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    "    'prod_1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data schema\n",
    "\n",
    "The data schema specifies the type of data present in the task with their specific names.\n",
    "A data schema must contain:\n",
    "- *sample id* column that is used to uniquely identify each sample\n",
    "- *timestamp* column that is used to order samples\n",
    "- *input* columns that specify the nature of the input. In this case, we have two input columns, one for the user input and one for the context. This distinction is specified through the subrole attribute.\n",
    "- *input additional embedding* optional column for additional embedding of the text data. Mirroring the input columns, two additional embedding columns needs to be specified.\n",
    "\n",
    "Prediction column must not be specified because it will be automatically added during the model creation with the name like `MODEL_NAME@MODEL_VERSION`. Same applies to its embedding column, which will be automatically added with the name `MODEL_NAME_embeddings@MODEL_VERSION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = ml3_models.DataSchema(\n",
    "    columns=[\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='timestamp',\n",
    "            role=ml3_enums.ColumnRole.TIME_ID,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.FLOAT,\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='sample-id',\n",
    "            role=ml3_enums.ColumnRole.ID,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.STRING,\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name=USER_INPUT_COL_NAME,\n",
    "            role=ml3_enums.ColumnRole.INPUT,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.STRING,\n",
    "            subrole=ml3_enums.ColumnSubRole.RAG_USER_INPUT\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name=CONTEXT_COL_NAME,\n",
    "            role=ml3_enums.ColumnRole.INPUT,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.STRING,\n",
    "            subrole=ml3_enums.ColumnSubRole.RAG_RETRIEVED_CONTEXT\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name=f'{USER_INPUT_COL_NAME}_embeddings',\n",
    "            role=ml3_enums.ColumnRole.INPUT_ADDITIONAL_EMBEDDING,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.ARRAY_1,\n",
    "            dims=(768,),\n",
    "            subrole=ml3_enums.ColumnSubRole.RAG_USER_INPUT\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name=f'{CONTEXT_COL_NAME}_embeddings',\n",
    "            role=ml3_enums.ColumnRole.INPUT_ADDITIONAL_EMBEDDING,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.ARRAY_1,\n",
    "            dims=(768,),\n",
    "            subrole=ml3_enums.ColumnSubRole.RAG_RETRIEVED_CONTEXT\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with ML cube Platform\n",
    "To start, we create an instance of ML cube Platform client using the provided api key.\n",
    "Then, we create a task, a dataschema, a model and finally we upload data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ML3PlatformClient(URL, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a task there are some information we need to specify:\n",
    "- **task_type:** artificial intelligence task type. In this case it is a RAG task.\n",
    "- **data_structure:** the type of input data. Since we are dealing with text data, we set it to TEXT.\n",
    "- **optional_target:** rag tasks don't have a target, hence it must be set to True\n",
    "- **text_language:** it is mandatory to specify the language used in the task.\n",
    "- **rag_context_separator**: the string used to separate different contexts. In this case it is None as the context is composed of a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = client.create_task(\n",
    "    project_id=PROJECT_ID,\n",
    "    name='RAG_task',\n",
    "    tags=[\"tag_1\", \"tag_2\"],\n",
    "    task_type=ml3_enums.TaskType.RAG,\n",
    "    data_structure=ml3_enums.DataStructure.TEXT,\n",
    "    optional_target=True, # Must be True in RAG tasks\n",
    "    text_language=ml3_enums.TextLanguage.ENGLISH,\n",
    "    rag_contexts_separator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_data_schema(task_id=task_id, data_schema=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we added the data schema, we are able to create our model.\n",
    "\n",
    "A model is uniquely identified by `name` and `model version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id: str = client.create_model(\n",
    "    task_id=task_id,\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    metric_name=None,  # Must be None in RAG tasks\n",
    "    preferred_suggestion_type=None,  # Must be None in RAG tasks\n",
    "    with_probabilistic_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference data are uploaded as historical data i.e., any data that do not come from production.\n",
    "Reference data are defined by their initial and final timestamps and they are used to configure the detection algorithms.\n",
    "\n",
    "Note that it is possible to add other historical data in any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_historical_data(\n",
    "    task_id=task_id,\n",
    "    inputs=historical_inputs_data,\n",
    "    predictions=[(model_id, historical_prediction_data)]\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.set_model_reference(\n",
    "    model_id,\n",
    "    from_timestamp=historical_initial_timestamp,\n",
    "    to_timestamp=historical_end_timestamp,\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to upload production data.\n",
    "Production data can be uploaded asynchronously, that means that we can upload each data category whenever, it is available without waiting for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    inputs=production_0_inputs_data,\n",
    "    predictions=[(model_id, production_0_prediction_data)]\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send production data asynchronously, first *inputs* and then *predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    inputs=production_1_inputs_data,\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    predictions=[(model_id, production_1_prediction_data)]\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
