{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "This notebook shows how to use the ML cube Platform with text data.\n",
    "We utilize a Huggingface dataset and a pre-trained model for Sentiment classification. We load the validation data and split the dataset in two parts, using the first as reference data and the second as production data. \n",
    "\n",
    "In a real-world scenario, the training and validation datasets would be considered historical/reference data, while production data would come from the production environment after the model's deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With this example you will learn:**\n",
    "- how to create a text classification task\n",
    "- how to define a data schema\n",
    "- how to create a model\n",
    "- how to upload historical data\n",
    "- how to set reference for the model\n",
    "- how to upload production data\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "These are the dependencies your Python environment is required to have in order to properly run this notebook.\n",
    "```\n",
    "ml3-platform-sdk>=0.0.22\n",
    "transformers[torch]==4.41.2\n",
    "torch==2.2.0\n",
    "datasets==2.15.0\n",
    "sentence-transformers==3.0.1\n",
    "polars==0.20.31\n",
    "json==2.0.9\n",
    "numpy==1.26.4\n",
    "tqdm==4.66.4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml3_platform_sdk import enums as ml3_enums\n",
    "from ml3_platform_sdk import models as ml3_models\n",
    "from ml3_platform_sdk.client import ML3PlatformClient\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import polars as pl\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://api.platform.mlcube.com'\n",
    "API_KEY = \"\"\n",
    "PROJECT_ID = ''\n",
    "model_name = 'mymodel'\n",
    "model_version = 'v0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, model and predictions\n",
    "Download dataset and model using Huggingface api.\n",
    "After the dataset and the model are downloaded we run the model to get predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = load_dataset('cardiffnlp/tweet_eval', name='sentiment', split='validation[:50%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, reference_portion=0.5, first_production_portion=0.5, seed=42):\n",
    "    sampled_dataset = DatasetDict()\n",
    "    \n",
    "    # Split the dataset into reference and production\n",
    "\n",
    "    split = dataset.train_test_split(test_size=reference_portion, seed=seed)\n",
    "\n",
    "    sampled_dataset['reference'] = split['train']\n",
    "\n",
    "    split_2 = split['test'].train_test_split(test_size=first_production_portion, seed=seed)\n",
    "\n",
    "    sampled_dataset['first_production'] = split_2['train']\n",
    "    sampled_dataset['second_production'] = split_2['test']\n",
    "\n",
    "    return sampled_dataset\n",
    "\n",
    "# Perform the sampling\n",
    "dataset = sample_dataset(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['reference']['text']), len(dataset['first_production']['text']), len(dataset['second_production']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pip = pipeline(model='cardiffnlp/twitter-roberta-base-sentiment-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data objects for ML cube Platform\n",
    "We use local data sources to upload data, hence, we need to create local files that will be shared with ML cube Platform.\n",
    "With text data we can upload data in json as a list of objects containing three fields: timestamp, sample-id, text.\n",
    "Text data can be composed of only text sequences but also with their embeddings (optional).\n",
    "On the other hand, target and predictions can be sent as csv tabular files.\n",
    "\n",
    "In ML cube Platform data are uploaded separately for each category:\n",
    "- **inputs:** TextData object in json format\n",
    "- **target:** TabularData object in csv format\n",
    "- **predictions:** TabularData object in csv format\n",
    "\n",
    "\n",
    "When dealing with unstructured data like text it is possible to send them in three ways:\n",
    "1. By sending only embeddings i.e., a numerical representation of the text sample as a vector, using `EmbeddingData`;\n",
    "2. By sending only the raw text, using `TextData`. In this case ML cube Platform will create the numerical representation using internal encoders;\n",
    "3. By sending the raw text along with the embeddings, using `TextData` with the  `embedding_source` attribute. This more complete option has two benefits:it allows the usage of a personal embedder, which is usually focused on the domain rather than a general one, and it enables the extraction of additional metrics from the text, providing more functionalities in the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_objects(\n",
    "    dataset,\n",
    "    model,\n",
    "    embedder,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    starting_id: int,\n",
    "    starting_timestamp: float,\n",
    "    prefix: str,\n",
    ") -> tuple[\n",
    "    ml3_models.TextData,\n",
    "    ml3_models.TabularData,\n",
    "    ml3_models.TabularData,\n",
    "    int,\n",
    "    float\n",
    "]:\n",
    "    \"\"\"Builds data objects for inputs, target and predictions.\n",
    "    \n",
    "    Since each sample has associated an id and a timestamp we generate \n",
    "    them as incremental counters.\n",
    "    Therefore, we need a starting point for both.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    dataset: huggingface dataset\n",
    "    starting_id: sample id to start from\n",
    "    starting_timestamp: timestamp to start from\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    text_data, last_sample_id, last_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(dataset['text'])\n",
    "    next_starting_sample_id = starting_id + n_samples\n",
    "    sample_ids = list(map(lambda x: f'sample_{x}', range(starting_id, starting_id + n_samples)))\n",
    "    # each sample has a time delta of 2 minutes\n",
    "    next_starting_timestamp = starting_timestamp + n_samples * 120\n",
    "    timestamps = list(np.arange(starting_timestamp, starting_timestamp + n_samples * 120, 120))\n",
    "\n",
    "    # Create inputs embedding file and save it\n",
    "    print('Creating text file')\n",
    "    text_samples = []\n",
    "    for (i, sample) in enumerate(dataset['text']):\n",
    "        text_samples.append({\n",
    "            'text': sample,\n",
    "            'sample-id': sample_ids[i],\n",
    "            'timestamp': timestamps[i]\n",
    "        })\n",
    "\n",
    "    with open(f'{prefix}_text_samples.json', 'w') as f:\n",
    "        json.dump(text_samples, f)\n",
    "    \n",
    "    # Create embedding dataframe\n",
    "    print('Creating embedding file')\n",
    "    embeddings = pl.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'sample-id': sample_ids,\n",
    "        'embedding': embedder.encode((dataset['text'])).tolist()\n",
    "    })\n",
    "    embeddings.write_parquet(f'{prefix}_embeddings.parquet')\n",
    "\n",
    "    print('Creating target file')\n",
    "    target = pl.DataFrame({\n",
    "        'label': dataset['label'],\n",
    "        'timestamp': timestamps,\n",
    "        'sample-id': sample_ids,\n",
    "    })\n",
    "    target.write_csv(f'{prefix}_target.csv')\n",
    "\n",
    "    print('Creating predictions file')\n",
    "    predicted_labels = []\n",
    "    for pred in tqdm.tqdm(model(KeyDataset(dataset, \"text\"))):\n",
    "        prediction = model.model.config.label2id[pred['label']]\n",
    "        if prediction not in LABELS:\n",
    "                prediction = LABELS[0]\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    predictions = pl.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'sample-id': sample_ids,\n",
    "        f'{model_name}@{model_version}': predicted_labels,\n",
    "    })\n",
    "    predictions.write_csv(f'{prefix}_predictions.csv')\n",
    "\n",
    "    print('Creating inputs data')\n",
    "    inputs_data = ml3_models.TextData(\n",
    "        source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.JSON,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=f'{prefix}_text_samples.json'\n",
    "        ),\n",
    "        embedding_source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.PARQUET,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=f'{prefix}_embeddings.parquet',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print('Creating target data')\n",
    "    target_data = ml3_models.TabularData(\n",
    "        source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.CSV,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=f'{prefix}_target.csv'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print('Creating predictions data')\n",
    "    predictions_data = ml3_models.TabularData(\n",
    "        source=ml3_models.LocalDataSource(\n",
    "            file_type=ml3_enums.FileType.CSV,\n",
    "            is_folder=False,\n",
    "            folder_type=None,\n",
    "            file_path=f'{prefix}_predictions.csv'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        inputs_data,\n",
    "        target_data,\n",
    "        predictions_data,\n",
    "        next_starting_sample_id,\n",
    "        next_starting_timestamp\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_initial_sample_id = 0\n",
    "training_initial_timestamp = datetime.datetime.now().timestamp()\n",
    "\n",
    "(\n",
    "    training_inputs_data,\n",
    "    training_target_data,\n",
    "    _,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['reference'],\n",
    "    sentiment_pip,\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    training_initial_sample_id,\n",
    "    training_initial_timestamp,\n",
    "    'reference'\n",
    ")\n",
    "training_end_timestamp = starting_timestamp - 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    production_0_inputs_data,\n",
    "    production_0_target_data,\n",
    "    production_0_prediction_data,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['first_production'],\n",
    "    sentiment_pip,\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    "    'prod_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    production_1_inputs_data,\n",
    "    production_1_target_data,\n",
    "    production_1_prediction_data,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    ") = build_data_objects(\n",
    "    dataset['second_production'],\n",
    "    sentiment_pip,\n",
    "    embedder,\n",
    "    model_name,\n",
    "    model_version,\n",
    "    starting_id,\n",
    "    starting_timestamp,\n",
    "    'prod_1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data schema\n",
    "\n",
    "The data schema specifies the type of data present in the task with their specific names.\n",
    "A data schema must contain:\n",
    "- *sample id*, column that is used to uniquely identify each sample\n",
    "- *timestamp*, column that is used to order samples\n",
    "- *input*, column that specifies the nature of the input. In this case, it's a string, as we are dealing with text data.\n",
    "- *input additional embedding*, optional column for the embedding of the text data\n",
    "- *target*, column that specifies the nature of the target. In this case, categorical with three possible values\n",
    "\n",
    "The prediction column must not be specified because it will be automatically added during the model creation, with a name like `MODEL_NAME@MODEL_VERSION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = ml3_models.DataSchema(\n",
    "    columns=[\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='timestamp',\n",
    "            role=ml3_enums.ColumnRole.TIME_ID,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.FLOAT,\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='sample-id',\n",
    "            role=ml3_enums.ColumnRole.ID,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.STRING,\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='text',\n",
    "            role=ml3_enums.ColumnRole.INPUT,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.STRING,\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name=\"embedding\",\n",
    "            role=ml3_enums.ColumnRole.INPUT_ADDITIONAL_EMBEDDING,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.ARRAY_1,\n",
    "            dims=(768,),\n",
    "        ),\n",
    "        ml3_models.ColumnInfo(\n",
    "            name='label',\n",
    "            role=ml3_enums.ColumnRole.TARGET,\n",
    "            is_nullable=False,\n",
    "            data_type=ml3_enums.DataType.CATEGORICAL,\n",
    "            possible_values=LABELS\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with ML cube Platform\n",
    "To start, we create an instance of ML cube Platform client using the provided api key.\n",
    "Then, we create a task, a dataschema, a model and finally we upload data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ML3PlatformClient(URL, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a task there are some information we need to specify:\n",
    "- **task_type:** artificial intelligence task type. In this case it is a multiclass classification\n",
    "- **data_structure:** the type of input data. In this case it is Text\n",
    "- **optional_target:** if the target can be missing for production data.\n",
    "    We assume that reference data being the training one always have the target.\n",
    "    However, it is possible that ofr other historical data or for production data target is not available, enabling this option, ML cube Platform does not force its presence and it will not stop breaks the jobs.\n",
    "- **text_language:** if the target can be missing for production data. We assume that reference data, being the training data, always have the target.\n",
    "    However, it is possible that target is not available in other historical data or production data. By enabling the optional target option, the ML cube Platform will not check its presence in the data sent to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = client.create_task(\n",
    "    project_id=PROJECT_ID,\n",
    "    name='task_name',\n",
    "    tags=[\"tag_1\", \"tag_2\"],\n",
    "    task_type=ml3_enums.TaskType.CLASSIFICATION_MULTICLASS,\n",
    "    data_structure=ml3_enums.DataStructure.TEXT,\n",
    "    optional_target=False,\n",
    "    text_language=ml3_enums.TextLanguage.ENGLISH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_data_schema(task_id=task_id, data_schema=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is uniquely identified by `name` and `model version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id: str = client.create_model(\n",
    "    task_id=task_id,\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    metric_name=ml3_enums.ModelMetricName.ACCURACY,\n",
    "    preferred_suggestion_type=ml3_enums.SuggestionType.SAMPLE_WEIGHTS,\n",
    "    with_probabilistic_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are uploaded as historical data i.e., any data that do not come from production.\n",
    "Then we indicate them as reference data of our model in order to set up the detection algorithms.\n",
    "\n",
    "Note that it is possible to add other historical data at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_historical_data(\n",
    "    task_id=task_id,\n",
    "    inputs=training_inputs_data,\n",
    "    target=training_target_data\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.set_model_reference(\n",
    "    model_id,\n",
    "    from_timestamp=training_initial_timestamp,\n",
    "    to_timestamp=training_end_timestamp,\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to upload production data.\n",
    "Notice that production data can be uploaded asynchronously, which means that we can upload each data category whenever it is available, without waiting for the others.\n",
    "This is specially true for *target* data, that usually are available with an amount of delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    inputs=production_0_inputs_data,\n",
    "    target=production_0_target_data,\n",
    "    predictions=[(model_id, production_0_prediction_data)]\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send production data asynchronously, first *inputs* and *predictions* and then *target*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    inputs=production_1_inputs_data,\n",
    "    target=None,\n",
    "    predictions=[(model_id, production_1_prediction_data)]\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = client.add_production_data(\n",
    "    task_id=task_id,\n",
    "    inputs=None,\n",
    "    target=production_1_target_data,\n",
    "    predictions=None\n",
    ")\n",
    "print(f'Waiting for job {job_id}')\n",
    "client.wait_job_completion(job_id=job_id)\n",
    "print(f'Job {job_id} completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
